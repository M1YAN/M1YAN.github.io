<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What are Diffusion Models? | M1YAN&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="
Update: 增加了条件生成以及潜在扩散模型的介绍。

生成模型
目前主流的生成模型包括生成对抗模型 (GAN)、变分自编码器 (VAE)和基于流的模型 (Flow-based models)。">
<meta name="author" content="Mi Yan">
<link rel="canonical" href="https://m1yan.github.io/posts/diffusion-model/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d77bdc992da4cb34344677f4385e53cf34c99acf4c535ef64a740cb221eac3d0.css" integrity="sha256-13vcmS2kyzQ0Rnf0OF5TzzTJms9MU172SnQMsiHqw9A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://m1yan.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://m1yan.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://m1yan.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://m1yan.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://m1yan.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="https://m1yan.github.io/posts/diffusion-model/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false}
          ],
          
          throwOnError : false
        });
    });
</script>

<link rel="stylesheet" href="https://s1.hdslb.com/bfs/static/jinkela/long/font/regular.css" />
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&display=swap" rel="stylesheet"><meta property="og:url" content="https://m1yan.github.io/posts/diffusion-model/">
  <meta property="og:site_name" content="M1YAN&#39;s Blog">
  <meta property="og:title" content="What are Diffusion Models?">
  <meta property="og:description" content=" Update: 增加了条件生成以及潜在扩散模型的介绍。
生成模型 目前主流的生成模型包括生成对抗模型 (GAN)、变分自编码器 (VAE)和基于流的模型 (Flow-based models)。">
  <meta property="og:locale" content="zh">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-07T00:00:00+00:00">
      <meta property="og:image" content="https://m1yan.github.io/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://m1yan.github.io/images/papermod-cover.png">
<meta name="twitter:title" content="What are Diffusion Models?">
<meta name="twitter:description" content="
Update: 增加了条件生成以及潜在扩散模型的介绍。

生成模型
目前主流的生成模型包括生成对抗模型 (GAN)、变分自编码器 (VAE)和基于流的模型 (Flow-based models)。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://m1yan.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What are Diffusion Models?",
      "item": "https://m1yan.github.io/posts/diffusion-model/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What are Diffusion Models?",
  "name": "What are Diffusion Models?",
  "description": " Update: 增加了条件生成以及潜在扩散模型的介绍。\n生成模型 目前主流的生成模型包括生成对抗模型 (GAN)、变分自编码器 (VAE)和基于流的模型 (Flow-based models)。\n",
  "keywords": [
    
  ],
  "articleBody": " Update: 增加了条件生成以及潜在扩散模型的介绍。\n生成模型 目前主流的生成模型包括生成对抗模型 (GAN)、变分自编码器 (VAE)和基于流的模型 (Flow-based models)。\n它们都能够生成较高质量的图像，但是也都具有一定的局限性。由于GAN模型具有对抗性训练的性质，因此其训练过程比较脆弱且难以稳定收敛，生成图像的多样性也较低。与GAN相比，VAE经常会生成较模糊、不够锐利的样本，因为VAE在优化过程中引入了KL散度正则项，鼓励潜变量分布与先验分布靠拢，因此会损失部分细节信息。基于流的生成模型通过严格的可逆变换实现对数据分布的精确密度估计，这意味着每一步变换需要是可逆且雅可比行列式可计算，因此在模型设计上对层结构有较强限制。\n扩散模型的设计思路来自非平衡热力学。模型定义了一个马尔可夫扩散步骤，缓慢地向图像中添加随机噪声，然后学习扩散的逆过程以从噪声中构建所需要的数据样本。\n生成模型的结构，图引自 Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. 什么是扩散模型？ 目前主流的基于扩散的生成模型包括扩散概率模型 (Diffusion Probabilistic Models)、条件噪声打分网络 (noise-conditioned score network)和去噪扩散概率模型 (denoising diffusion probabilistic models, DDPM)，扩散过程包括前向扩散过程和逆向扩散过程。\n前向扩散过程 给定从真实数据分布中采样的数据点 $x_0$ ~ $q(x) $ ，定义一个前向扩散过程，在这个过程中，我们向样本中添加高斯噪声 $T$ 步，产生一系列含有噪声的样本 $ x_1, …, x_T $。 $$ q(x_t|x_{t-1})=\\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1},\\beta_tI) \\ q(x_{1:T}|x_0) = \\prod_{t=1}^Tq(x_t|x_{t-1}) $$ 随着加噪步数的增加，图像特征逐渐消失，最终当T趋近于无穷时，$x_T$ 相当于各向同性的高斯分布。\n通过缓慢地添加（去除）噪声的正向（反向）扩散过程，图引自Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. 上述采样过程能够使我们计算出在时间t时刻的采样$x_t$。令\n$$ \\alpha_t=1-\\beta_t, \\overline{\\alpha}_t = \\prod_{i=1}^t\\alpha_i \\\\ x_t = \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\ =\\sqrt{\\alpha_t\\alpha_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-2} \\\\ =... \\\\ =\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha_t}}\\epsilon $$ 逆向扩散过程 为了实现从无序噪声恢复到数据分布（即反向扩散过程），需要对后验分布 $q(x_{t-1}|x_t, x_0)$ 进行分析。根据贝叶斯公式：\n$$ q(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)} $$ 由于前向过程定义为条件独立的马尔可夫链，有： $$ q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) \\\\ q(x_{t-1} \\mid x_0) = \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0, (1-\\bar{\\alpha}_{t-1})I) $$ 将以上分布代入后，可得到后验分布仍是高斯分布形式： $$ q(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I) $$ 其中： $$ \\tilde{\\mu}_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} x_t, \\\\ \\tilde{\\beta}_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t $$ 反向扩散过程的核心在于：如果我们能够对 $q(x{t-1} | x_t)$ 进行近似，就可以从纯噪声一步一步还原为原始数据分布。由于我们不知道$x_0$，我们希望有一个参数化的模型 $p\\theta$ 来近似 $q(x_{t-1}|x_t,x_0)$： $$ p_\\theta(x_{t-1} \\mid x_t) \\approx q(x_{t-1} \\mid x_t, x_0) $$ 若使用模型 $\\epsilon_\\theta(x_t,t)$ 来预测噪声，则可得到简化的逆扩散公式： $$ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\\right) + \\sqrt{\\tilde{\\beta}_t} z, \\quad z \\sim \\mathcal{N}(0, I) $$ t=1时省略最后的噪声项，最终可以得到$x_0$的样本。可以看到，扩散模型通过预测噪声$\\epsilon_\\theta$来重构之前时间步的样本。 可以得到忽略加权项的简化目标来训练扩散模型，最终设计的损失函数如下：\n$$ L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0,\\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right] $$ 其中， $$ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I), \\quad t \\sim \\text{Uniform}\\{1,\\ldots,T\\} $$ 若考虑不同时间步的加权，可以定义加权损失： $$ L(\\theta) = \\sum_{t=1}^{T} w_t \\mathbb{E}_{x_0,\\epsilon}\\left[\\| \\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 \\right] $$ DDPM论文中的算法如下： 实现一个DDPM 为了更清楚地了解扩散模型的整体架构，而不是为了探究复杂的概率论和数学原理，使用diffusers库实现DDPM的训练和推理。\n我们使用huggingface上的huggan/smithsonian_butterflies_subset作为训练数据集，该数据集包含自然界各种各样的蝴蝶，可用于无条件的图像生成过程。\n参数配置 首先按照以下配置进行训练和推理步骤的参数配置：\nfrom dataclasses import dataclass @dataclass class TrainingConfig: image_size = 128 train_batch_size = 32 eval_batch_size = 8 # how many images to sample during evaluation num_epochs = 50 gradient_accumulation_steps = 1 learning_rate = 1e-4 lr_warmup_steps = 500 save_image_epochs = 10 save_model_epochs = 25 mixed_precision = \"fp16\" # `no` for float32, `fp16` for automatic mixed precision output_dir = \"output\" # the model name locally and on the HF Hub seed = 42 device = \"cuda\" config = TrainingConfig() 训练数据准备 然后使用huggingface的datasets库进行数据集的下载和导入：\nfrom datasets import load_dataset config.dataset_name = \"huggan/smithsonian_butterflies_subset\" dataset = load_dataset(config.dataset_name, split=\"train\") 使用以下代码查看数据集中的图像：\nimport matplotlib.pyplot as plt fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for i, image in enumerate(dataset[:4][\"image\"]): axs[i].imshow(image) axs[i].set_axis_off() fig.show() 使用torchvision库中的transforms模块，将图像的尺寸和数值归一化处理：\nfrom torchvision import transforms preprocess = transforms.Compose([ transforms.Resize((config.image_size, config.image_size)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), ]) def transform(examples): images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]] return {\"img\": images} dataset.set_transform(transform) print(dataset[0]['img'].shape) # torch.Size([3, 128, 128]) 定义一个dataloader用于数据集的批量加载：\nimport torch train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True) 使用U-Net进行噪声预测 在扩散模型中，可以使用MLP或者U-Net来进行噪声的预测，从而将噪声一步一步去噪得到真实图像。选择U-Net作为噪声预测的模型，U-Net的架构由下采样堆栈和上采样堆栈构成。\n下采样：每个步骤包括重复应用两个 3x3 卷积（无填充卷积），每个卷积后跟一个 ReLU 和一个步幅为 2 的 2x2 最大池化。在每个下采样步骤中，特征通道的数量都会加倍。 上采样：每个步骤包括对特征图的上采样，然后进行 2x2 卷积，并且每次将特征通道数量减半。 捷径连接：上下采样堆栈相应层通过捷径连接，为上采样过程提供必要的高分辨率特征。 实现如下：\nfrom diffusers import UNet2DModel model = UNet2DModel( sample_size=config.image_size, # the target image resolution in_channels=3, # the number of input channels, 3 for RGB images out_channels=3, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(128, 128, 256, 256, 512, 512), # the number of output channels for each UNet block down_block_types=( \"DownBlock2D\", # a regular ResNet downsampling block \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", # a ResNet downsampling block with spatial self-attention \"DownBlock2D\", ), up_block_types=( \"UpBlock2D\", # a regular ResNet upsampling block \"AttnUpBlock2D\", # a ResNet upsampling block with spatial self-attention \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", ), ) # Check input and output shapes sample_image = dataset[0]['img'].unsqueeze(0) print(\"Input Shape\", sample_image.shape) print(\"Output Shape\", model(sample_image, timestep=0).sample.shape) 通过检查输入U-Net和输出U-Net的图像形状，可以得知输入和预测噪声的形状一致，满足扩散模型的需求。\nDDPM Scheduler 创建一个噪声调度器，用来在不同的时间步中为图像加噪。\n# Create a DDPM scheduler import torch from PIL import Image from diffusers import DDPMScheduler noise_scheduler = DDPMScheduler(num_train_timesteps=1000) noise = torch.randn(sample_image.shape) timesteps = torch.LongTensor([50]) noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps) Image.fromarray(((noisy_image.permute(0,2,3,1)+1.0)*127.5).type(torch.uint8).numpy()[0]) 从加噪后的输出可以看出，图像中出现了明显的噪声。\n创建优化器和学习率调度器 # Create optim and lr scheduler from diffusers.optimization import get_cosine_schedule_with_warmup optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate) lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.lr_warmup_steps, num_training_steps= (len(train_dataloader)*config.num_epochs) ) 设计损失函数 扩散模型的核心在于优化预测噪声的模型，因此需要使预测噪声的模型 (U-Net) 输出的噪声与实际噪声的分布接近。因此损失函数可以简单地设计为：\nnoise_pred = model(noisy_images, timesteps, return_dict=False)[0] loss = F.mse_loss(noise_pred, noise) 训练过程 使用huggingface的accelerate库进行方便的模型加载、权重保存以及模型评估。训练的整体思路是生成图像不同时间步中加入噪声后的图像，U-Net接受加噪后的图像以及其对应的时间步，预测出该步骤加入的噪声。预测噪声与实际加入的噪声使用loss进行计算，最小化loss，进而使U-Net具有预测噪声的能力。最终在推理过程中能够使用U-Net在每个时间步进行去噪，最后生成接近真实分布的图像。训练循环代码如下：\nfrom accelerate import Accelerator from tqdm.auto import tqdm from pathlib import Path import torch.nn.functional as F import os def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler): # Initialize accelerator accelerator = Accelerator( mixed_precision=config.mixed_precision, gradient_accumulation_steps=config.gradient_accumulation_steps, project_dir=os.path.join(config.output_dir, \"logs\") ) if accelerator.is_main_process: if config.output_dir is not None: os.makedirs(config.output_dir, exist_ok=True) # Prepare everything model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare( model, optimizer, train_dataloader, lr_scheduler ) global_step = 0 # Train! for epoch in range(config.num_epochs): progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process) progress_bar.set_description(f\"Epoch {epoch}\") for step, batch in enumerate(train_dataloader): clean_images = batch[\"img\"] # Sample noise to add to the clean image noise = torch.randn(clean_images.shape, device=config.device) bs = clean_images.shape[0] # Sample a random timestep for each image timesteps = torch.randint( 0, noise_scheduler.config.num_train_timesteps, (bs,), device=config.device, dtype=torch.int64 ) # forward diffusion process noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) with accelerator.accumulate(model): # Predict Noise residual noise_pred = model(noisy_images, timesteps, return_dict=False)[0] loss = F.mse_loss(noise_pred, noise) accelerator.backward(loss) if accelerator.sync_gradients: accelerator.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step} progress_bar.set_postfix(**logs) accelerator.log(logs, step=global_step) global_step += 1 # Evaluation if accelerator.is_main_process: pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler) if (epoch + 1) % config.save_image_epochs == 0: evaluate(config, epoch, pipeline) if (epoch + 1) % config.save_model_epochs == 0: pipeline.save_pretrained(os.path.join(config.output_dir, f\"epoch_{epoch}\")) elif (epoch + 1) == config.num_epochs: pipeline.save_pretrained(os.path.join(config.output_dir, f\"final\")) 其中，模型评估的代码如下：\n# Evaluation from diffusers import DDPMPipeline from diffusers.utils import make_image_grid import os def evaluate(config, epoch, pipeline): images = pipeline( batch_size = config.eval_batch_size, generator = torch.Generator(device=config.device).manual_seed(config.seed), ).images image_grid = make_image_grid(images, rows=2, cols=4) test_dir = os.path.join(config.output_dir, \"test\") os.makedirs(test_dir, exist_ok=True) image_grid.save(os.path.join(test_dir, f\"epoch_{epoch}.png\")) 训练与推理结果 最后使用以下代码在Jupyter Notebook中启动训练：\nrom accelerate import notebook_launcher args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler) notebook_launcher(train_loop, args, num_processes=1) 最终不同训练epoch的推理结果如下：\n可以看出，随着训练步数的增加，生成的图像越来越向真实的图像分布（蝴蝶形态）靠拢，说明经过训练后，扩散模型具有了生成图像的能力。\n使用IS和FID指标进行图像质量评估 什么是IS (Inception Score) ? Inception Score 是一种对生成图像的质量和多样性进行评价的指标。其思路是利用一个预训练好的分类模型（通常是 Inception v3）对生成的图像进行分类，然后根据分类结果的分布来计算得分。\n$p(y|x)$ 为给定生成图像 $x$ 的类别分布，$p(y) = \\int p(y|x) p(x) dx$ 为所有生成图像的平均类别分布，$KL(\\cdot|\\cdot)$ 为KL散度，则IS为： $$ \\text{IS} = \\exp\\left( \\mathbb{E}_{x}\\bigl[ KL(p(y \\mid x) | p(y)) \\bigr] \\right) $$\n直观上：\n如果生成图像的质量高，则概率分布应该集中在某些明确的类上（即分布峰值较高，说明图像能够被轻松分类） 如果生成图像的多样性高，则概率分布应该均匀覆盖多个类别。 综合来看，IS高时，说明生成图像既清晰又多样。\n什么是FID (Frechet Inception Distance) ? FID 用于衡量生成分布和真实数据分布在特征空间（通常是 Inception v3 的中间特征层）上的差异。与IS不同，FID需要真实样本和生成样本作为对比，关注两者之间的统计差异。\n设真实数据特征分布为 $\\mathcal{N}(\\mu_r, \\Sigma_r)$，生成数据特征分布为 $\\mathcal{N}(\\mu_g, \\Sigma_g)$，则FID定义为两高斯分布的Fréchet距离： $$ \\text{FID}(\\mu_r, \\Sigma_r, \\mu_g, \\Sigma_g) = |\\mu_r - \\mu_g|^2 + \\text{Tr}\\left(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}\\right). $$ 其中，$\\mu_r, \\Sigma_r$ 为真实分布特征的均值和协方差，$\\mu_g, \\Sigma_g$ 为生成分布特征的均值和协方差，$\\text{Tr}(\\cdot)$ 为迹运算，$( \\Sigma_r \\Sigma_g )^{1/2}$ 为矩阵的对称正定平方根。\n直观上：\nFID衡量的是两个高斯分布之间的Fréchet距离，当两组特征分布一致时，FID为0（理想情况下）。\n如果生成图像质量越高越逼近真实分布，那么 $\\mu_g \\approx \\mu_r$ 且 $\\Sigma_g \\approx \\Sigma_r$，因此FID会很低。\n如果生成图像与真实分布偏差大，分布统计差异明显，FID会较高。\nIS评估 测量IS和FID指标，需要的图像数量至少需要上万张，由于生图速度较慢，使用500张生成图像进行IS指标的测量。\n首先使用以下代码进行sampling：\n# Generate samples from diffusers import DDPMPipeline import random pipeline = DDPMPipeline.from_pretrained(\"/openbayes/home/miyan/works/Diffusion-Model-0-1/output/epoch_49\").to(config.device) samples_num = 500 batch_size = 20 for epoch in range(samples_num // batch_size): images = pipeline( batch_size = batch_size, generator = [torch.Generator(device=config.device).manual_seed(random.randint(0, 100000)) for _ in range(batch_size)], ).images test_dir = os.path.join(config.output_dir, \"samples\") os.makedirs(test_dir, exist_ok=True) for i, image in enumerate(images): image.save(os.path.join(test_dir, f\"{i+epoch*batch_size}.png\")) 使用以下代码进行IS指标的测量：\nimport torch import torch.nn as nn import torch.nn.functional as F import torchvision.models as models import numpy as np from scipy.linalg import sqrtm def calculate_inception_score(images, device, batch_size=32, splits=10): \"\"\" 计算 Inception Score (IS)。 参数: images: torch.Tensor，形状为(N, C, H, W) 的生成图像 device: torch.device，计算设备（CPU或GPU） batch_size: 批大小 splits: 将生成的图片集分为几份计算IS 返回: (is_mean, is_std): IS的均值和标准差 \"\"\" # 加载预训练的Inception v3模型，用于分类 inception = models.inception_v3(pretrained=True, transform_input=True).to(device) inception.eval() preds = [] # 分批次计算预测概率分布 with torch.no_grad(): for i in range(0, len(images), batch_size): batch = images[i:i+batch_size].to(device) # Inception v3要求输入为299x299，如果 images 已经是此大小且已标准化则无需再次处理 logits = inception(batch) probs = F.softmax(logits, dim=1) preds.append(probs.cpu().numpy()) preds = np.concatenate(preds, axis=0) # (N, 1000) # 计算IS N = preds.shape[0] split_scores = [] for k in range(splits): part = preds[k * (N // splits) : (k+1) * (N // splits), :] p_y = np.mean(part, axis=0) scores = [] for i in range(part.shape[0]): p_yx = part[i] scores.append(np.sum(p_yx * (np.log(p_yx + 1e-10) - np.log(p_y + 1e-10)))) split_scores.append(np.exp(np.mean(scores))) is_mean = np.mean(split_scores) is_std = np.std(split_scores) return is_mean, is_std import os from PIL import Image import torch import torchvision.transforms as transforms from torch.utils.data import DataLoader from torchvision import datasets import numpy as np device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") ####################### # 数据预处理Transform ####################### # Inception v3预期输入尺寸为299x299，且通常使用标准化到[-1,1] transform = transforms.Compose([ transforms.Resize((299, 299)), transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) ]) ####################### # 加载生成的图像 (fake_images) ####################### def load_fake_images_from_folder(folder, transform): images = [] for filename in os.listdir(folder): if filename.lower().endswith(('png','jpg','jpeg')): img_path = os.path.join(folder, filename) img = Image.open(img_path).convert('RGB') img = transform(img) images.append(img) # 将所有图像合并为一个Tensor: (N, C, H, W) if len(images) \u003e 0: images = torch.stack(images, dim=0) else: images = torch.empty(0) # 如果没有图像则返回空tensor return images fake_folder = \"/openbayes/home/miyan/works/Diffusion-Model-0-1/output/samples\" fake_images = load_fake_images_from_folder(fake_folder, transform) # (N,3,299,299) is_mean, is_std = calculate_inception_score(fake_images, device) print(\"IS:\", is_mean, is_std) FID评估 使用以下代码进行FID指标的测量：\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2): \"\"\" 计算Fréchet Inception Distance所需的Fréchet距离。 参数: mu1, sigma1: 实际数据特征均值和协方差矩阵 mu2, sigma2: 生成数据特征均值和协方差矩阵 返回: fid: FID分数 \"\"\" diff = mu1 - mu2 diff_sq = diff.dot(diff) # 计算矩阵的对称矩阵平方根 covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False) if np.iscomplexobj(covmean): covmean = covmean.real fid = diff_sq + np.trace(sigma1 + sigma2 - 2 * covmean) return fid def calculate_fid(real_features, fake_features): \"\"\" 计算 Frechet Inception Distance (FID)。 参数: real_features: np.ndarray, shape (N, 2048)，真实图像特征 fake_features: np.ndarray, shape (M, 2048)，生成图像特征 返回: fid: FID分数（越低越好） \"\"\" mu_real = np.mean(real_features, axis=0) sigma_real = np.cov(real_features, rowvar=False) mu_fake = np.mean(fake_features, axis=0) sigma_fake = np.cov(fake_features, rowvar=False) fid = calculate_frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake) return fid transform_img = transforms.Compose([ transforms.Resize((299, 299)), transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) ]) dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\") def transform(examples): images = [transform_img(image.convert(\"RGB\")) for image in examples[\"image\"]] return {\"img\": images} dataset.set_transform(transform) real_loader = DataLoader(dataset, batch_size=32, shuffle=False) inception = models.inception_v3(pretrained=True, transform_input=True).to(device) inception.eval() # 计算FID需要real_features和fake_features（需要先提取特征） # 可通过迭代real_loader对真实数据提取特征： real_features_list = [] with torch.no_grad(): for batch in real_loader: imgs = batch[\"img\"] imgs = imgs.to(device) feats = inception(imgs) real_features_list.append(feats.cpu().numpy()) real_features = np.concatenate(real_features_list, axis=0) # 对 fake_images 同样提取特征 fake_features = [] with torch.no_grad(): for i in range(0, len(fake_images), 32): batch = fake_images[i:i+32].to(device) feats = inception(batch) fake_features.append(feats.cpu().numpy()) fake_features = np.concatenate(fake_features, axis=0) fid_score = calculate_fid(real_features, fake_features) print(\"FID:\", fid_score) 最终经测量得出的IS和FID指标如下：\nIS (mean ± std) FID 2.3693223±0.2305675 606.8871224099385 条件生成 Conditioned Generation 在使用带有条件信息（如文本描述）的图像训练生成模型时，通常会生成以类标签或者一段描述性文本为条件的样本。主要方法包括分类器引导的扩散 (Classifier Guided Diffusion) 和无分类器引导的扩散 (Classifier-Free Guidance)。\nClassifier Guided Diffusion 为了将类别信息明确地加入传播过程中，通过在含有噪声的图像$x_t$上训练一个分类器$p_\\phi(y | \\mathbf{x}_t)$ 并且使用梯度 $\\nabla_{\\mathbf{x}_t} \\log p_\\phi(y | \\mathbf{x}_t)$引导采样过程朝向调节信息y（如标签信息或描述文本）方向预测噪声。 无条件的噪声预测器使用以下公式进行噪声的预测和去噪过程：\n$$ \\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(\\mathbf{x}_t, t) \\right) + \\sigma_t \\mathbf{z}, $$ 其中，$\\epsilon_\\theta(\\mathbf{x}_t, t)$是无条件的噪声预测器。 分类器引导的扩散模型反向采样公式为：\n$$ \\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta^{\\text{guided}}(\\mathbf{x}_t, t, y) \\right) + \\sigma_t \\mathbf{z}. $$ 对于条件引导的噪声预测器与无条件噪声预测器的关系，有： $$ \\epsilon_\\theta^{\\text{guided}}(\\mathbf{x}_t, t, y) = \\epsilon_\\theta(\\mathbf{x}_t, t) - w \\cdot \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\nabla_{\\mathbf{x}_t} \\log p_\\phi(y | \\mathbf{x}_t), $$ 其中，$\\epsilon_\\theta(\\mathbf{x}_t, t)$ 是无条件噪声预测器，$\\nabla_{\\mathbf{x}_t} \\log p_\\phi(y | \\mathbf{x}_t)$ 是分类器对类别 $y$ 的梯度，$w$是分类器引导强度的调节参数，$\\sqrt{1 - \\bar{\\alpha}_t}$ 用于将梯度项映射到噪声空间。 Classifier-Free Guidance 没有独立的分类器$p_\\phi(y | \\mathbf{x}_t)$ ，可以通过合并条件和非条件引导的扩散步骤来实现无分类器引导的条件扩散。为了避免训练一个显式的分类器 $p_\\theta(y | \\mathbf{x}_t)$，Classifier-Free Guidance通过直接学习两个噪声预测器来实现条件生成。 条件噪声预测器$\\epsilon_\\theta(\\mathbf{x}_t, t, y)$：基于目标条件 y 的噪声预测器。 无条件噪声预测器 $\\epsilon_\\theta(\\mathbf{x}_t, t)$：不依赖任何条件的噪声预测器。 通过线性组合条件和无条件噪声预测器，可以构造出一种增强条件生成效果的噪声预测器：\n$$ \\epsilon_\\theta^{\\text{guided}}(\\mathbf{x}_t, t, y) = \\epsilon_\\theta(\\mathbf{x}_t, t) + w \\cdot \\left(\\epsilon_\\theta(\\mathbf{x}_t, t, y) - \\epsilon_\\theta(\\mathbf{x}_t, t)\\right), $$ 其中：$w \\geq 1$ 是引导强度（通常称为“放大系数”），$\\epsilon_\\theta(\\mathbf{x}_t, t, y) - \\epsilon_\\theta(\\mathbf{x}_t, t)$ 表示条件信息对噪声预测的增量。 在训练过程中，无条件和条件噪声预测器通过单个神经网络进行学习，其中条件信息$y$被定期丢弃，以便模型知道如何无条件的生成图像，即\n$$ \\epsilon_\\theta(\\mathbf{x}_t, t)= \\epsilon_\\theta(\\mathbf{x}_t, t, \\tilde{y}) \\\\ \\tilde{y} = \\begin{cases} y, \u0026 \\text{with probability } 1 - p_\\text{drop}, \\\\ \\varnothing, \u0026 \\text{with probability } p_\\text{drop}. \\end{cases} $$ 潜在扩散模型 (Latent Diffusion Model) 潜在扩散模型通过在潜空间而不是像素空间运行扩散过程，从而降低训练成本并加快推理速度。模型发现的动机是观察到图像的大多数位置对感知细节都有着帮助，而且语义和概念信息经过压缩后依然存在。LDM通过使用自动编码器将信息编码到潜在空间，然后在潜在扩散过程中生成语义概念。\n首先，给定高维数据$x_0$（如图像），通过一个预训练的自动编码器将其映射到潜在空间：\n$$ \\mathbf{z}_0 = E(\\mathbf{x}_0), $$ 扩散和去噪过程都发生在潜在空间中，去噪模型增加了交叉注意力机制，用于处理用于图像生成的灵活条件信息（如类标签、语义信息等）。该设计相当于使用交叉注意力机制将不同模态的信息表示融合到模型中。每种信息都与特定的编码器 $\\tau_\\theta$配对，对于条件输入$y$，潜空间中的条件为$\\tau_\\theta(y)$： $$ \\mathbf{Attention} = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V}, \\\\ where \\ \\mathbf{Q}=\\mathbf{W_Q}\\varphi(z), \\mathbf{K}=\\mathbf{W_K}\\tau_\\theta(y), \\mathbf{V}=\\mathbf{W_V}\\tau_\\theta(y) $$ Latent Diffusion Model 的结构，图引自High-Resolution Image Synthesis with Latent Diffusion Models 使用稳定扩散模型进行条件生成 训练参数 由于重新训练一个大规模的稳定扩散模型非常困难，因此使用图像-文本对数据集对预训练模型进行微调，来测试模型的生成效果。在stable-diffusion-v1-4的预训练权重上进行训练，采用lambdalabs/naruto-blip-captions（《火影忍者》中各个角色的图像-文本对）数据集进行训练。\n训练参数的设置如下：\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\" export dataset_name=\"lambdalabs/naruto-blip-captions\" accelerate launch --mixed_precision=\"fp16\" train_text_to_image.py \\ --pretrained_model_name_or_path=$MODEL_NAME \\ --dataset_name=$dataset_name \\ --use_ema \\ --resolution=512 --center_crop --random_flip \\ --train_batch_size=1 \\ --gradient_accumulation_steps=4 \\ --gradient_checkpointing \\ --checkpointing_steps=5000 \\ --max_train_steps=15000 \\ --learning_rate=1e-05 \\ --max_grad_norm=1 \\ --enable_xformers_memory_efficient_attention \\ --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\ --output_dir=\"sd-naruto-model\" 其中一些重要的参数有：\n--pretrain_model_name_or_path：Hub上的模型名称或预训练模型的本地路径 --dataset_name：Hub上的数据集名称或本地数据集路径 --output_dir：训练模型的保存位置 设置Stable Diffusion的各种组成结构 由于需要使用文本作为条件进行生成，因此需要tokenizer和text encoder，将文本tokenize为一些tokens，再将tokens经过text encoder变为768维的embeddings。\ntokenizer、noise_scheduler、text_encoder、vae和U-Net的导入如下：\n# Load scheduler, tokenizer and models. noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\") tokenizer = CLIPTokenizer.from_pretrained( args.pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=args.revision ) # Load text encoder and vae with ContextManagers(deepspeed_zero_init_disabled_context_manager()): text_encoder = CLIPTextModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision, variant=args.variant ) vae = AutoencoderKL.from_pretrained( args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision, variant=args.variant ) # Load U-Net unet = UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.non_ema_revision ) 训练过程只调整U-Net的权重，冻结vae和text encoder的权重。\nvae.requires_grad_(False) text_encoder.requires_grad_(False) unet.train() 定义优化器用于优化U-Net的权重：\noptimizer = optimizer_cls( unet.parameters(), lr=args.learning_rate, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay, eps=args.adam_epsilon, ) 训练过程与DDPM部分流程一致，加入了将图像编码到潜在空间以及将文本tokens编码为embeddings的过程，其代码实现如下：\n# Convert images to latent space latents =vae.encode(batch[\"pixel_values\"].to(weight_dtype)).latent_dist.sample() latents = latents * vae.config.scaling_factor # Get the text embedding for conditioning encoder_hidden_states = text_encoder(batch[\"input_ids\"], return_dict=False)[0] 损失函数如下：\n# Predict the noise residual and compute loss model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0] # Loss loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\") 可以看到，噪声预测的过程中需要输入含有噪声的图像、时间步以及生成条件（文本编码的embeddings）。\n完整代码位于Github仓库。使用bash train.sh启动训练。\n训练与推理结果 在训练过程中加入Validation，可以看到在图像-文本对数据的训练下，生成图像的结果向着条件偏移，如下图中以Yoda和dog为prompt生成的图像。\n训练完毕后，经过推理能够生成一些具有《火影忍者》画面特征的图像，如下图所示。\n从推理结果中可以看出，U-Net经过Fine-tune之后，能够将预测噪声和去噪过程向着条件（《火影忍者》画面元素和画风）的方向进行引导，并且生成过程中受到文本信息的引导。\n总结 扩散模型作为生成模型的一种，通过正向和反向的扩散过程，实现了噪声预测和去噪，进而实现了从噪声中生成图像的功能。条件扩散模型通过将条件信息加入噪声预测器中，使得去噪过程能够在条件引导下进行。潜在扩散模型通过将信息编码到潜在空间以及在U-Net中加入交叉注意力机制，实现了模态的对齐以及更加高效的扩散性能。\nCitation 文章部分内容来自\nWeng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.\n文章代码开源在\nhttps://github.com/M1YAN/Diffusion-Model-0-1\n",
  "wordCount" : "7246",
  "inLanguage": "zh",
  "image": "https://m1yan.github.io/images/papermod-cover.png","datePublished": "2024-12-07T00:00:00Z",
  "dateModified": "2024-12-07T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Mi Yan"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://m1yan.github.io/posts/diffusion-model/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "M1YAN's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://m1yan.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://m1yan.github.io/" accesskey="h" title="M1YAN&#39;s Blog (Alt + H)">M1YAN&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://m1yan.github.io/en/" title="English"
                            aria-label="English">English</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://m1yan.github.io/archives" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="https://m1yan.github.io/search/" title="搜索">
                    <span>搜索</span>
                </a>
            </li>
            <li>
                <a href="https://m1yan.github.io/tags/" title="标签">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://m1yan.github.io/">主页</a>&nbsp;»&nbsp;<a href="https://m1yan.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      What are Diffusion Models?
    </h1>
    <div class="post-meta"><span title='2024-12-07 00:00:00 +0000 UTC'>十二月 7, 2024</span>&nbsp;·&nbsp;15 分钟&nbsp;·&nbsp;Mi Yan&nbsp;|&nbsp;<a href="https://github.com/M1YAN/M1YAN.github.io/tree/main/posts/posts/diffusion-model.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b" aria-label="生成模型">生成模型</a></li>
                <li>
                    <a href="#%e4%bb%80%e4%b9%88%e6%98%af%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b" aria-label="什么是扩散模型？">什么是扩散模型？</a><ul>
                        
                <li>
                    <a href="#%e5%89%8d%e5%90%91%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b" aria-label="前向扩散过程">前向扩散过程</a></li>
                <li>
                    <a href="#%e9%80%86%e5%90%91%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b" aria-label="逆向扩散过程">逆向扩散过程</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%ae%9e%e7%8e%b0%e4%b8%80%e4%b8%aaddpm" aria-label="实现一个DDPM">实现一个DDPM</a><ul>
                        
                <li>
                    <a href="#%e5%8f%82%e6%95%b0%e9%85%8d%e7%bd%ae" aria-label="参数配置">参数配置</a></li>
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87" aria-label="训练数据准备">训练数据准备</a></li>
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8u-net%e8%bf%9b%e8%a1%8c%e5%99%aa%e5%a3%b0%e9%a2%84%e6%b5%8b" aria-label="使用U-Net进行噪声预测">使用U-Net进行噪声预测</a></li>
                <li>
                    <a href="#ddpm-scheduler" aria-label="DDPM Scheduler">DDPM Scheduler</a></li>
                <li>
                    <a href="#%e5%88%9b%e5%bb%ba%e4%bc%98%e5%8c%96%e5%99%a8%e5%92%8c%e5%ad%a6%e4%b9%a0%e7%8e%87%e8%b0%83%e5%ba%a6%e5%99%a8" aria-label="创建优化器和学习率调度器">创建优化器和学习率调度器</a></li>
                <li>
                    <a href="#%e8%ae%be%e8%ae%a1%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0" aria-label="设计损失函数">设计损失函数</a></li>
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e8%bf%87%e7%a8%8b" aria-label="训练过程">训练过程</a></li>
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%8e%a8%e7%90%86%e7%bb%93%e6%9e%9c" aria-label="训练与推理结果">训练与推理结果</a></li>
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8is%e5%92%8cfid%e6%8c%87%e6%a0%87%e8%bf%9b%e8%a1%8c%e5%9b%be%e5%83%8f%e8%b4%a8%e9%87%8f%e8%af%84%e4%bc%b0" aria-label="使用IS和FID指标进行图像质量评估">使用IS和FID指标进行图像质量评估</a><ul>
                        
                <li>
                    <a href="#%e4%bb%80%e4%b9%88%e6%98%afis-inception-score-" aria-label="什么是IS (Inception Score) ?">什么是IS (Inception Score) ?</a></li>
                <li>
                    <a href="#%e4%bb%80%e4%b9%88%e6%98%affid-frechet-inception-distance-" aria-label="什么是FID (Frechet Inception Distance) ?">什么是FID (Frechet Inception Distance) ?</a></li>
                <li>
                    <a href="#is%e8%af%84%e4%bc%b0" aria-label="IS评估">IS评估</a></li>
                <li>
                    <a href="#fid%e8%af%84%e4%bc%b0" aria-label="FID评估">FID评估</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e6%9d%a1%e4%bb%b6%e7%94%9f%e6%88%90-conditioned-generation" aria-label="条件生成 Conditioned Generation">条件生成 Conditioned Generation</a><ul>
                        
                <li>
                    <a href="#classifier-guided-diffusion" aria-label="Classifier Guided Diffusion">Classifier Guided Diffusion</a></li>
                <li>
                    <a href="#classifier-free-guidance" aria-label="Classifier-Free Guidance">Classifier-Free Guidance</a></li>
                <li>
                    <a href="#%e6%bd%9c%e5%9c%a8%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b-latent-diffusion-model" aria-label="潜在扩散模型 (Latent Diffusion Model)">潜在扩散模型 (Latent Diffusion Model)</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%bd%bf%e7%94%a8%e7%a8%b3%e5%ae%9a%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b%e8%bf%9b%e8%a1%8c%e6%9d%a1%e4%bb%b6%e7%94%9f%e6%88%90" aria-label="使用稳定扩散模型进行条件生成">使用稳定扩散模型进行条件生成</a><ul>
                        
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e5%8f%82%e6%95%b0" aria-label="训练参数">训练参数</a></li>
                <li>
                    <a href="#%e8%ae%be%e7%bd%aestable-diffusion%e7%9a%84%e5%90%84%e7%a7%8d%e7%bb%84%e6%88%90%e7%bb%93%e6%9e%84" aria-label="设置Stable Diffusion的各种组成结构">设置Stable Diffusion的各种组成结构</a></li>
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%8e%a8%e7%90%86%e7%bb%93%e6%9e%9c-1" aria-label="训练与推理结果">训练与推理结果</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a></li>
                <li>
                    <a href="#citation" aria-label="Citation">Citation</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Update: 增加了条件生成以及潜在扩散模型的介绍。</p>
</blockquote>
<h2 id="生成模型">生成模型<a hidden class="anchor" aria-hidden="true" href="#生成模型">#</a></h2>
<p>目前主流的生成模型包括<strong>生成对抗模型 (GAN)</strong>、<strong>变分自编码器 (VAE)<strong>和</strong>基于流的模型 (Flow-based models)</strong>。</p>
<p>它们都能够生成较高质量的图像，但是也都具有一定的局限性。由于GAN模型具有对抗性训练的性质，因此其训练过程比较脆弱且难以稳定收敛，生成图像的多样性也较低。与GAN相比，VAE经常会生成较模糊、不够锐利的样本，因为VAE在优化过程中引入了KL散度正则项，鼓励潜变量分布与先验分布靠拢，因此会损失部分细节信息。基于流的生成模型通过严格的可逆变换实现对数据分布的精确密度估计，这意味着每一步变换需要是可逆且雅可比行列式可计算，因此在模型设计上对层结构有较强限制。</p>
<p>扩散模型的设计思路来自非平衡热力学。模型定义了一个马尔可夫扩散步骤，缓慢地向图像中添加随机噪声，然后学习扩散的逆过程以从噪声中构建所需要的数据样本。</p>
<div>			<!--块级封装-->
    <center>	<!--将图片和文字居中-->
        <a href="https://smms.app/image/nCgbWikxo9zf7Iq" target="_blank">
            <img src="https://s2.loli.net/2024/12/08/nCgbWikxo9zf7Iq.png">
        </a>
        <figcaption style="color: gray;">
            生成模型的结构，图引自
            <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">
                Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log.
            </a>
        </figcaption>
    </center>
</div>
<h2 id="什么是扩散模型">什么是扩散模型？<a hidden class="anchor" aria-hidden="true" href="#什么是扩散模型">#</a></h2>
<p>目前主流的基于扩散的生成模型包括<strong>扩散概率模型 (Diffusion Probabilistic Models)</strong>、<strong>条件噪声打分网络 (noise-conditioned score network)<strong>和</strong>去噪扩散概率模型 (denoising diffusion probabilistic models, DDPM)</strong>，扩散过程包括前向扩散过程和逆向扩散过程。</p>
<h3 id="前向扩散过程">前向扩散过程<a hidden class="anchor" aria-hidden="true" href="#前向扩散过程">#</a></h3>
<p>给定从真实数据分布中采样的数据点 $x_0$ ~ $q(x) $ ，定义一个前向扩散过程，在这个过程中，我们向样本中添加高斯噪声 $T$ 步，产生一系列含有噪声的样本 $ x_1, &hellip;, x_T $。
$$
q(x_t|x_{t-1})=\mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1},\beta_tI) \
q(x_{1:T}|x_0) = \prod_{t=1}^Tq(x_t|x_{t-1})
$$
随着加噪步数的增加，图像特征逐渐消失，最终当T趋近于无穷时，$x_T$ 相当于各向同性的高斯分布。</p>
<div>			<!--块级封装-->
    <center>	<!--将图片和文字居中-->
    <a href="https://smms.app/image/wOd7fAKGz3HLo6x" target="_blank"><img src="https://s2.loli.net/2024/12/08/wOd7fAKGz3HLo6x.png" alt="img2.png"></a>
		<figcaption>
        <font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
	  通过缓慢地添加（去除）噪声的正向（反向）扩散过程，图引自<a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log.</a>
	</font></font></figcaption>
	</center>
</div>
<p>上述采样过程能够使我们计算出在时间t时刻的采样$x_t$。令</p>
<div>
$$
\alpha_t=1-\beta_t, \overline{\alpha}_t = \prod_{i=1}^t\alpha_i \\
x_t = \sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1} \\
=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\epsilon_{t-2} \\
=... \\
=\sqrt{\overline{\alpha}_t}x_0+\sqrt{1-\overline{\alpha_t}}\epsilon
$$
</div>
<h3 id="逆向扩散过程">逆向扩散过程<a hidden class="anchor" aria-hidden="true" href="#逆向扩散过程">#</a></h3>
<p>为了实现从无序噪声恢复到数据分布（即反向扩散过程），需要对后验分布 $q(x_{t-1}|x_t, x_0)$ 进行分析。根据贝叶斯公式：</p>
<div>
$$
q(x_{t-1} \mid x_t, x_0) = \frac{q(x_t \mid x_{t-1}, x_0) q(x_{t-1} \mid x_0)}{q(x_t \mid x_0)}
$$
</div>
由于前向过程定义为条件独立的马尔可夫链，有：
<div>
$$
q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) \\
q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0, (1-\bar{\alpha}_{t-1})I)
$$
</div>
将以上分布代入后，可得到后验分布仍是高斯分布形式：
<div>
$$
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I)
$$
</div>
其中：
<div>
$$
\tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t, \\
\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t
$$
</div>
反向扩散过程的核心在于：如果我们能够对 $q(x{t-1} | x_t)$ 进行近似，就可以从纯噪声一步一步还原为原始数据分布。由于我们不知道$x_0$，我们希望有一个参数化的模型 $p\theta$ 来近似 $q(x_{t-1}|x_t,x_0)$：
<div>
$$
p_\theta(x_{t-1} \mid x_t) \approx q(x_{t-1} \mid x_t, x_0)
$$
</div>
若使用模型 $\epsilon_\theta(x_t,t)$ 来预测噪声，则可得到简化的逆扩散公式：
<div>
$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t)\right) 
+ \sqrt{\tilde{\beta}_t} z, \quad z \sim \mathcal{N}(0, I)
$$
</div>
t=1时省略最后的噪声项，最终可以得到$x_0$的样本。可以看到，扩散模型通过预测噪声$\epsilon_\theta$来重构之前时间步的样本。
<p>可以得到忽略加权项的简化目标来训练扩散模型，最终设计的损失函数如下：</p>
<div>
$$
L_{\text{simple}}(\theta) = \mathbb{E}_{t,x_0,\epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$
</div>
其中，
<div>
$$
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, I), \quad t \sim \text{Uniform}\{1,\ldots,T\}
$$
</div>
若考虑不同时间步的加权，可以定义加权损失：
<div>
$$
L(\theta) = \sum_{t=1}^{T} w_t \mathbb{E}_{x_0,\epsilon}\left[\| \epsilon - \epsilon_\theta(x_t, t)\|^2 \right]
$$
</div>
DDPM论文中的算法如下：
<p><img alt="img3.png" loading="lazy" src="https://s2.loli.net/2024/12/08/xBhvcql3Ja6rfPK.png"></p>
<h2 id="实现一个ddpm">实现一个DDPM<a hidden class="anchor" aria-hidden="true" href="#实现一个ddpm">#</a></h2>
<p>为了更清楚地了解扩散模型的整体架构，而不是为了探究复杂的概率论和数学原理，使用diffusers库实现DDPM的训练和推理。</p>
<img src="https://s2.loli.net/2024/12/07/laMoYsF6RZ7QGPy.png" alt="8DEECDF3-E90B-4CD9-9B9E-F7D168F6445F.png" style="zoom:50%;" />
<p>我们使用huggingface上的<a href="https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset">huggan/smithsonian_butterflies_subset</a>作为训练数据集，该数据集包含自然界各种各样的蝴蝶，可用于无条件的图像生成过程。</p>
<h3 id="参数配置">参数配置<a hidden class="anchor" aria-hidden="true" href="#参数配置">#</a></h3>
<p>首先按照以下配置进行训练和推理步骤的参数配置：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">32</span>
</span></span><span class="line"><span class="cl">    <span class="n">eval_batch_size</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># how many images to sample during evaluation</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl">    <span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span></span><span class="line"><span class="cl">    <span class="n">lr_warmup_steps</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_image_epochs</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">    <span class="n">save_model_epochs</span> <span class="o">=</span> <span class="mi">25</span>
</span></span><span class="line"><span class="cl">    <span class="n">mixed_precision</span> <span class="o">=</span> <span class="s2">&#34;fp16&#34;</span>  <span class="c1"># `no` for float32, `fp16` for automatic mixed precision</span>
</span></span><span class="line"><span class="cl">    <span class="n">output_dir</span> <span class="o">=</span> <span class="s2">&#34;output&#34;</span>  <span class="c1"># the model name locally and on the HF Hub</span>
</span></span><span class="line"><span class="cl">    <span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">config</span> <span class="o">=</span> <span class="n">TrainingConfig</span><span class="p">()</span>
</span></span></code></pre></div><h3 id="训练数据准备">训练数据准备<a hidden class="anchor" aria-hidden="true" href="#训练数据准备">#</a></h3>
<p>然后使用huggingface的datasets库进行数据集的下载和导入：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&#34;huggan/smithsonian_butterflies_subset&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>使用以下代码查看数据集中的图像：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">4</span><span class="p">][</span><span class="s2">&#34;image&#34;</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img alt="image-20241206234647023.png" loading="lazy" src="https://s2.loli.net/2024/12/07/eXvryQjgUIxGZNa.png"></p>
<p>使用torchvision库中的transforms模块，将图像的尺寸和数值归一化处理：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">preprocess</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="n">config</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">image_size</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">))</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&#34;image&#34;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span><span class="s2">&#34;img&#34;</span><span class="p">:</span> <span class="n">images</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([3, 128, 128])</span>
</span></span></code></pre></div><p>定义一个dataloader用于数据集的批量加载：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="使用u-net进行噪声预测">使用U-Net进行噪声预测<a hidden class="anchor" aria-hidden="true" href="#使用u-net进行噪声预测">#</a></h3>
<p>在扩散模型中，可以使用MLP或者U-Net来进行噪声的预测，从而将噪声一步一步去噪得到真实图像。选择U-Net作为噪声预测的模型，U-Net的架构由下采样堆栈和上采样堆栈构成。</p>
<ul>
<li>下采样：每个步骤包括重复应用两个 3x3 卷积（无填充卷积），每个卷积后跟一个 ReLU 和一个步幅为 2 的 2x2 最大池化。在每个下采样步骤中，特征通道的数量都会加倍。</li>
<li>上采样：每个步骤包括对特征图的上采样，然后进行 2x2 卷积，并且每次将特征通道数量减半。</li>
<li>捷径连接：上下采样堆栈相应层通过捷径连接，为上采样过程提供必要的高分辨率特征。</li>
</ul>
<p><img alt="img4.png" loading="lazy" src="https://s2.loli.net/2024/12/08/5MP6TLmQ8nVSFYh.png"></p>
<p>实现如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">UNet2DModel</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">UNet2DModel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">sample_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">image_size</span><span class="p">,</span>  <span class="c1"># the target image resolution</span>
</span></span><span class="line"><span class="cl">    <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># the number of input channels, 3 for RGB images</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># the number of output channels</span>
</span></span><span class="line"><span class="cl">    <span class="n">layers_per_block</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># how many ResNet layers to use per UNet block</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_out_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>  <span class="c1"># the number of output channels for each UNet block</span>
</span></span><span class="line"><span class="cl">    <span class="n">down_block_types</span><span class="o">=</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>  <span class="c1"># a regular ResNet downsampling block</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;AttnDownBlock2D&#34;</span><span class="p">,</span>  <span class="c1"># a ResNet downsampling block with spatial self-attention</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;DownBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">up_block_types</span><span class="o">=</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>  <span class="c1"># a regular ResNet upsampling block</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;AttnUpBlock2D&#34;</span><span class="p">,</span>  <span class="c1"># a ResNet upsampling block with spatial self-attention</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;UpBlock2D&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check input and output shapes</span>
</span></span><span class="line"><span class="cl"><span class="n">sample_image</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;img&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Input Shape&#34;</span><span class="p">,</span> <span class="n">sample_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Output Shape&#34;</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">sample_image</span><span class="p">,</span> <span class="n">timestep</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><p>通过检查输入U-Net和输出U-Net的图像形状，可以得知输入和预测噪声的形状一致，满足扩散模型的需求。</p>
<h3 id="ddpm-scheduler">DDPM Scheduler<a hidden class="anchor" aria-hidden="true" href="#ddpm-scheduler">#</a></h3>
<p>创建一个噪声调度器，用来在不同的时间步中为图像加噪。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Create a DDPM scheduler</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">DDPMScheduler</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">noise_scheduler</span> <span class="o">=</span> <span class="n">DDPMScheduler</span><span class="p">(</span><span class="n">num_train_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">sample_image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">50</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">noisy_image</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">add_noise</span><span class="p">(</span><span class="n">sample_image</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(((</span><span class="n">noisy_image</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)</span><span class="o">*</span><span class="mf">127.5</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</span></span></code></pre></div><p>从加噪后的输出可以看出，图像中出现了明显的噪声。</p>
<img src="https://s2.loli.net/2024/12/07/8htAoGfOK4JVwxg.png" alt="image-20241207000026865.png" style="zoom:150%;" />
<h3 id="创建优化器和学习率调度器">创建优化器和学习率调度器<a hidden class="anchor" aria-hidden="true" href="#创建优化器和学习率调度器">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Create optim and lr scheduler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">diffusers.optimization</span> <span class="kn">import</span> <span class="n">get_cosine_schedule_with_warmup</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_cosine_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                            <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_warmup_steps</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                            <span class="n">num_training_steps</span><span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="o">*</span><span class="n">config</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                            <span class="p">)</span>
</span></span></code></pre></div><h3 id="设计损失函数">设计损失函数<a hidden class="anchor" aria-hidden="true" href="#设计损失函数">#</a></h3>
<p>扩散模型的核心在于优化预测噪声的模型，因此需要使预测噪声的模型 (U-Net) 输出的噪声与实际噪声的分布接近。因此损失函数可以简单地设计为：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">noise_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="训练过程">训练过程<a hidden class="anchor" aria-hidden="true" href="#训练过程">#</a></h3>
<p>使用huggingface的accelerate库进行方便的模型加载、权重保存以及模型评估。训练的整体思路是生成图像不同时间步中加入噪声后的图像，U-Net接受加噪后的图像以及其对应的时间步，预测出该步骤加入的噪声。预测噪声与实际加入的噪声使用loss进行计算，最小化loss，进而使U-Net具有预测噪声的能力。最终在推理过程中能够使用U-Net在每个时间步进行去噪，最后生成接近真实分布的图像。训练循环代码如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_loop</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize accelerator</span>
</span></span><span class="line"><span class="cl">    <span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">mixed_precision</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">mixed_precision</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">project_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">&#34;logs&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">is_main_process</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">output_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="c1"># Prepare everything</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Train!</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">),</span> <span class="n">disable</span><span class="o">=</span><span class="ow">not</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">is_local_main_process</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">clean_images</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&#34;img&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Sample noise to add to the clean image</span>
</span></span><span class="line"><span class="cl">            <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">clean_images</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">bs</span> <span class="o">=</span> <span class="n">clean_images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Sample a random timestep for each image</span>
</span></span><span class="line"><span class="cl">            <span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="mi">0</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># forward diffusion process</span>
</span></span><span class="line"><span class="cl">            <span class="n">noisy_images</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="o">.</span><span class="n">add_noise</span><span class="p">(</span><span class="n">clean_images</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Predict Noise residual</span>
</span></span><span class="line"><span class="cl">                <span class="n">noise_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">sync_gradients</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">accelerator</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span> <span class="c1"># Gradient clipping</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">logs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;loss&#34;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s2">&#34;lr&#34;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;step&#34;</span><span class="p">:</span> <span class="n">global_step</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="o">**</span><span class="n">logs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">accelerator</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logs</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Evaluation</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">is_main_process</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span> <span class="o">=</span> <span class="n">DDPMPipeline</span><span class="p">(</span><span class="n">unet</span><span class="o">=</span><span class="n">accelerator</span><span class="o">.</span><span class="n">unwrap_model</span><span class="p">(</span><span class="n">model</span><span class="p">),</span> <span class="n">scheduler</span><span class="o">=</span><span class="n">noise_scheduler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">save_image_epochs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">evaluate</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">save_model_epochs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">pipeline</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">elif</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">pipeline</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;final&#34;</span><span class="p">))</span>
</span></span></code></pre></div><p>其中，模型评估的代码如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Evaluation</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">DDPMPipeline</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">diffusers.utils</span> <span class="kn">import</span> <span class="n">make_image_grid</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="o">.</span><span class="n">images</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_image_grid</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">rows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">&#34;test&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">image_grid</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;epoch_</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.png&#34;</span><span class="p">))</span>
</span></span></code></pre></div><h3 id="训练与推理结果">训练与推理结果<a hidden class="anchor" aria-hidden="true" href="#训练与推理结果">#</a></h3>
<p>最后使用以下代码在Jupyter Notebook中启动训练：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">rom</span> <span class="n">accelerate</span> <span class="kn">import</span> <span class="nn">notebook_launcher</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">notebook_launcher</span><span class="p">(</span><span class="n">train_loop</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">num_processes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>最终不同训练epoch的推理结果如下：</p>
<p><img alt="butterfly-result.png" loading="lazy" src="https://s2.loli.net/2024/12/07/2HUB9PVKhJZr6sN.png"></p>
<p>可以看出，随着训练步数的增加，生成的图像越来越向真实的图像分布（蝴蝶形态）靠拢，说明经过训练后，扩散模型具有了生成图像的能力。</p>
<h3 id="使用is和fid指标进行图像质量评估">使用IS和FID指标进行图像质量评估<a hidden class="anchor" aria-hidden="true" href="#使用is和fid指标进行图像质量评估">#</a></h3>
<h4 id="什么是is-inception-score-">什么是IS (Inception Score) ?<a hidden class="anchor" aria-hidden="true" href="#什么是is-inception-score-">#</a></h4>
<p>Inception Score 是一种对生成图像的质量和多样性进行评价的指标。其思路是利用一个预训练好的分类模型（通常是 Inception v3）对生成的图像进行分类，然后根据分类结果的分布来计算得分。</p>
<p>$p(y|x)$ 为给定生成图像 $x$ 的类别分布，$p(y) = \int p(y|x) p(x) dx$ 为所有生成图像的平均类别分布，$KL(\cdot|\cdot)$ 为KL散度，则IS为：
$$
\text{IS} = \exp\left( \mathbb{E}_{x}\bigl[ KL(p(y \mid x) | p(y)) \bigr] \right)
$$</p>
<p>直观上：</p>
<ul>
<li>如果生成图像的质量高，则概率分布应该集中在某些明确的类上（即分布峰值较高，说明图像能够被轻松分类）</li>
<li>如果生成图像的多样性高，则概率分布应该均匀覆盖多个类别。</li>
</ul>
<p>综合来看，IS高时，说明生成图像既清晰又多样。</p>
<h4 id="什么是fid-frechet-inception-distance-">什么是FID (Frechet Inception Distance) ?<a hidden class="anchor" aria-hidden="true" href="#什么是fid-frechet-inception-distance-">#</a></h4>
<p>FID 用于衡量生成分布和真实数据分布在特征空间（通常是 Inception v3 的中间特征层）上的差异。与IS不同，FID需要真实样本和生成样本作为对比，关注两者之间的统计差异。</p>
<p>设真实数据特征分布为 $\mathcal{N}(\mu_r, \Sigma_r)$，生成数据特征分布为 $\mathcal{N}(\mu_g, \Sigma_g)$，则FID定义为两高斯分布的Fréchet距离：
$$
\text{FID}(\mu_r, \Sigma_r, \mu_g, \Sigma_g) = |\mu_r - \mu_g|^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right).
$$
其中，$\mu_r, \Sigma_r$ 为真实分布特征的均值和协方差，$\mu_g, \Sigma_g$ 为生成分布特征的均值和协方差，$\text{Tr}(\cdot)$ 为迹运算，$( \Sigma_r \Sigma_g )^{1/2}$ 为矩阵的对称正定平方根。</p>
<p>直观上：</p>
<ul>
<li>
<p>FID衡量的是两个高斯分布之间的Fréchet距离，当两组特征分布一致时，FID为0（理想情况下）。</p>
</li>
<li>
<p>如果生成图像质量越高越逼近真实分布，那么 $\mu_g \approx \mu_r$ 且 $\Sigma_g \approx \Sigma_r$，因此FID会很低。</p>
</li>
<li>
<p>如果生成图像与真实分布偏差大，分布统计差异明显，FID会较高。</p>
</li>
</ul>
<h4 id="is评估">IS评估<a hidden class="anchor" aria-hidden="true" href="#is评估">#</a></h4>
<p>测量IS和FID指标，需要的图像数量至少需要上万张，由于生图速度较慢，使用500张生成图像进行IS指标的测量。</p>
<p>首先使用以下代码进行sampling：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Generate samples</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">diffusers</span> <span class="kn">import</span> <span class="n">DDPMPipeline</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl"><span class="n">pipeline</span> <span class="o">=</span> <span class="n">DDPMPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&#34;/openbayes/home/miyan/works/Diffusion-Model-0-1/output/epoch_49&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">samples_num</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples_num</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">generator</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100000</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="o">.</span><span class="n">images</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">&#34;samples&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">image</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="n">epoch</span><span class="o">*</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">.png&#34;</span><span class="p">))</span>
</span></span></code></pre></div><p>使用以下代码进行IS指标的测量：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">sqrtm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_inception_score</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    计算 Inception Score (IS)。
</span></span></span><span class="line"><span class="cl"><span class="s2">    参数:
</span></span></span><span class="line"><span class="cl"><span class="s2">        images: torch.Tensor，形状为(N, C, H, W) 的生成图像
</span></span></span><span class="line"><span class="cl"><span class="s2">        device: torch.device，计算设备（CPU或GPU）
</span></span></span><span class="line"><span class="cl"><span class="s2">        batch_size: 批大小
</span></span></span><span class="line"><span class="cl"><span class="s2">        splits: 将生成的图片集分为几份计算IS
</span></span></span><span class="line"><span class="cl"><span class="s2">    返回:
</span></span></span><span class="line"><span class="cl"><span class="s2">        (is_mean, is_std): IS的均值和标准差
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载预训练的Inception v3模型，用于分类</span>
</span></span><span class="line"><span class="cl">    <span class="n">inception</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">inception_v3</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">inception</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">preds</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 分批次计算预测概率分布</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">batch</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Inception v3要求输入为299x299，如果 images 已经是此大小且已标准化则无需再次处理</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">inception</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">probs</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (N, 1000)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算IS</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">split_scores</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">splits</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">part</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">//</span> <span class="n">splits</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">//</span> <span class="n">splits</span><span class="p">),</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">part</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">part</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">            <span class="n">p_yx</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p_yx</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_yx</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_y</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">        <span class="n">split_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">is_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">split_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">split_scores</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">is_mean</span><span class="p">,</span> <span class="n">is_std</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#######################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 数据预处理Transform</span>
</span></span><span class="line"><span class="cl"><span class="c1">#######################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Inception v3预期输入尺寸为299x299，且通常使用标准化到[-1,1]</span>
</span></span><span class="line"><span class="cl"><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">299</span><span class="p">,</span> <span class="mi">299</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#######################</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 加载生成的图像 (fake_images)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#######################</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">load_fake_images_from_folder</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">transform</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">folder</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s1">&#39;png&#39;</span><span class="p">,</span><span class="s1">&#39;jpg&#39;</span><span class="p">,</span><span class="s1">&#39;jpeg&#39;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">img</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 将所有图像合并为一个Tensor: (N, C, H, W)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">images</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 如果没有图像则返回空tensor</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">images</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fake_folder</span> <span class="o">=</span> <span class="s2">&#34;/openbayes/home/miyan/works/Diffusion-Model-0-1/output/samples&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">fake_images</span> <span class="o">=</span> <span class="n">load_fake_images_from_folder</span><span class="p">(</span><span class="n">fake_folder</span><span class="p">,</span> <span class="n">transform</span><span class="p">)</span>  <span class="c1"># (N,3,299,299)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">is_mean</span><span class="p">,</span> <span class="n">is_std</span> <span class="o">=</span> <span class="n">calculate_inception_score</span><span class="p">(</span><span class="n">fake_images</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;IS:&#34;</span><span class="p">,</span> <span class="n">is_mean</span><span class="p">,</span> <span class="n">is_std</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="fid评估">FID评估<a hidden class="anchor" aria-hidden="true" href="#fid评估">#</a></h4>
<p>使用以下代码进行FID指标的测量：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_frechet_distance</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    计算Fréchet Inception Distance所需的Fréchet距离。
</span></span></span><span class="line"><span class="cl"><span class="s2">    参数:
</span></span></span><span class="line"><span class="cl"><span class="s2">        mu1, sigma1: 实际数据特征均值和协方差矩阵
</span></span></span><span class="line"><span class="cl"><span class="s2">        mu2, sigma2: 生成数据特征均值和协方差矩阵
</span></span></span><span class="line"><span class="cl"><span class="s2">    返回:
</span></span></span><span class="line"><span class="cl"><span class="s2">        fid: FID分数
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">diff</span> <span class="o">=</span> <span class="n">mu1</span> <span class="o">-</span> <span class="n">mu2</span>
</span></span><span class="line"><span class="cl">    <span class="n">diff_sq</span> <span class="o">=</span> <span class="n">diff</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算矩阵的对称矩阵平方根</span>
</span></span><span class="line"><span class="cl">    <span class="n">covmean</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sqrtm</span><span class="p">(</span><span class="n">sigma1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sigma2</span><span class="p">),</span> <span class="n">disp</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">iscomplexobj</span><span class="p">(</span><span class="n">covmean</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">covmean</span> <span class="o">=</span> <span class="n">covmean</span><span class="o">.</span><span class="n">real</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">fid</span> <span class="o">=</span> <span class="n">diff_sq</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">sigma1</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">covmean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">fid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">calculate_fid</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">fake_features</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    计算 Frechet Inception Distance (FID)。
</span></span></span><span class="line"><span class="cl"><span class="s2">    参数:
</span></span></span><span class="line"><span class="cl"><span class="s2">        real_features: np.ndarray, shape (N, 2048)，真实图像特征
</span></span></span><span class="line"><span class="cl"><span class="s2">        fake_features: np.ndarray, shape (M, 2048)，生成图像特征
</span></span></span><span class="line"><span class="cl"><span class="s2">    返回:
</span></span></span><span class="line"><span class="cl"><span class="s2">        fid: FID分数（越低越好）
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mu_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">mu_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fake_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sigma_fake</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">fake_features</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">fid</span> <span class="o">=</span> <span class="n">calculate_frechet_distance</span><span class="p">(</span><span class="n">mu_real</span><span class="p">,</span> <span class="n">sigma_real</span><span class="p">,</span> <span class="n">mu_fake</span><span class="p">,</span> <span class="n">sigma_fake</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">fid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">transform_img</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">299</span><span class="p">,</span> <span class="mi">299</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&#34;huggan/smithsonian_butterflies_subset&#34;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&#34;train&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">transform_img</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&#34;RGB&#34;</span><span class="p">))</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&#34;image&#34;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span><span class="s2">&#34;img&#34;</span><span class="p">:</span> <span class="n">images</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">dataset</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">real_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">inception</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">inception_v3</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">inception</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 计算FID需要real_features和fake_features（需要先提取特征）</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 可通过迭代real_loader对真实数据提取特征：</span>
</span></span><span class="line"><span class="cl"><span class="n">real_features_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">real_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">imgs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&#34;img&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">feats</span> <span class="o">=</span> <span class="n">inception</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">real_features_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feats</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">real_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">real_features_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 对 fake_images 同样提取特征</span>
</span></span><span class="line"><span class="cl"><span class="n">fake_features</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake_images</span><span class="p">),</span> <span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch</span> <span class="o">=</span> <span class="n">fake_images</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">32</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">feats</span> <span class="o">=</span> <span class="n">inception</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">fake_features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feats</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">fake_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">fake_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fid_score</span> <span class="o">=</span> <span class="n">calculate_fid</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">fake_features</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;FID:&#34;</span><span class="p">,</span> <span class="n">fid_score</span><span class="p">)</span>
</span></span></code></pre></div><p>最终经测量得出的IS和FID指标如下：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">IS (mean ± std)</th>
          <th style="text-align: center">FID</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">2.3693223±0.2305675</td>
          <td style="text-align: center">606.8871224099385</td>
      </tr>
  </tbody>
</table>
<h2 id="条件生成-conditioned-generation">条件生成 Conditioned Generation<a hidden class="anchor" aria-hidden="true" href="#条件生成-conditioned-generation">#</a></h2>
<p>在使用带有条件信息（如文本描述）的图像训练生成模型时，通常会生成以类标签或者一段描述性文本为条件的样本。主要方法包括分类器引导的扩散 (Classifier Guided Diffusion) 和无分类器引导的扩散 (Classifier-Free Guidance)。</p>
<h3 id="classifier-guided-diffusion">Classifier Guided Diffusion<a hidden class="anchor" aria-hidden="true" href="#classifier-guided-diffusion">#</a></h3>
<div>
为了将类别信息明确地加入传播过程中，通过在含有噪声的图像$x_t$上训练一个分类器$p_\phi(y | \mathbf{x}_t)$ 并且使用梯度 $\nabla_{\mathbf{x}_t} \log p_\phi(y | \mathbf{x}_t)$引导采样过程朝向调节信息y（如标签信息或描述文本）方向预测噪声。
</div>
<p>无条件的噪声预测器使用以下公式进行噪声的预测和去噪过程：</p>
<div>
$$
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(\mathbf{x}_t, t) \right) + \sigma_t \mathbf{z},
$$
</div>
其中，$\epsilon_\theta(\mathbf{x}_t, t)$是无条件的噪声预测器。
<p>分类器引导的扩散模型反向采样公式为：</p>
<div>
$$
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta^{\text{guided}}(\mathbf{x}_t, t, y) \right) + \sigma_t \mathbf{z}.
$$
</div>
对于条件引导的噪声预测器与无条件噪声预测器的关系，有：
<div>
$$
\epsilon_\theta^{\text{guided}}(\mathbf{x}_t, t, y) = \epsilon_\theta(\mathbf{x}_t, t) - w \cdot \sqrt{1 - \bar{\alpha}_t} \cdot \nabla_{\mathbf{x}_t} \log p_\phi(y | \mathbf{x}_t),
$$
</div>
其中，$\epsilon_\theta(\mathbf{x}_t, t)$ 是无条件噪声预测器，$\nabla_{\mathbf{x}_t} \log p_\phi(y | \mathbf{x}_t)$ 是分类器对类别 $y$ 的梯度，$w$是分类器引导强度的调节参数，$\sqrt{1 - \bar{\alpha}_t}$ 用于将梯度项映射到噪声空间。
<h3 id="classifier-free-guidance">Classifier-Free Guidance<a hidden class="anchor" aria-hidden="true" href="#classifier-free-guidance">#</a></h3>
<div>
没有独立的分类器$p_\phi(y | \mathbf{x}_t)$ ，可以通过合并条件和非条件引导的扩散步骤来实现无分类器引导的条件扩散。为了避免训练一个显式的分类器 $p_\theta(y | \mathbf{x}_t)$，Classifier-Free Guidance通过直接学习两个噪声预测器来实现条件生成。
</div>
<ol>
<li><strong>条件噪声预测器</strong>$\epsilon_\theta(\mathbf{x}_t, t, y)$：基于目标条件 y 的噪声预测器。</li>
<li><strong>无条件噪声预测器</strong> $\epsilon_\theta(\mathbf{x}_t, t)$：不依赖任何条件的噪声预测器。</li>
</ol>
<p>通过线性组合条件和无条件噪声预测器，可以构造出一种增强条件生成效果的噪声预测器：</p>
<div>
$$
\epsilon_\theta^{\text{guided}}(\mathbf{x}_t, t, y) = \epsilon_\theta(\mathbf{x}_t, t) + w \cdot \left(\epsilon_\theta(\mathbf{x}_t, t, y) - \epsilon_\theta(\mathbf{x}_t, t)\right),
$$
</div>
其中：$w \geq 1$ 是引导强度（通常称为“放大系数”），$\epsilon_\theta(\mathbf{x}_t, t, y) - \epsilon_\theta(\mathbf{x}_t, t)$ 表示条件信息对噪声预测的增量。
<p>在训练过程中，无条件和条件噪声预测器通过单个神经网络进行学习，其中条件信息$y$被定期丢弃，以便模型知道如何无条件的生成图像，即</p>
<div>
$$
\epsilon_\theta(\mathbf{x}_t, t)= \epsilon_\theta(\mathbf{x}_t, t, \tilde{y}) \\
\tilde{y} =
\begin{cases}
y, & \text{with probability } 1 - p_\text{drop}, \\
\varnothing, & \text{with probability } p_\text{drop}.
\end{cases}
$$
</div>
<h3 id="潜在扩散模型-latent-diffusion-model">潜在扩散模型 (Latent Diffusion Model)<a hidden class="anchor" aria-hidden="true" href="#潜在扩散模型-latent-diffusion-model">#</a></h3>
<p>潜在扩散模型通过在潜空间而不是像素空间运行扩散过程，从而降低训练成本并加快推理速度。模型发现的动机是观察到图像的大多数位置对感知细节都有着帮助，而且语义和概念信息经过压缩后依然存在。LDM通过使用自动编码器将信息编码到潜在空间，然后在潜在扩散过程中生成语义概念。</p>
<p>首先，给定高维数据$x_0$（如图像），通过一个预训练的自动编码器将其映射到潜在空间：</p>
<div>
$$
\mathbf{z}_0 = E(\mathbf{x}_0),
$$
</div>
扩散和去噪过程都发生在潜在空间中，去噪模型增加了交叉注意力机制，用于处理用于图像生成的灵活条件信息（如类标签、语义信息等）。该设计相当于使用交叉注意力机制将不同模态的信息表示融合到模型中。每种信息都与特定的编码器 $\tau_\theta$配对，对于条件输入$y$，潜空间中的条件为$\tau_\theta(y)$：
<div>
$$
\mathbf{Attention} = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right)\mathbf{V}, \\
where \ \mathbf{Q}=\mathbf{W_Q}\varphi(z), \mathbf{K}=\mathbf{W_K}\tau_\theta(y), \mathbf{V}=\mathbf{W_V}\tau_\theta(y)
$$
</div>
<div>			<!--块级封装-->
    <center>	<!--将图片和文字居中-->
    <a href="https://smms.app/image/LCFR5rwUh613JlS" target="_blank"><img src="https://s2.loli.net/2024/12/08/LCFR5rwUh613JlS.png" ></a>
		<figcaption><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
	  Latent Diffusion Model 的结构，图引自<a href="https://arxiv.org/abs/2112.10752" target="_blank">High-Resolution Image Synthesis with Latent Diffusion Models</a>
	</font></font></figcaption>
	</center>
</div>
<h2 id="使用稳定扩散模型进行条件生成">使用稳定扩散模型进行条件生成<a hidden class="anchor" aria-hidden="true" href="#使用稳定扩散模型进行条件生成">#</a></h2>
<h3 id="训练参数">训练参数<a hidden class="anchor" aria-hidden="true" href="#训练参数">#</a></h3>
<p>由于重新训练一个大规模的稳定扩散模型非常困难，因此使用图像-文本对数据集对预训练模型进行微调，来测试模型的生成效果。在<a href="https://huggingface.co/CompVis/stable-diffusion-v1-4">stable-diffusion-v1-4</a>的预训练权重上进行训练，采用<a href="https://huggingface.co/datasets/lambdalabs/naruto-blip-captions">lambdalabs/naruto-blip-captions</a>（《火影忍者》中各个角色的图像-文本对）数据集进行训练。</p>
<p>训练参数的设置如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">export</span> <span class="n">MODEL_NAME</span><span class="o">=</span><span class="s2">&#34;CompVis/stable-diffusion-v1-4&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">export</span> <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&#34;lambdalabs/naruto-blip-captions&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">accelerate</span> <span class="n">launch</span> <span class="o">--</span><span class="n">mixed_precision</span><span class="o">=</span><span class="s2">&#34;fp16&#34;</span>  <span class="n">train_text_to_image</span><span class="o">.</span><span class="n">py</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="err">$</span><span class="n">MODEL_NAME</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">dataset_name</span><span class="o">=</span><span class="err">$</span><span class="n">dataset_name</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">use_ema</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">resolution</span><span class="o">=</span><span class="mi">512</span> <span class="o">--</span><span class="n">center_crop</span> <span class="o">--</span><span class="n">random_flip</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">train_batch_size</span><span class="o">=</span><span class="mi">1</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">gradient_checkpointing</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">checkpointing_steps</span><span class="o">=</span><span class="mi">5000</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">max_train_steps</span><span class="o">=</span><span class="mi">15000</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-05</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">max_grad_norm</span><span class="o">=</span><span class="mi">1</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">enable_xformers_memory_efficient_attention</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">lr_scheduler</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span> <span class="o">--</span><span class="n">lr_warmup_steps</span><span class="o">=</span><span class="mi">0</span> \
</span></span><span class="line"><span class="cl">  <span class="o">--</span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&#34;sd-naruto-model&#34;</span>
</span></span></code></pre></div><p>其中一些重要的参数有：</p>
<ul>
<li><code>--pretrain_model_name_or_path</code>：Hub上的模型名称或预训练模型的本地路径</li>
<li><code>--dataset_name</code>：Hub上的数据集名称或本地数据集路径</li>
<li><code>--output_dir</code>：训练模型的保存位置</li>
</ul>
<h3 id="设置stable-diffusion的各种组成结构">设置Stable Diffusion的各种组成结构<a hidden class="anchor" aria-hidden="true" href="#设置stable-diffusion的各种组成结构">#</a></h3>
<p>由于需要使用文本作为条件进行生成，因此需要tokenizer和text encoder，将文本tokenize为一些tokens，再将tokens经过text encoder变为768维的embeddings。</p>
<p><code>tokenizer</code>、<code>noise_scheduler</code>、<code>text_encoder</code>、<code>vae</code>和<code>U-Net</code>的导入如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load scheduler, tokenizer and models.</span>
</span></span><span class="line"><span class="cl"><span class="n">noise_scheduler</span> <span class="o">=</span> <span class="n">DDPMScheduler</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&#34;scheduler&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">CLIPTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">args</span><span class="o">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&#34;tokenizer&#34;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">revision</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load text encoder and vae</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">ContextManagers</span><span class="p">(</span><span class="n">deepspeed_zero_init_disabled_context_manager</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">        <span class="n">text_encoder</span> <span class="o">=</span> <span class="n">CLIPTextModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">args</span><span class="o">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&#34;text_encoder&#34;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">revision</span><span class="p">,</span> <span class="n">variant</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">variant</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">vae</span> <span class="o">=</span> <span class="n">AutoencoderKL</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">args</span><span class="o">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&#34;vae&#34;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">revision</span><span class="p">,</span> <span class="n">variant</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">variant</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load U-Net</span>
</span></span><span class="line"><span class="cl"><span class="n">unet</span> <span class="o">=</span> <span class="n">UNet2DConditionModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">args</span><span class="o">.</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">subfolder</span><span class="o">=</span><span class="s2">&#34;unet&#34;</span><span class="p">,</span> 			<span class="n">revision</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">non_ema_revision</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><p>训练过程只调整U-Net的权重，冻结vae和text encoder的权重。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">vae</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">text_encoder</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">unet</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span></code></pre></div><p>定义优化器用于优化U-Net的权重：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">unet</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">        <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">adam_beta1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">adam_beta2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">weight_decay</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">adam_weight_decay</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">eps</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">adam_epsilon</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span></code></pre></div><p>训练过程与DDPM部分流程一致，加入了将图像编码到潜在空间以及将文本tokens编码为embeddings的过程，其代码实现如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Convert images to latent space</span>
</span></span><span class="line"><span class="cl"><span class="n">latents</span> <span class="o">=</span><span class="n">vae</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&#34;pixel_values&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">weight_dtype</span><span class="p">))</span><span class="o">.</span><span class="n">latent_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">latents</span> <span class="o">=</span> <span class="n">latents</span> <span class="o">*</span> <span class="n">vae</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">scaling_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get the text embedding for conditioning</span>
</span></span><span class="line"><span class="cl"><span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">text_encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&#34;input_ids&#34;</span><span class="p">],</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span></code></pre></div><p>损失函数如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Predict the noise residual and compute loss</span>
</span></span><span class="line"><span class="cl"><span class="n">model_pred</span> <span class="o">=</span> <span class="n">unet</span><span class="p">(</span><span class="n">noisy_latents</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">encoder_hidden_states</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Loss</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">model_pred</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>可以看到，噪声预测的过程中需要输入含有噪声的图像、时间步以及生成条件（文本编码的embeddings）。</p>
<p>完整代码位于<a href="https://github.com/M1YAN/Diffusion-Model-0-1">Github仓库</a>。使用<code>bash train.sh</code>启动训练。</p>
<h3 id="训练与推理结果-1">训练与推理结果<a hidden class="anchor" aria-hidden="true" href="#训练与推理结果-1">#</a></h3>
<p>在训练过程中加入Validation，可以看到在图像-文本对数据的训练下，生成图像的结果向着条件偏移，如下图中以<code>Yoda</code>和<code>dog</code>为prompt生成的图像。</p>
<p><img alt="image-20241208160844602.png" loading="lazy" src="https://s2.loli.net/2024/12/08/eH8a5i7vMJZRupQ.png"></p>
<p>训练完毕后，经过推理能够生成一些具有《火影忍者》画面特征的图像，如下图所示。</p>
<p><img alt="image-20241208161051797.png" loading="lazy" src="https://s2.loli.net/2024/12/08/DuEyeNclAHTsV8S.png"></p>
<p>从推理结果中可以看出，U-Net经过Fine-tune之后，能够将预测噪声和去噪过程向着条件（《火影忍者》画面元素和画风）的方向进行引导，并且生成过程中受到文本信息的引导。</p>
<h2 id="总结">总结<a hidden class="anchor" aria-hidden="true" href="#总结">#</a></h2>
<p>扩散模型作为生成模型的一种，通过正向和反向的扩散过程，实现了噪声预测和去噪，进而实现了从噪声中生成图像的功能。条件扩散模型通过将条件信息加入噪声预测器中，使得去噪过程能够在条件引导下进行。潜在扩散模型通过将信息编码到潜在空间以及在U-Net中加入交叉注意力机制，实现了模态的对齐以及更加高效的扩散性能。</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>文章部分内容来自</p>
<blockquote>
<p>Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a>.</p>
</blockquote>
<p>文章代码开源在</p>
<blockquote>
<p><a href="https://github.com/M1YAN/Diffusion-Model-0-1">https://github.com/M1YAN/Diffusion-Model-0-1</a></p>
</blockquote>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://m1yan.github.io/posts/diffusers-tutorials/">
    <span class="title">下一页 »</span>
    <br>
    <span>Diffusers Tutorials</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="M1YAN/M1YAN.github.io"
        data-repo-id="R_kgDOLdlkMA"
        data-category="Announcements"
        data-category-id="DIC_kwDOLdlkMM4Ck7BX"
        data-mapping="url"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© <a href="https://github.com/adityatelange/hugo-PaperMod/graphs/contributors">PaperMod Contributors</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>
<script src="https://immmmm.com/waterfall.min.js"></script>
<script src="https://immmmm.com/imgStatus.min.js"></script>
<script>
  document.addEventListener('DOMContentLoaded', () => {
    
    var photosAll = document.getElementsByTagName('gallery') || '';
    if(photosAll){
      for(var i=0;i < photosAll.length;i++){
        photosAll[i].innerHTML = '<div class="gallery-photos">'+photosAll[i].innerHTML+'</div>'
        var photosIMG = photosAll[i].getElementsByTagName('img')
        for(var j=0;j < photosIMG.length;j++){
          wrap(photosIMG[j], document.createElement('div'));
        }
      }
    }
    function wrap(el, wrapper) {
      wrapper.className = "gallery-photo";
      el.parentNode.insertBefore(wrapper, el);
      wrapper.appendChild(el);
    }
    
    let galleryPhotos = document.querySelectorAll('.gallery-photos') || ''
    if(galleryPhotos){
      imgStatus.watch('.gallery-photo img', function(imgs) {
        if(imgs.isDone()){
          for(var i=0;i < galleryPhotos.length;i++){
            waterfall(galleryPhotos[i]);
            let pagePhoto = galleryPhotos[i].querySelectorAll('.gallery-photo');
            for(var j=0;j < pagePhoto.length;j++){pagePhoto[j].className += " visible"};
          }
        }
      });
      window.addEventListener('resize', function () {
        for(var i=0;i < galleryPhotos.length;i++){
          waterfall(galleryPhotos[i]);
        }
      });
    }
  });
</script>
<script type="text/javascript" src="https://immmmm.com/view-image.js"></script>
<script>
  window.ViewImage && ViewImage.init('.gallery-photo img')
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
