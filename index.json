[{"content":"","permalink":"https://m1yan.github.io/slides/reasoning_model-05.14/","summary":"Overview of Efficient inference for Reasoning Model.","title":"Reasoning Model 05.14"},{"content":"","permalink":"https://m1yan.github.io/slides/reasoning_model-04.16/","summary":"Correctness of intermediate answers during reasoning.","title":"Reasoning Model 4.16"},{"content":"","permalink":"https://m1yan.github.io/slides/model_editing_4.9/","summary":"Repetition of output in Model Post-Editing and ThinkEdit Reading.","title":"Model Editing 4.9"},{"content":"","permalink":"https://m1yan.github.io/slides/model_editing-03.12/","summary":"Thoughts on MoE Model Editing.","title":"Model Editing 3.12"},{"content":"","permalink":"https://m1yan.github.io/slides/model_editing-02.26/","summary":"Overview of Model Editing, ROME and MEND.","title":"Model Editing 2.26"},{"content":" 全文为Weng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil’Log.的中文翻译版本。\n在过去几年里，扩散模型在图像合成方面显示出了优异的效果。现在，研究界已经开始着手一项更加艰巨的任务——将扩散模型用于视频生成。该任务可以看作是图像生成的超集，因为图像可以认为是1帧的视频，但是图像生成更加具有挑战性，原因如下：\n它对跨帧的时间一致性有额外的要求，这自然需要将更多的世界知识编码到模型中。（例如物体运动的物理规则） 与文本或者图像相比，收集大量高质量、高维度的视频数据更加困难，文本视频对的获取也更加困难和复杂。 从头开始建模一个视频生成模型 首先，让我们回顾一下设计和训练扩散视频模型的方法，这意味着我们不依赖预先训练的图像生成器。\n参数以及关于采样的相关知识 设 $x \\sim q_{real}$ 是从实际数据分布中采样的数据点。现在我们在时间上添加少量的高斯噪声，产生一系列噪声变化 $x$ ，表示为 ${\\textbf{z}_t | t = 1, …,T}$，噪声随着 $t$ 的增加而增加，最后一个 $q(\\textbf{z}_T) \\sim \\mathcal{N}(\\textbf{0}, \\textbf{I})$。噪声添加的前向过程是一个高斯过程。使用 $\\alpha_t, \\sigma_t$ 定义高斯过程的可微分噪声公式：\n$$ q(\\textbf{z}_t|\\textbf{x}) = \\mathcal{N}(\\textbf{z}_t; \\alpha_t\\textbf{x}, \\sigma_t^2\\textbf{I}) $$ 为了表示 $q(\\textbf{z}_t|\\textbf{z}_s), 0 \\leq s \u003c t \\leq T$，我们有： $$ \\begin{aligned} \\textbf{z}_t \u0026= \\alpha_t \\textbf{x} + \\sigma_t \\bf{\\epsilon}_t \\\\ \\textbf{z}_s \u0026= \\alpha_s \\textbf{x} + \\sigma_s \\bf{\\epsilon}_s \\\\ \\textbf{z}_t \u0026= \\frac{\\alpha_t}{\\alpha_s}\\bf{z}_t + \\sigma_t\\bf{\\epsilon}_t - \\frac{\\alpha_t\\sigma_s}{\\alpha_s}\\bf{\\epsilon}_s \\\\ Thus\\ q(\\bf{z}_t|\\bf{z}_s) \u0026= \\mathcal{N}(\\bf{z}_t; \\frac{\\alpha_t}{\\alpha_s}\\bf{z}_s, (1- \\frac{\\alpha_t^2\\sigma_s^2}{\\sigma_t^2\\alpha_s^2})\\sigma_t^2\\bf{I}) \\end{aligned} $$ 设对数信噪比为 $\\lambda_t = log[\\alpha_t^2 / \\sigma_t^2]$ ，我们可以将DDIM更新表示为：\n$$ q(\\bf{z}_t|\\bf{z}_s) = \\mathcal{N}(\\bf{z}_t, \\frac{\\alpha_t}{\\alpha_s}\\bf{z}_t, \\sigma_{t|s}^2\\bf{I}) \\ where \\ \\sigma_{t|s}^2 = (1-e^{\\lambda_t-\\lambda_s})\\sigma_t^2 $$ 有一个特殊的 $\\bf{v}$ 预测（$\\bf{v} = \\alpha_t\\bf{\\epsilon} - \\sigma_t\\bf{x}$ ）参数化，由Salimans和Ho（2022）提出。与参数化 $\\epsilon$ 相比，它被证明能够有助于避免视频生成中的偏色现象。 $\\bf{v}$ 参数化是通过角度坐标中的技巧得出的。首先，我们定义 $\\phi_t = arctan(\\sigma_t/\\alpha_t)$ ，因此我们有 $\\alpha_\\phi = cos\\phi, \\sigma_\\phi = sin\\phi, \\bf{z}_\\phi = cos\\phi \\bf{x} + sin\\phi \\bf{\\epsilon}$ 。$\\bf{z}_t$ 的速率可以表示为：\n$$ \\bf{v_t} = \\nabla_t \\bf{z}_t = \\frac{dcos\\phi}{d\\phi}\\bf{x}+\\frac{dsin\\phi}{d\\phi}\\bf{\\epsilon} = cos\\phi \\bf{\\epsilon} - sin\\phi \\bf{x} $$ 然后我们可以推断， $$ \\begin{aligned} \\sin \\phi \\mathbf{x} \u0026 =\\cos \\phi \\boldsymbol{\\epsilon}-\\mathbf{v}_{\\phi} \\\\ \u0026 =\\frac{\\cos \\phi}{\\sin \\phi}\\left(\\mathbf{z}_{\\phi}-\\cos \\phi \\mathbf{x}\\right)-\\mathbf{v}_{\\phi} \\\\ \\sin ^{2} \\phi \\mathbf{x} \u0026 =\\cos \\phi \\mathbf{z}_{\\phi}-\\cos ^{2} \\phi \\mathbf{x}-\\sin \\phi \\mathbf{v}_{\\phi} \\\\ \\mathbf{x} \u0026 =\\cos \\phi \\mathbf{z}_{\\phi}-\\sin \\phi \\mathbf{v}_{\\phi} \\\\ \\text { Similarly } \\boldsymbol{\\epsilon} \u0026 =\\sin \\phi \\mathbf{z}_{\\phi}+\\cos \\phi \\mathbf{v}_{\\phi} \\end{aligned} $$ 相应地，DDIM的更新规则也会更新： $$ \\begin{aligned} \\mathbf{z}_{\\phi_{s}} \u0026 =\\cos \\phi_{s} \\hat{\\mathbf{x}}_{\\theta}\\left(\\mathbf{z}_{\\phi_{t}}\\right)+\\sin \\phi_{s} \\hat{\\epsilon}_{\\theta}\\left(\\mathbf{z}_{\\phi_{t}}\\right) \\quad \\\\ \u0026; \\hat{\\mathbf{x}}_{\\theta}(.), \\hat{\\epsilon}_{\\theta}(.) \\text { are two models to predict } \\mathbf{x}, \\boldsymbol{\\epsilon} \\text { based on } \\mathbf{z}_{\\phi_{t}} \\\\ \u0026 =\\cos \\phi_{s}\\left(\\cos \\phi_{t} \\mathbf{z}_{\\phi_{t}}-\\sin \\phi_{t} \\hat{\\mathbf{v}}_{\\theta}\\left(\\mathbf{z}_{\\phi_{t}}\\right)\\right)+\\sin \\phi_{s}\\left(\\sin \\phi_{t} \\mathbf{z}_{\\phi_{t}}+\\cos \\phi_{t} \\hat{\\mathbf{v}}_{\\theta}\\left(\\mathbf{z}_{\\phi_{t}}\\right)\\right) \\\\ \u0026 =\\left(\\cos \\phi_{s} \\cos \\phi_{t}+\\sin \\phi_{s} \\sin \\phi_{t}\\right) \\mathbf{z}_{\\phi_{t}}+\\left(\\sin \\phi_{s} \\cos \\phi_{t}-\\cos \\phi_{s} \\sin \\phi_{t}\\right) \\hat{\\mathbf{v}}_{\\theta}\\left(\\mathbf{z}_{\\phi_{t}}\\right) \\\\ \u0026 =\\cos \\left(\\phi_{s}-\\phi_{t}\\right) \\mathbf{z}_{\\phi_{t}}+\\sin \\left(\\phi_{s}-\\phi_{t}\\right) \\hat{\\mathbf{v}}_{\\theta}\\left(\\mathbf{z}_{\\phi_{t}}\\right) \\quad \\\\ \u0026; \\text { trigonometric identity functions. } \\end{aligned} $$ 图1:可视化扩散更新步骤在角度坐标中的工作原理（图片来源：Salimans Ho, 2022) 模型的 $\\bf{v}$ 预测是预测 $\\bf{v}_\\phi=cos\\phi\\boldsymbol{\\epsilon} - sin\\phi\\bf{x} = \\alpha_t \\boldsymbol{\\epsilon} - \\sigma_t \\bf{x}$ 。\n在视频生成的情形中，我们需要扩散模型运行多个步骤的上采样，以延长视频长度或提高帧数。这需要对以第一个 $\\bf{x}^a$ 为条件的第二个图像 $\\bf{x}^b \\sim p_\\theta(\\bf{x}^b|\\bf{x}^a)$ 进行采样，其中 $\\bf{x}^b$ 可能是自回归扩展的帧或者低帧数视频中间缺失的帧。\n$\\bf{x}^b$ 的采样不仅需要自身对应的噪声变量外，还需要考虑 $\\bf{x}^a$ 条件。视频扩散模型（VDM; Ho \u0026amp; Salimans, 2022）提出了使用调整后的降噪模型的重建指导方法，使得采样 $\\bf{x}^b$ 可以适当地以以下基于 $\\bf{x}^a$ 的条件进行：\n$$ \\begin{align} \\mathbb{E}_q[\\mathbf{x}^b | \\mathbf{z}_t, \\mathbf{x}^a] \u0026= \\mathbb{E}_q[\\mathbf{x}^b | \\mathbf{z}_t] + \\frac{\\sigma_t^2}{\\alpha_t} \\nabla_{\\mathbf{z}_t^b} \\log q(\\mathbf{x}^a | \\mathbf{z}_t) \\\\ q(\\mathbf{x}^a | \\mathbf{z}_t) \u0026\\approx \\mathcal{N}\\left[\\hat{\\mathbf{x}}_\\theta^a(\\mathbf{z}_t), \\frac{\\sigma_t^2}{\\alpha_t^2} \\mathbf{I}\\right] \\\\ \\tilde{\\mathbf{x}}_\\theta^b(\\mathbf{z}_t) \u0026= \\hat{\\mathbf{x}}_\\theta^b(\\mathbf{z}_t) - \\frac{w_r \\alpha_t}{2} \\nabla_{\\mathbf{z}_t^b} \\left\\| \\mathbf{x}^a - \\hat{\\mathbf{x}}_\\theta^a(\\mathbf{z}_t) \\right\\|_2^2 \\end{align} $$ 其中，$\\hat{\\mathbf{x}}_\\theta^a(\\mathbf{z}_t)$ 和 $\\hat{\\mathbf{x}}_\\theta^b(\\mathbf{z}_t)$ 是由去噪模型 $\\bf{x}^a$ 和 $\\bf{x}^b$ 提供的重建图像。并且 $\\omega_r$ 是一个加权因子，发现较大的 $\\omega_r \u003e 1$ 可以提高样本质量。也可以使用相同的方法对低分辨率视频进行条件调节，将样本扩展到高分辨率。 模型架构：3D U-Net和DIT 与文本到图像的扩散模型类似，U-Net和Transformer仍然是两种常见的架构选择。Google有一系列采用U-Net架构的扩散视频论文，OpenAI最近的Sora模型采用了transformer架构。\nVDM（Ho \u0026amp; Salimans, et al. 2022）采用了标准的扩散模型设置，但是改变了视频建模的架构。它扩展了2D U-Net以用于3D数据 (Clcek et al. 2016) ，其中每一个特征图代表帧x高度x宽度x通道的4D张量。这个3D U-Net在空间和时间上被分解，这意味着每一层只在时间或者空间维度上起作用，而不能同时在两者上起作用。\n处理空间： 与2D U-Net相同，每个旧的2D卷积层都扩展为仅空间3D的卷积层，例如，3*3卷积变成了1*3*3卷积。 每个空间注意力块（spatial attention block）都保持为空间上的注意力，其中第一个轴（frame）被视为批量维度。 处理时间： 在每个空间注意力块之后添加一个时间注意力块（temporal attention block）。它在第一个轴（frame）上执行注意力，并将空间轴作为批处理维度。相对位置嵌入（relative position embedding）用于跟踪帧的顺序。时间注意力块对于模型捕获良好的时间连贯性非常重要。 图2:3D U-Net架构。 Imagen Video（Ho, et al. 2022）基于一系列扩散模型构建，以提高视频生成的质量，并升级以24fps输出1280*768的视频。Imagen Video架构由以下组件构成，总共有7个扩散模型。\n一个冻结的T5文本编码器，用于提供文本嵌入作为条件输入 一个基本的视频扩散模型 交错的时间和空间超分辨率扩散模型的级联，包括3个TSR（时间超分辨率）和3个SSR（空间超分辨率）组件。 图3:Imagen Video中的级联采样架构。在实践中，文本嵌入被注入到所有的组件中，而不仅仅是基本模型。 基本去噪模型同时对具有共享参数的所有帧执行空间操作，然后时间层跨帧混合激活以更好地捕获时间一致性，这被证明比帧自回归的方法效果更好。\n图4：Imagen Video扩散模型中块时空分离的架构。 SSR和TSR模型都以与通道上含有噪声的数据 $\\bf{z}_t$ 相连的上采样输入为条件。SSR通过双线性调整大小进行上采样，而TSR通过重复帧或者填充空白帧来上采样。\nImagen Video还应用渐进式蒸馏来加快采样速度，每次蒸馏迭代都可以将所需的采样步骤减少一半。在实验中，能够将7个组件提炼为每个组件仅8个采样步骤，而不会在感知质量上造成任何明显的损失。\n为了实现更好的扩展工作，Sora（Brooks et al. 2024）利用了DiT（Diffusion Transformer）架构，该架构在视频和图像潜在编码的时空patch上运行。视觉输入表示为一系列时空patch，这些patch充当Transformer的输入token。\n图5：Sora是Diffusion Transformer架构。 调整图像模型以生成视频 扩散视频模型建模的另一种突出方法是通过插入时间层来“膨胀”预先训练的图像到文本扩散模型，然后我们可以选择只对视频数据上的新层进行微调，或者完全避免额外的训练。新模型继承了文本-图像对的先验知识，有助于减轻对文本-视频对的数据需求。\n使用视频数据进行微调 Make-A-Video（Singer et al. 2022）扩展了具有时间维度的预训练扩散图像模型，该模型由3个关键组件构成：\n基于文本-图像对训练的基本图像生成模型。 时空卷积层和注意力层来扩展网络以覆盖时间维度。 用于生成高帧率的帧插值网络。 图6：Make-A-Video架构。 最终的视频推理方案可以表述为：\n$$ \\hat{\\bf{y}}_t = SR_h \\circ SR_l^t \\circ \\uparrow_F \\circ D^t \\circ P \\circ (\\hat{\\bf{x}}, CLIP_{text}(\\bf{x})) $$ 其中： $\\bf{x}$ 是输入文本。 $\\hat{\\bf{x}}$ 是BPE编码的文本。 $CLIP_{text}(\\cdot)$ 是CLIP文本编码器，则 $\\bf{x_e} = CLIP_{text}(\\bf{x})$ 。 $P(\\cdot)$ 是先验的，$\\bf{y}_e$ 在给定文本嵌入 $\\bf{x}_e$ 和BPE编码文本 $\\hat{\\bf{x}}: \\bf{y}_e = P(\\bf{x}_e, \\hat{\\bf{x}})$ 的情况下生成图像嵌入。这部分是在文本-图像对数据上训练的，而不是在视频数据上微调的。 $D^t(\\cdot)$ 是生成一系列16帧的时空解码器，其中每帧都是低分辨率的64*64的RGB图像 $\\hat{\\bf{y}}_l$。 $\\uparrow_F(\\cdot)$ 是帧插值网络，通过在生成的帧之间进行插值来提高有效帧速率。这是一个经过微调的模型，用于预测视频上采样的掩码帧。 $SR_h(\\cdot), SR_l^t(\\cdot)$ 是时间和空间超分辨率模型，将图像分辨率分别提高到256*256和768*768。 $\\hat{\\bf{y}}_t$ 是最终生成的视频。 时空SR层包括伪3D卷积层和伪3D注意力层：\n伪3D卷积层：每个时空2D卷积层（从预训练图像模型初始化）后跟一个时间1D层（初始化为一个恒等函数）。从概念上讲，2D卷积层首先生成多个帧，然后将帧重塑为视频片段。 伪3D注意力层：在每个预先训练的空间注意力层之后，堆叠一个时间注意力层，用于近似一个完整的时空注意力层。 图7：伪3D卷积层（左）和伪3D注意力层（右）的工作原理。 它们可以表示为：\n$$ \\begin{align} Conv_{P3D} = Conv_{1D}(Conv_{2D}(\\bf{h})\\circ T) \\circ T \\\\ Attn_{P3D} = flatten^{-1}((Atten_{2D}flatten(\\bf{h}) \\circ T) \\circ T) \\end{align} $$ 其中输入张量 $\\bf{h} \\in \\mathbb{R}^{B \\times C \\times F \\times H \\times W}$ （对应于批量大小、通道、帧、高度和宽度），$\\circ T$ 代表时间和空间维度之间的交换，即张量形状变为 $\\bf{h'} \\in \\mathbb{R}^{B \\times F \\times C \\times H \\times W}$ ；$flatten(\\cdot)$ 将 $\\bf{h}$ 转换为 $\\bf{h''} \\in \\mathbb{R}^{B \\times F \\times C \\times H W}$ ，$flatten^{-1}(\\cdot)$ 代表反转该过程。 在训练阶段，Make-A-Video管道的不同组件是独立训练的。\n解码器 $D^t$ 、生图先验 $P$ 和两个超分辨率组件 $SR_h, SR_l^t$ 首先单独在图像上进行训练，没有成对的文本。 接下来添加新的时间层，初始化为一个恒等函数，对未标记的视频数据进行微调。 Tune-A-Video（Wu et al. 2023）膨胀了一个预训练的图像扩散模型，以实现一次性的适应视频生成的微调：给定一个包含 $m$ 帧的视频，$\\mathcal{V} = {v_i|i=1,\u0026hellip;,m}$ 与描述性提示 $\\tau$ 配对，任务是基于轻微编辑和一个相关的文本描述 $\\tau$ 生成一个新的视频 $\\mathcal{V^*}$ 。例如：$\\tau$ = \u0026quot;A man is skiing\u0026quot;可以扩展为 $\\tau$ = \u0026quot;Spiderman is skiing on the beach\u0026quot;。Tune-A-Video旨在用于对象编辑、背景更改和风格迁移。\n除了膨胀2D卷积层外，Tune-A-Video的U-Net架构还集成了ST-Attention（时空注意力）块，通过查询前几帧的相关位置来捕获时间一致性。给定frame $v_i$ 、前一帧 $v_{i-1}$ 和第一帧 $v_1$ 的潜在特征，投射到query $\\bf{Q}$ 、key $\\bf{K}$ 、和value $\\bf{V}$ 中，则ST-Attention可以被定义为：\n$$ \\begin{align} \\bf{Q} \u0026= \\bf{W}^Q \\bf{z}_{v_i}, \\\\ \\bf{K} \u0026= W^K [\\bf{z}_{v_i}, \\bf{z}_{v_{i-1}}], \\\\ \\bf{V} \u0026= W^V [\\bf{z}_{v_i}, \\bf{z}_{v_{i-1}}] \\\\ \\bf{O} \u0026= softmax(\\frac{\\bf{Q}\\bf{K}^T}{\\sqrt{d}}) \\cdot \\bf{V} \\end{align} $$ 图8：Tune-A-Video架构概述。首先在采样阶段之前对单个视频运行轻量级微调阶段。由于整个时间自注意力 (T-Attn) 层是新添加的，因此会进行微调，但在微调期间，只有ST-Attn和Cross-Attn中的查询投影会更新，以保留先前的文本到图像的知识。ST-Attn提高了时空一致性，Cross-Attn优化了文本-视频的对齐。 Gen-1模型（Esser et al. 2023）的目标是根据文本输入编辑给定视频。它分解了视频生成条件 $p(\\bf{x}|s,c)$ 中对于结构structure和内容content的要求，但是，要对这两个方面进行清晰的分解并不容易。\n$\\bf{c}$ 指视频的外观和语义信息，即从文本中采样以进行条件编辑。帧的CLIP嵌入可以很好地表示内容，并且很大程度上与结构特征保持正交。 $\\bf{s}$ 描述网格和动力学，包括对象的形状、位置、时间变化，并从输入的视频中采样，可以使用深度信息或者针对于其他特定任务的信息（例如，人体姿态或者面部特征）。 Gen-1的架构变化相当标准，即在剩余块中的每个2D空间卷积层后添加1D时间卷积层，并在注意力块的每一个2D空间注意力块之后添加1D时间注意力块。在训练期间，结构变量 $\\bf{s}$ 与扩散潜空间中的变量 $\\bf{z}$ 连接，其中变量 $\\bf{c}$ 在交叉注意力层中提供。在推理时，在推理时，片段嵌入通过之前的 CLIP 文本嵌入转换为 CLIP 图像嵌入。\n图9：Gen-1模型架构。 Video LDM（Blattmann et al. 2023）首先训练了一个LDM图像生成器。然后，对模型进行微调，添加了时间维度以生成视频。微调仅应用于编码图像序列中新添加的时序层。Video LDM中的时间层 ${l_\\phi^i | i=1,\u0026hellip;L }$ 与在微调期间保持的现有空间层 $l_\\theta^i$ 交错，只微调新参数 $\\phi$ ，而不微调预训练主干模型参数 $\\theta$ 。Video LDM的pipeline首先以低fps生成关键帧，然后通过两步潜在帧插值进行处理以提高fps。\n长度 $T$ 的输入序列被看作基本图像生成模型 $\\theta$ 生成的一批图像（即 $\\bf{B} \\cdot \\bf{T}$ ），然后被重塑为时间层的视频 $l_\\phi^i$ 格式。存在一个跳跃连接，使通过学习的合并参数 $\\alpha$ 实现时间层输出 $\\bf{z\u0026rsquo;}$ 和空间输出 $\\bf{z}$ 的组合。在实验中，实现了两种类型的时间混合层：（1）单独的时间注意力层加在空间注意力层之后和（2）基于3D卷积的残差块按合并参数组合。\n图10：用于图像生成的预训练LDM扩展为视频生成器。B, T, C, H, W分别是批量大小、序列长度、通道数、高度和宽度。c是可选的生成条件。 但是，LDM预训练的自动编码器仍然存在一个问题，即只能看到图像，而看不到视频。单纯将其用于视频生成可能会产生闪烁伪影，而没有良好的时间一致性。因此，Video LDM在解码器中添加了额外的时间层，并且使用由3D卷积构建的基于patch的时间判别器对视频数据进行微调，而编码器保持不变，因而可以继续使用预训练的LDM。在时间解码器微调期间，冻结的编码器独立处理视频中的每个帧，并且使用视频感知判别器在帧之间强制执行时间一致的重建。\n图11：Video LDM时间解码器的训练架构。解码器经过微调，在编码器保持冻结状态时在跨帧判别器下具有时间一致性。 与Video LDM类似，Stable Video Diffusion（SVD, Blattmann et al. 2023）也基于LDM，在每个空间卷积层和注意力层后插入时间层，但SVD对整个模型进行了微调。训练SVD分为三个阶段：\n文本到图像的预训练 ：有助于提高视频质量以及提示的跟随。 视频预训练：分离训练更加有利，理想情况下应该在一个更大规模的精选的数据集上进行。 高质量视频微调：使用具有高视觉保真度的较小的、带有字幕的视频进行。 SVD特别强调了数据集管理在模型性能中的关键作用。他们通过使用剪辑检测架构在为每个模型获得更多的视频切片，然后应用了三种不同的用于生成字幕的模型：（1）CoCa用于挑选帧；（2）V-BLIP用于获取视频字幕；（3）基于前面两个模型，使用LLM生成字幕。然后他们继续优化视频数据集，通过删除运动较少的视频（通过以2fps计算的低光流分数过滤），视频中具有过多的文本（应用OCR识别具有大量文本的视频），具有较低的美学价值的视频（使用CLIP对视频的第一帧、中间一帧以及最后一帧提取特征，计算美学分数和文本-图像相似度）。实验表明，经过过滤的、高质量的数据集会带来更好的模型质量，即使这个数据集要小得多。\n先生成远距离的关键帧，然后添加具有时间超分辨率的帧插值，一个关键的挑战是如何保持高质量的时间一致性。Lumiere（Bar-Tal et al. 2024）采用了时空U-Net（STUNet）架构，通过单次传递一次性生成视频的整个持续时间，消除了对TSR（时间超分辨率）组件的依赖。STUNet在时间和空间维度上对视频进行下采样，因此大量的计算发生在时空潜在空间中。\n图12：Lumiere删除了TSR（时间超分辨率）模型。由于内存限制，膨胀的SSR网络只能在视频的短片段上运行，因此SSR模型在一组短且重叠的视频片段上运行。 STUNet对预训练的文本到图像的U-Net进行膨胀，以便能够在时间和空间维度上对视频进行下采样和上采样。卷积块由预先训练的文本到图像层构成，伴随着一系列的时空卷积。在U-Net的瓶颈特征处，包括预先训练的从文本到图像的注意力块，其中包含了1D的时间注意力。训练仅发生在新添加的层中。\n图13：（a）时空U-Net（STUNet），（b）基于卷积的块，（c）基于注意力的块。 免训练适配 令人惊讶的是，无需任何训练即可调整预先训练的文本到图像的生成模型以输出视频。\n如果我们天真地采样一系列潜在空间中的特征，然后解码出一个对应的视频，那么在时间上的对象和语义的一致性是无法保证的。Text2Video-Zero（Khachatryan et al. 2023）通过增强预训练的图像扩散模型，使用了两个关键的时间一致性机制实现了零样本、免训练的视频生成：\n使用运动动力学对潜在空间的序列进行采样，以保持全局场景一致。 在第一帧上通过每帧的新跨层注意力重新设计了帧级的自注意力，以保留前景对象的上下文、外观和身份。 图14：Text2Video-Zero架构概述。 使用运动信息对一系列潜空间变量 $\\bf{x}_T^1,\u0026hellip;,\\bf{x}_T^m$ 进行采样的过程如下：\n定义控制全局场景和照相机运动方向 $\\bf{\\delta} = (\\delta_x, \\delta_y) \\in \\mathbb{R}^2$ ；默认情况下，我们设置 $\\bf{\\delta} = (1, 1)$ 。此外，定义一个控制全局运动量的超参数 $\\lambda \u0026gt; 0$ 。 首先随机采样第一帧的潜空间变量，$\\bf{x}_T^1 \\sim \\mathcal{N}(0,I)$ ； 使用预训练的图像扩散模型执行后向更新步骤，例如论文中的稳定扩散（SD）模型，并获取相应的潜在空间变量 $\\bf{x}_{T\u0026rsquo;}^1$，其中 $T\u0026rsquo; = T - \\Delta t$ 。 对于潜在空间变量序列中的每一帧，我们应用相应的运动平移，并使用定义的 $\\delta ^k = \\lambda(k-1) \\delta$ 进行变形操作来获得 $\\bf{\\tilde{x}}_{T\u0026rsquo;}^k$ 。 5. 最后，将DDIM前向步骤应用于所有 $\\bf{\\tilde{x}}_{T'}^{2:m}$ 以获取 $\\bf{x}_{T}^{2:m}$ 。 $$ \\begin{align} \u0026 \\bf{x}_{T'}^1 = DDIM-backward(\\bf{x}_{T}^1, \\Delta t) where\\ T' = T - \\Delta t \\\\ \u0026 W_k \\leftarrow \\text{a warp operation of }\\delta^k = \\lambda(k-1)\\delta \\\\ \u0026 \\bf{\\tilde{x}}_{T'}^k = W_k(\\bf{x}_{T'}^1) \\\\ \u0026 \\bf{x}_{T}^k = DDIM-forward(\\bf{\\tilde{x}}_{T'}^k, \\Delta t)\\ for \\ k = 2,...,m \\end{align} $$ 此外，Text2Video-Zero将预训练SD模型中的自注意力层替换为参考第一帧的新跨帧注意力机制。其动机是在生成的整个视频中保留有关前景对象的外观、形状和身份信息。\n$$ \\bf{Cross-Frame-Attn}(\\bf(Q)^k, \\bf{K}^{1:m}, \\bf{V}^{1:m}) = Softmax(\\frac{\\bf{Q}^k(\\bf{K})^\\top}{\\sqrt{c}})\\bf{V}^1 $$ 下面的操作可供选择，可以使用背景蒙版进一步平滑和提高背景一致性。假设我们使用现有方法获得了第 $k$ 帧的前景掩码 $\\bf{M}_k$ ，使用背景平滑需要在扩散步骤 $t$ 中合并实际潜变量和扭曲的潜变量，即背景矩阵为： $$ \\mathbf{\\bar{x}}_t^k = \\mathbf{M}^k \\odot \\mathbf{x}_t^k + (1-\\mathbf{M}^k) \\odot (\\alpha \\mathbf{\\tilde{x}}_t^k + (1-\\alpha)\\mathbf{x}_t^k) \\text{ for k =1,...,m} $$ 其中 $\\bf{x}_t^k$ 是实际的潜空间变量，$\\bf{\\tilde{x}}_t^k$ 是在背景上被变形的潜空间变量；$\\alpha$ 是一个超参数，实验中设置了 $\\alpha = 0.6$ 。 Text2video-zero可以在每个扩散的时间步 $t = T,\u0026hellip;,1$ 中对每一帧 $\\bf{x}_t^k\\ k=1,\u0026hellip;,m$ 应用预训练的 ControlNet 分支，并将 ControlNet 分支输出添加到主U-Net到跳跃连接中。\nControlVideo（Zhang et al. 2023） 旨在生成以文本提示 $\\tau$ 和运动序列（例如深度或边缘图）为条件的视频，$\\bf{c} = {c^i}_{i=0}^{N-1}$ 。它基于 ControlNet 改进出三个新机制：\n跨帧注意力：在自注意力模块中添加了全跨帧的交互。它通过将潜空间中的帧映射到 $\\bf{Q, K, V}$ 矩阵来引入所有帧之间的交互，这与 Text2Video-zero 不同，Text2Video-zero 仅配置所有帧关注第一帧。 交错帧更加平滑：在每个时间步 $t$ ，平滑器会对偶数帧或奇数帧进行插值，以平滑其对应的3帧。在平滑步骤后，帧数会随着时间的推移而减少。 分层采样器：利用分层采样器在内存限制下能够生成具有时间一致性的长视频。长视频被拆分成多个短片段，每个短片段都选中了一个关键帧。该模型预先生成这些具有完全跨帧注意到关键帧，以实现长期的一致性，并且每个相应的片段都是根据关键帧顺序合成的。 图15：ControlVideo 模型架构（左），全跨帧注意力示意图（右）。 Citation Weng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil’Log. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.\nReferences [1] Cicek et al. 2016. “3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation.”\n[2] Ho \u0026amp; Salimans, et al. “Video Diffusion Models.” 2022 | webpage\n[3] Bar-Tal et al. 2024 “Lumiere: A Space-Time Diffusion Model for Video Generation.”\n[4] Brooks et al. “Video generation models as world simulators.” OpenAI Blog, 2024.\n[5] Zhang et al. 2023 “ControlVideo: Training-free Controllable Text-to-Video Generation.”\n[6] Khachatryan et al. 2023 “Text2Video-Zero: Text-to-image diffusion models are zero-shot video generators.”\n[7] Ho, et al. 2022 “Imagen Video: High Definition Video Generation with Diffusion Models.”\n[8] Singer et al. “Make-A-Video: Text-to-Video Generation without Text-Video Data.” 2022.\n[9] Wu et al. “Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation.” ICCV 2023.\n[10] Blattmann et al. 2023 “Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.”\n[11] Blattmann et al. 2023 “Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets.”\n[12] Esser et al. 2023 “Structure and Content-Guided Video Synthesis with Diffusion Models.”\n[13] Bar-Tal et al. 2024 “Lumiere: A Space-Time Diffusion Model for Video Generation.”\n","permalink":"https://m1yan.github.io/posts/video_generation/","summary":"\u003cblockquote\u003e\n\u003cp\u003e全文为\u003ca href=\"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\u003eWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil’Log.\u003c/a\u003e的中文翻译版本。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e在过去几年里，扩散模型在图像合成方面显示出了优异的效果。现在，研究界已经开始着手一项更加艰巨的任务——将扩散模型用于视频生成。该任务可以看作是图像生成的超集，因为图像可以认为是1帧的视频，但是图像生成更加具有挑战性，原因如下：\u003c/p\u003e","title":"Diffusion Model for Video Generation"},{"content":"序 I used to rule the world\nSeas would rise when I gave the word\nNow in the morning I sleep alone\nSweep the streets I used to own\nViva La Vida\n我喜欢在晾衣服的时候听歌，许久未听的《Viva La Vida》在耳边突然响起。窗外寒风萧瑟，与歌词居然格外的相配。我看向屋内，温暖的灯光和各自忙碌的舍友与我隔离开来，忽然有种恍惚之感。记得我第一次听这首歌是因为这首歌是某个魔方速拧混剪视频的配乐，当时上小学还是初中的我对魔方特别的痴迷。当时的那个手拿魔方的少年似乎突然就站在了异乡的某个角落。\n回忆过去，才意识到时间过得如此之快。时间似乎一直不停歇地推着我们走，我想，也许对抗时间的最好方式，就是将如第一次还原魔方一般难忘的瞬间记录下来，或许这就是“Viva La vida”的真正含义。\n大同篇 2024的前几个月都在寒假和开学之中平淡如水地度过，上半年印象最深刻的就是劳动节假期的大同之旅。由于北邮不调休的优良传统，我们得以错峰在五一正式假期之前去游玩。在大同，给我印象最深刻的，居然是在酒店里晚上跟Zhrli一起看的超晦气鬼片《咒》:( 在这样一个佛教寺庙遍地的城市，居然折磨自己看这种东西😥，不仅晚上没睡好觉，早上起来逛一堆寺庙还能代入电影里的情节wwwww。\n不过大同的悬空寺还是给我带来了很强的视觉震撼，几根柱子就能把一整个几层的寺庙支撑在崖壁上，甚至凑近了还能看到被风吹动微微摇晃的柱子。越是这样壮观危险的场景，越是让人感受到大自然的敬畏。\n之后我们还去看了应县木塔，据说木塔塔顶有释迦摩尼的舍利子。到了木塔下面，听小伙伴说木塔的柱子每隔几个小时会轮流悬空，于是我们就绕着塔边往木柱子下面插小纸片，还真的有柱子与地面有很大的缝隙！但是就算这样，这个木塔还是稳稳地、静静地屹立在这里，见证千年荣辱。\n离开木塔时，路过一棵古老的松树，上面挂满了铜制的风铃，一阵风吹过，一连串清脆的铃音不绝于耳，给人一种净化心灵的超脱之感。不得不说这确实是高僧修行的好地方。\n我们还去了大名鼎鼎的云冈石窟，沿着山体绵延数公里的崖壁上，全是大大小小的石窟。当时给我印象最深刻的是，很多佛像的头都因为保存不当或是失窃而不见了，只剩佛身孤零零地坐在那里。这里埋下了一个伏笔，我居然在某一个地方还能见到缺失的佛头。\n上海篇 然后就是暑假的社会实践——上海之行啦~由于Kim是Shanghai人，也是顺理成章带队带我们领略上海风情。在上海，除了顺利完成参观企业等实践任务，我们也马不停蹄地逛了上海的好多景点。我们从南京路步行街开始City Walk，一路经过外滩走到外白渡桥，上海的历史感和现代感就这样在沿途展开。\n此外，在静安寺闲逛时，恰好赶上苹果的头显设备在内地发售，我们抱着试试看的态度进店想体验体验（因为体验需要预约）。但是非常幸运地由于前面几个预约的客人没来，“插队”体验到了头显。虽然画面清晰度没有我想象的那么清晰，但是使用手机捏合的操作逻辑非常丝滑~\n最后我们在渡口做轮渡去看了晚上的陆家嘴，晚上天空飘着不大不小的雨，雨水在地面上反射出一片片光怪陆离的霓虹，为这次夜晚之行加入了一些梦幻感。我们沿着黄埔江边走了很远，本来小伙伴们都正经打着伞，后来YanmHa直接丢了伞开始在雨中奔跑，于是大家都放开跑了起来哈哈哈。虽然走了很远脚很痛，衣服鞋子都湿了，但是那种短暂了忘记了烦恼的感觉真的很不错。\n现在回忆起，当时逛的每一条街，吃的每一顿饭，都因为和小伙伴们一起，而变得特别难忘。或许旅行也是增进友谊的一种很好的方式~\n日本篇 随着暑假的开始，我们在沙河校区的两年时光也宣告正式结束，学院100号人浩浩荡荡搬入西土城校区，三环内的新生活开始了！\n在暑假，我报名参加了日本电気通信大学的学术交流活动，有幸能够得到学院的资助前往日本进行为期7天的学术交流。\n落地成田机场后，我们购买了西瓜卡，乘坐天空线前往酒店的所在地新宿区。新宿区密密麻麻的住宅和商业区被分成了很多町，窄窄的巷子很有日式风情——行人靠左走的马路指示线、上方交错纵横的电线、很节约空间的双层停车场以及几步一个的711和罗森。\n在东京的前几天，我们参加了项目的启动仪式，逛了逛学校周边的商场，发现有chikawa的快闪店（不过东西都好贵www，最后只买了张贴纸），以及吃了在上海要排大队的回转寿司🍣。寿司一盘均价大约七八块钱，而且用的都是生牛羊鱼肉，看见寿司嗖的一下通过轨道送过来，还是一种挺新奇的体验的hhh~\n在晚上，我们去了涩谷，去看了据说是全世界最繁忙的十字路口。根据小红书的攻略，我们登上商场二楼的星巴克，果然拍出了过马路时人山人海的场景，视觉上还是很震撼的！不得不感慨东京真的是一座承载着很多人的梦想的超级大都市~\n随后几天，因为项目要忙到下午很晚，所以只有晚上有时间出来闲逛，我们也去了著名的歌舞伎町，不过也是真的不敢靠近任何一家18+的酒吧啊hhhh~\n在倒数第二天晚上，我们去了银座。银座果然是富人集合地，随手一拍都是个兰博基尼😰，周围也都是逛不起的奢侈品店。我们走着走着，东京塔就这样突然出现在眼前。我们之前了解了东京塔的历史，所以也并不想专门来看这个塔，但是既然都到这了，和小伙伴还是咬咬牙买了登塔的末班票，卡着还有二十分钟关塔的时间上去了。在塔上，窗外可以看到壮观的东京都夜景，密密麻麻的街道和灯光就这样铺开在眼前，人在这样的钢铁水泥森林中真是“渺沧海之一粟”。在如此的上帝视角下，仿佛能够看透城市的运转规则，写字楼里忙碌的白领，住宅区温馨的一家人，马路上川流不息的车辆……\n在日本的最后一天，我们终于在白天有时间，去逛了东京国立博物馆和浅草寺。发现博物馆还需要买票，又得夸夸国内的大型博物馆，都是免费的（如果能约的上的话hhh）。没错，在东京国立博物馆的东洋馆里，很巧地见到了在大同失窃的佛头。看到很多精巧的青铜器和甲骨残片都在馆里，真想不到这些文物是以怎样的方式流落海外的，真是令人扼腕叹息啊。在浅草寺抽了一签，抽到了凶签，反正我也不迷信，那就让这个签留在日本吧，把我的坏运气也留在日本算了hhhh。\n最后提一下项目hhh，我们小组拿到了“最佳性能奖”，也算是这几天的努力没有白费（可能也就是比别人晚走半小时hhh）~\n总结一下本次日本之行，算是体验到了不同文化的文化差异，看到了更大的世界（也看到了东亚人一样的卷wwww）。\n张家港篇 开学之后呢，在期中左右的时间，我又报名参加的CCF China Net中国网络大会（也是不放过任何一次能出去玩学习的机会），前往张家港。\n会议很高端，参会人员大多是博士和教授，显得我们这些本科生起到了一个气氛组的作用hhh，而且网络这个大方向也与我感兴趣的研究方向不是特别匹配，于是我也权当是来长长见识、拓展拓展视野啦。在会上social有同学问我博几的，只能跟他说我是本科生🤡。\n学业篇 上面流水账似的啰里啰嗦大半天，感觉怎么今年印象深刻的事情都是出去玩hhhh，所以下面也说说正事（咳咳）。\n今年学业上的进展呢，就是下定决心改变了当时选定的科研方向，选择在AI这个大方向探索下去。同时呢，也有幸遇到了很负责任的导师愿意带我，遇到了很强很有耐心的师兄手把手指导我。都说“师傅领进门，修行靠个人”，有这么多老师、师兄带我一起进步，真的是莫大的幸运😭。同时，申报的“未来学者计划”也成功立项和通过答辩了，也很感激学院能给予我们科研的平台和机会。学习和搞科研的过程并不总是一帆风顺，需要付出巨大的耐心和精力，前一天幻想的完美实验结果，第二天醒来一看可能就“天塌了”。但是，微小的成就正是建立在这一次次的失败上的。希望自己明年能积累更多的经验，无论是成功还是失败的经验，对个人来说都是一次历练。\n结 “我步入丛林，是因为我想活的有意义，我希望活的深刻，汲取生命全部的精髓！把非生命的一些全部击溃，以免在生命终结之时，发现自己从来没有活过……”\n——梭罗《瓦尔登湖》\n这是我很喜欢的一句话，虽然听起来非常的理想化，有一股鸡汤味，但是这句话蕴含的生命力非常动人。就像《Viva La Vida》歌词里路易十六的自白，即使有一天会离去，但仍然竭尽全力去看整个世界。“生命万岁”，生命当然是有限的，但是这些闪光的时刻却是无限的，正是这些闪光的时刻，赋予生命以无限的意义。\n2025，祝好！\n","permalink":"https://m1yan.github.io/posts/2024%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","summary":"\u003ch2 id=\"序\"\u003e序\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI used to rule the world\u003c/p\u003e\n\u003cp\u003eSeas would rise when I gave the word\u003c/p\u003e\n\u003cp\u003eNow in the morning I sleep alone\u003c/p\u003e\n\u003cp\u003eSweep the streets I used to own\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eViva La Vida\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"2024年终总结"},{"content":" Update [2024.12.7]：增加条件生成以及潜在扩散模型的介绍。\nUpdate [2024.12.11]：增加评估指标的对比以及超参数调整。\nUpdate [2024.12.12]：增加对于扩散模型个性化生成微调方法的介绍\n生成模型 目前主流的生成模型包括生成对抗模型 (GAN)、变分自编码器 (VAE)和基于流的模型 (Flow-based models)。\n它们都能够生成较高质量的图像，但是也都具有一定的局限性。由于GAN模型具有对抗性训练的性质，因此其训练过程比较脆弱且难以稳定收敛，生成图像的多样性也较低。与GAN相比，VAE经常会生成较模糊、不够锐利的样本，因为VAE在优化过程中引入了KL散度正则项，鼓励潜变量分布与先验分布靠拢，因此会损失部分细节信息。基于流的生成模型通过严格的可逆变换实现对数据分布的精确密度估计，这意味着每一步变换需要是可逆且雅可比行列式可计算，因此在模型设计上对层结构有较强限制。\n扩散模型的设计思路来自非平衡热力学。模型定义了一个马尔可夫扩散步骤，缓慢地向图像中添加随机噪声，然后学习扩散的逆过程以从噪声中构建所需要的数据样本。\n生成模型的结构，图引自 Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. 什么是扩散模型？ 目前主流的基于扩散的生成模型包括扩散概率模型 (Diffusion Probabilistic Models)、条件噪声打分网络 (noise-conditioned score network)和去噪扩散概率模型 (denoising diffusion probabilistic models, DDPM)，扩散过程包括前向扩散过程和逆向扩散过程。\n前向扩散过程 给定从真实数据分布中采样的数据点 $x_0$ ~ $q(x) $ ，定义一个前向扩散过程，在这个过程中，我们向样本中添加高斯噪声 $T$ 步，产生一系列含有噪声的样本 $ x_1, \u0026hellip;, x_T $。 $$ q(x_t|x_{t-1})=\\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1},\\beta_tI) \\ q(x_{1:T}|x_0) = \\prod_{t=1}^Tq(x_t|x_{t-1}) $$ 随着加噪步数的增加，图像特征逐渐消失，最终当T趋近于无穷时，$x_T$ 相当于各向同性的高斯分布。\n通过缓慢地添加（去除）噪声的正向（反向）扩散过程，图引自Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. 上述采样过程能够使我们计算出在时间t时刻的采样$x_t$。令\n$$ \\alpha_t=1-\\beta_t, \\overline{\\alpha}_t = \\prod_{i=1}^t\\alpha_i \\\\ x_t = \\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon_{t-1} \\\\ =\\sqrt{\\alpha_t\\alpha_{t-1}}x_{t-2}+\\sqrt{1-\\alpha_t\\alpha_{t-1}}\\epsilon_{t-2} \\\\ =... \\\\ =\\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha_t}}\\epsilon $$ 逆向扩散过程 为了实现从无序噪声恢复到数据分布（即反向扩散过程），需要对后验分布 $q(x_{t-1}|x_t, x_0)$ 进行分析。根据贝叶斯公式：\n$$ q(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)} $$ 由于前向过程定义为条件独立的马尔可夫链，有： $$ q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) \\\\ q(x_{t-1} \\mid x_0) = \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0, (1-\\bar{\\alpha}_{t-1})I) $$ 将以上分布代入后，可得到后验分布仍是高斯分布形式： $$ q(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t I) $$ 其中： $$ \\tilde{\\mu}_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1-\\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} x_t, \\\\ \\tilde{\\beta}_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}\\beta_t $$ 反向扩散过程的核心在于：如果我们能够对 $q(x{t-1} | x_t)$ 进行近似，就可以从纯噪声一步一步还原为原始数据分布。由于我们不知道$x_0$，我们希望有一个参数化的模型 $p\\theta$ 来近似 $q(x_{t-1}|x_t,x_0)$： $$ p_\\theta(x_{t-1} \\mid x_t) \\approx q(x_{t-1} \\mid x_t, x_0) $$ 若使用模型 $\\epsilon_\\theta(x_t,t)$ 来预测噪声，则可得到简化的逆扩散公式： $$ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\\right) + \\sqrt{\\tilde{\\beta}_t} z, \\quad z \\sim \\mathcal{N}(0, I) $$ t=1时省略最后的噪声项，最终可以得到$x_0$的样本。可以看到，扩散模型通过预测噪声$\\epsilon_\\theta$来重构之前时间步的样本。 可以得到忽略加权项的简化目标来训练扩散模型，最终设计的损失函数如下：\n$$ L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0,\\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right] $$ 其中， $$ x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I), \\quad t \\sim \\text{Uniform}\\{1,\\ldots,T\\} $$ 若考虑不同时间步的加权，可以定义加权损失： $$ L(\\theta) = \\sum_{t=1}^{T} w_t \\mathbb{E}_{x_0,\\epsilon}\\left[\\| \\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 \\right] $$ DDPM论文中的算法如下： 实现一个DDPM 为了更清楚地了解扩散模型的整体架构，而不是为了探究复杂的概率论和数学原理，使用diffusers库实现DDPM的训练和推理。\n我们使用huggingface上的huggan/smithsonian_butterflies_subset作为训练数据集，该数据集包含自然界各种各样的蝴蝶，可用于无条件的图像生成过程。\n参数配置 首先按照以下配置进行训练和推理步骤的参数配置：\nfrom dataclasses import dataclass @dataclass class TrainingConfig: image_size = 128 train_batch_size = 32 eval_batch_size = 8 # how many images to sample during evaluation num_epochs = 50 gradient_accumulation_steps = 1 learning_rate = 1e-4 lr_warmup_steps = 500 save_image_epochs = 10 save_model_epochs = 25 mixed_precision = \u0026#34;fp16\u0026#34; # `no` for float32, `fp16` for automatic mixed precision output_dir = \u0026#34;output\u0026#34; # the model name locally and on the HF Hub seed = 42 device = \u0026#34;cuda\u0026#34; config = TrainingConfig() 训练数据准备 然后使用huggingface的datasets库进行数据集的下载和导入：\nfrom datasets import load_dataset config.dataset_name = \u0026#34;huggan/smithsonian_butterflies_subset\u0026#34; dataset = load_dataset(config.dataset_name, split=\u0026#34;train\u0026#34;) 使用以下代码查看数据集中的图像：\nimport matplotlib.pyplot as plt fig, axs = plt.subplots(1, 4, figsize=(16, 4)) for i, image in enumerate(dataset[:4][\u0026#34;image\u0026#34;]): axs[i].imshow(image) axs[i].set_axis_off() fig.show() 使用torchvision库中的transforms模块，将图像的尺寸和数值归一化处理：\nfrom torchvision import transforms preprocess = transforms.Compose([ transforms.Resize((config.image_size, config.image_size)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.5], [0.5]), ]) def transform(examples): images = [preprocess(image.convert(\u0026#34;RGB\u0026#34;)) for image in examples[\u0026#34;image\u0026#34;]] return {\u0026#34;img\u0026#34;: images} dataset.set_transform(transform) print(dataset[0][\u0026#39;img\u0026#39;].shape) # torch.Size([3, 128, 128]) 定义一个dataloader用于数据集的批量加载：\nimport torch train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True) 使用U-Net进行噪声预测 在扩散模型中，可以使用MLP或者U-Net来进行噪声的预测，从而将噪声一步一步去噪得到真实图像。选择U-Net作为噪声预测的模型，U-Net的架构由下采样堆栈和上采样堆栈构成。\n下采样：每个步骤包括重复应用两个 3x3 卷积（无填充卷积），每个卷积后跟一个 ReLU 和一个步幅为 2 的 2x2 最大池化。在每个下采样步骤中，特征通道的数量都会加倍。 上采样：每个步骤包括对特征图的上采样，然后进行 2x2 卷积，并且每次将特征通道数量减半。 捷径连接：上下采样堆栈相应层通过捷径连接，为上采样过程提供必要的高分辨率特征。 实现如下：\nfrom diffusers import UNet2DModel model = UNet2DModel( sample_size=config.image_size, # the target image resolution in_channels=3, # the number of input channels, 3 for RGB images out_channels=3, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(128, 128, 256, 256, 512, 512), # the number of output channels for each UNet block down_block_types=( \u0026#34;DownBlock2D\u0026#34;, # a regular ResNet downsampling block \u0026#34;DownBlock2D\u0026#34;, \u0026#34;DownBlock2D\u0026#34;, \u0026#34;DownBlock2D\u0026#34;, \u0026#34;AttnDownBlock2D\u0026#34;, # a ResNet downsampling block with spatial self-attention \u0026#34;DownBlock2D\u0026#34;, ), up_block_types=( \u0026#34;UpBlock2D\u0026#34;, # a regular ResNet upsampling block \u0026#34;AttnUpBlock2D\u0026#34;, # a ResNet upsampling block with spatial self-attention \u0026#34;UpBlock2D\u0026#34;, \u0026#34;UpBlock2D\u0026#34;, \u0026#34;UpBlock2D\u0026#34;, \u0026#34;UpBlock2D\u0026#34;, ), ) # Check input and output shapes sample_image = dataset[0][\u0026#39;img\u0026#39;].unsqueeze(0) print(\u0026#34;Input Shape\u0026#34;, sample_image.shape) print(\u0026#34;Output Shape\u0026#34;, model(sample_image, timestep=0).sample.shape) 通过检查输入U-Net和输出U-Net的图像形状，可以得知输入和预测噪声的形状一致，满足扩散模型的需求。\nDDPM Scheduler 创建一个噪声调度器，用来在不同的时间步中为图像加噪。\n# Create a DDPM scheduler import torch from PIL import Image from diffusers import DDPMScheduler noise_scheduler = DDPMScheduler(num_train_timesteps=1000) noise = torch.randn(sample_image.shape) timesteps = torch.LongTensor([50]) noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps) Image.fromarray(((noisy_image.permute(0,2,3,1)+1.0)*127.5).type(torch.uint8).numpy()[0]) 从加噪后的输出可以看出，图像中出现了明显的噪声。\n创建优化器和学习率调度器 # Create optim and lr scheduler from diffusers.optimization import get_cosine_schedule_with_warmup optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate) lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.lr_warmup_steps, num_training_steps= (len(train_dataloader)*config.num_epochs) ) 设计损失函数 扩散模型的核心在于优化预测噪声的模型，因此需要使预测噪声的模型 (U-Net) 输出的噪声与实际噪声的分布接近。因此损失函数可以简单地设计为：\nnoise_pred = model(noisy_images, timesteps, return_dict=False)[0] loss = F.mse_loss(noise_pred, noise) 训练过程 使用huggingface的accelerate库进行方便的模型加载、权重保存以及模型评估。训练的整体思路是生成图像不同时间步中加入噪声后的图像，U-Net接受加噪后的图像以及其对应的时间步，预测出该步骤加入的噪声。预测噪声与实际加入的噪声使用loss进行计算，最小化loss，进而使U-Net具有预测噪声的能力。最终在推理过程中能够使用U-Net在每个时间步进行去噪，最后生成接近真实分布的图像。训练循环代码如下：\nfrom accelerate import Accelerator from tqdm.auto import tqdm from pathlib import Path import torch.nn.functional as F import os def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler): # Initialize accelerator accelerator = Accelerator( mixed_precision=config.mixed_precision, gradient_accumulation_steps=config.gradient_accumulation_steps, project_dir=os.path.join(config.output_dir, \u0026#34;logs\u0026#34;) ) if accelerator.is_main_process: if config.output_dir is not None: os.makedirs(config.output_dir, exist_ok=True) # Prepare everything model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare( model, optimizer, train_dataloader, lr_scheduler ) global_step = 0 # Train! for epoch in range(config.num_epochs): progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process) progress_bar.set_description(f\u0026#34;Epoch {epoch}\u0026#34;) for step, batch in enumerate(train_dataloader): clean_images = batch[\u0026#34;img\u0026#34;] # Sample noise to add to the clean image noise = torch.randn(clean_images.shape, device=config.device) bs = clean_images.shape[0] # Sample a random timestep for each image timesteps = torch.randint( 0, noise_scheduler.config.num_train_timesteps, (bs,), device=config.device, dtype=torch.int64 ) # forward diffusion process noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) with accelerator.accumulate(model): # Predict Noise residual noise_pred = model(noisy_images, timesteps, return_dict=False)[0] loss = F.mse_loss(noise_pred, noise) accelerator.backward(loss) if accelerator.sync_gradients: accelerator.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) logs = {\u0026#34;loss\u0026#34;: loss.detach().item(), \u0026#34;lr\u0026#34;: lr_scheduler.get_last_lr()[0], \u0026#34;step\u0026#34;: global_step} progress_bar.set_postfix(**logs) accelerator.log(logs, step=global_step) global_step += 1 # Evaluation if accelerator.is_main_process: pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler) if (epoch + 1) % config.save_image_epochs == 0: evaluate(config, epoch, pipeline) if (epoch + 1) % config.save_model_epochs == 0: pipeline.save_pretrained(os.path.join(config.output_dir, f\u0026#34;epoch_{epoch}\u0026#34;)) elif (epoch + 1) == config.num_epochs: pipeline.save_pretrained(os.path.join(config.output_dir, f\u0026#34;final\u0026#34;)) 其中，模型评估的代码如下：\n# Evaluation from diffusers import DDPMPipeline from diffusers.utils import make_image_grid import os def evaluate(config, epoch, pipeline): images = pipeline( batch_size = config.eval_batch_size, generator = torch.Generator(device=config.device).manual_seed(config.seed), ).images image_grid = make_image_grid(images, rows=2, cols=4) test_dir = os.path.join(config.output_dir, \u0026#34;test\u0026#34;) os.makedirs(test_dir, exist_ok=True) image_grid.save(os.path.join(test_dir, f\u0026#34;epoch_{epoch}.png\u0026#34;)) 训练与推理结果 最后使用以下代码在Jupyter Notebook中启动训练：\nrom accelerate import notebook_launcher args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler) notebook_launcher(train_loop, args, num_processes=1) 最终不同训练epoch的推理结果如下：\n可以看出，随着训练步数的增加，生成的图像越来越向真实的图像分布（蝴蝶形态）靠拢，说明经过训练后，扩散模型具有了生成图像的能力。\n使用IS和FID指标进行图像质量评估 什么是IS (Inception Score) ? Inception Score 是一种对生成图像的质量和多样性进行评价的指标。其思路是利用一个预训练好的分类模型（通常是 Inception v3）对生成的图像进行分类，然后根据分类结果的分布来计算得分。\n$p(y|x)$ 为给定生成图像 $x$ 的类别分布，$p(y) = \\int p(y|x) p(x) dx$ 为所有生成图像的平均类别分布，$KL(\\cdot|\\cdot)$ 为KL散度，则IS为： $$ \\text{IS} = \\exp\\left( \\mathbb{E}_{x}\\bigl[ KL(p(y \\mid x) | p(y)) \\bigr] \\right) $$\n直观上：\n如果生成图像的质量高，则概率分布应该集中在某些明确的类上（即分布峰值较高，说明图像能够被轻松分类） 如果生成图像的多样性高，则概率分布应该均匀覆盖多个类别。 综合来看，IS高时，说明生成图像既清晰又多样。\n什么是FID (Frechet Inception Distance) ? FID 用于衡量生成分布和真实数据分布在特征空间（通常是 Inception v3 的中间特征层）上的差异。与IS不同，FID需要真实样本和生成样本作为对比，关注两者之间的统计差异。\n设真实数据特征分布为 $\\mathcal{N}(\\mu_r, \\Sigma_r)$，生成数据特征分布为 $\\mathcal{N}(\\mu_g, \\Sigma_g)$，则FID定义为两高斯分布的Fréchet距离： $$ \\text{FID}(\\mu_r, \\Sigma_r, \\mu_g, \\Sigma_g) = |\\mu_r - \\mu_g|^2 + \\text{Tr}\\left(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}\\right). $$ 其中，$\\mu_r, \\Sigma_r$ 为真实分布特征的均值和协方差，$\\mu_g, \\Sigma_g$ 为生成分布特征的均值和协方差，$\\text{Tr}(\\cdot)$ 为迹运算，$( \\Sigma_r \\Sigma_g )^{1/2}$ 为矩阵的对称正定平方根。\n直观上：\nFID衡量的是两个高斯分布之间的Fréchet距离，当两组特征分布一致时，FID为0（理想情况下）。\n如果生成图像质量越高越逼近真实分布，那么 $\\mu_g \\approx \\mu_r$ 且 $\\Sigma_g \\approx \\Sigma_r$，因此FID会很低。\n如果生成图像与真实分布偏差大，分布统计差异明显，FID会较高。\nIS与FID的对比 指标 IS（Inception Score） FID（Fréchet Inception Distance） 目标 测量生成图像的多样性和清晰度 测量生成图像与真实图像分布的相似性 公式 基于分类分布的KL散度 高斯分布均值和协方差的Fréchet距离 数值范围 无上限 0 （越低越好） 优点 简单直观，关注多样性和清晰度 综合考虑细节和分布相似性 缺点 不考虑生成图像与真实图像的匹配程度 计算依赖样本量，复杂度稍高 适用场景 快速评估生成图像的基本性能 更全面的衡量生成图像质量和真实感 IS评估 测量IS和FID指标，需要的图像数量至少需要上万张，由于生图速度较慢，使用500张生成图像进行IS指标的测量。\n首先使用以下代码进行sampling：\n# Generate samples from diffusers import DDPMPipeline import random pipeline = DDPMPipeline.from_pretrained(\u0026#34;/openbayes/home/miyan/works/Diffusion-Model-0-1/output/epoch_49\u0026#34;).to(config.device) samples_num = 500 batch_size = 20 for epoch in range(samples_num // batch_size): images = pipeline( batch_size = batch_size, generator = [torch.Generator(device=config.device).manual_seed(random.randint(0, 100000)) for _ in range(batch_size)], ).images test_dir = os.path.join(config.output_dir, \u0026#34;samples\u0026#34;) os.makedirs(test_dir, exist_ok=True) for i, image in enumerate(images): image.save(os.path.join(test_dir, f\u0026#34;{i+epoch*batch_size}.png\u0026#34;)) 使用以下代码进行IS指标的测量：\nimport torch import torch.nn as nn import torch.nn.functional as F import torchvision.models as models import numpy as np from scipy.linalg import sqrtm def calculate_inception_score(images, device, batch_size=32, splits=10): \u0026#34;\u0026#34;\u0026#34; 计算 Inception Score (IS)。 参数: images: torch.Tensor，形状为(N, C, H, W) 的生成图像 device: torch.device，计算设备（CPU或GPU） batch_size: 批大小 splits: 将生成的图片集分为几份计算IS 返回: (is_mean, is_std): IS的均值和标准差 \u0026#34;\u0026#34;\u0026#34; # 加载预训练的Inception v3模型，用于分类 inception = models.inception_v3(pretrained=True, transform_input=True).to(device) inception.eval() preds = [] # 分批次计算预测概率分布 with torch.no_grad(): for i in range(0, len(images), batch_size): batch = images[i:i+batch_size].to(device) # Inception v3要求输入为299x299，如果 images 已经是此大小且已标准化则无需再次处理 logits = inception(batch) probs = F.softmax(logits, dim=1) preds.append(probs.cpu().numpy()) preds = np.concatenate(preds, axis=0) # (N, 1000) # 计算IS N = preds.shape[0] split_scores = [] for k in range(splits): part = preds[k * (N // splits) : (k+1) * (N // splits), :] p_y = np.mean(part, axis=0) scores = [] for i in range(part.shape[0]): p_yx = part[i] scores.append(np.sum(p_yx * (np.log(p_yx + 1e-10) - np.log(p_y + 1e-10)))) split_scores.append(np.exp(np.mean(scores))) is_mean = np.mean(split_scores) is_std = np.std(split_scores) return is_mean, is_std import os from PIL import Image import torch import torchvision.transforms as transforms from torch.utils.data import DataLoader from torchvision import datasets import numpy as np device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) ####################### # 数据预处理Transform ####################### # Inception v3预期输入尺寸为299x299，且通常使用标准化到[-1,1] transform = transforms.Compose([ transforms.Resize((299, 299)), transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) ]) ####################### # 加载生成的图像 (fake_images) ####################### def load_fake_images_from_folder(folder, transform): images = [] for filename in os.listdir(folder): if filename.lower().endswith((\u0026#39;png\u0026#39;,\u0026#39;jpg\u0026#39;,\u0026#39;jpeg\u0026#39;)): img_path = os.path.join(folder, filename) img = Image.open(img_path).convert(\u0026#39;RGB\u0026#39;) img = transform(img) images.append(img) # 将所有图像合并为一个Tensor: (N, C, H, W) if len(images) \u0026gt; 0: images = torch.stack(images, dim=0) else: images = torch.empty(0) # 如果没有图像则返回空tensor return images fake_folder = \u0026#34;/openbayes/home/miyan/works/Diffusion-Model-0-1/output/samples\u0026#34; fake_images = load_fake_images_from_folder(fake_folder, transform) # (N,3,299,299) is_mean, is_std = calculate_inception_score(fake_images, device) print(\u0026#34;IS:\u0026#34;, is_mean, is_std) FID评估 使用以下代码进行FID指标的测量：\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2): \u0026#34;\u0026#34;\u0026#34; 计算Fréchet Inception Distance所需的Fréchet距离。 参数: mu1, sigma1: 实际数据特征均值和协方差矩阵 mu2, sigma2: 生成数据特征均值和协方差矩阵 返回: fid: FID分数 \u0026#34;\u0026#34;\u0026#34; diff = mu1 - mu2 diff_sq = diff.dot(diff) # 计算矩阵的对称矩阵平方根 covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False) if np.iscomplexobj(covmean): covmean = covmean.real fid = diff_sq + np.trace(sigma1 + sigma2 - 2 * covmean) return fid def calculate_fid(real_features, fake_features): \u0026#34;\u0026#34;\u0026#34; 计算 Frechet Inception Distance (FID)。 参数: real_features: np.ndarray, shape (N, 2048)，真实图像特征 fake_features: np.ndarray, shape (M, 2048)，生成图像特征 返回: fid: FID分数（越低越好） \u0026#34;\u0026#34;\u0026#34; mu_real = np.mean(real_features, axis=0) sigma_real = np.cov(real_features, rowvar=False) mu_fake = np.mean(fake_features, axis=0) sigma_fake = np.cov(fake_features, rowvar=False) fid = calculate_frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake) return fid transform_img = transforms.Compose([ transforms.Resize((299, 299)), transforms.ToTensor(), transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) ]) dataset = load_dataset(\u0026#34;huggan/smithsonian_butterflies_subset\u0026#34;, split=\u0026#34;train\u0026#34;) def transform(examples): images = [transform_img(image.convert(\u0026#34;RGB\u0026#34;)) for image in examples[\u0026#34;image\u0026#34;]] return {\u0026#34;img\u0026#34;: images} dataset.set_transform(transform) real_loader = DataLoader(dataset, batch_size=32, shuffle=False) inception = models.inception_v3(pretrained=True, transform_input=True).to(device) inception.eval() # 计算FID需要real_features和fake_features（需要先提取特征） # 可通过迭代real_loader对真实数据提取特征： real_features_list = [] with torch.no_grad(): for batch in real_loader: imgs = batch[\u0026#34;img\u0026#34;] imgs = imgs.to(device) feats = inception(imgs) real_features_list.append(feats.cpu().numpy()) real_features = np.concatenate(real_features_list, axis=0) # 对 fake_images 同样提取特征 fake_features = [] with torch.no_grad(): for i in range(0, len(fake_images), 32): batch = fake_images[i:i+32].to(device) feats = inception(batch) fake_features.append(feats.cpu().numpy()) fake_features = np.concatenate(fake_features, axis=0) fid_score = calculate_fid(real_features, fake_features) print(\u0026#34;FID:\u0026#34;, fid_score) 最终经测量得出的IS和FID指标如下：\nIS (mean ± std) FID 2.369±0.231 606.887 调整超参数以提升图像质量 超参数 扩散模型的超参数见训练配置：\nclass TrainingConfig: image_size = 128 train_batch_size = 32 eval_batch_size = 8 # how many images to sample during evaluation num_epochs = 100 gradient_accumulation_steps = 1 learning_rate = 1e-4 lr_warmup_steps = 1000 save_image_epochs = 10 save_model_epochs = 500 mixed_precision = \u0026#34;fp16\u0026#34; # `no` for float32, `fp16` for automatic mixed precision output_dir = \u0026#34;output\u0026#34; # the model name locally and on the HF Hub seed = 42 device = \u0026#34;cuda\u0026#34; image_size：图像的分辨率，较小图像尺寸可以加快训练速度，但无法捕获复杂细节；较大图像尺寸能提升图像质量，但是会增加计算负担。 train_batch_size：一次迭代中用于训练的样本数，较大的批量大小能提升模型稳定性，减少优化过程中的梯度震荡。 eval_batch_size：评估阶段生成图像的数量，对训练过程没有影响。 num_epochs：训练数据的完整训练轮数。 gradient_accumulation_steps：梯度累计步数，增大此值能够实现更大的等效批量大小，从而提升训练的稳定性和图像质量。 learning_rate：控制模型参数的更新步幅。 lr_warmup_steps：在初始阶段逐步提升学习率，避免参数更新过快。 调整超参数重新训练 由于一次训练时间过长，尝试进行某些参数的调整以尽量提升图像质量。\n将num_epochs提高到100轮，增加模型对于图像信息的学习轮数；将lr_warmup_steps提高至1000步，增加模型适应优化的过程，避免初期梯度下降不稳定的情况发生。\n训练过程中每10个epoch验证一次，验证结果如下：\n使用IS和FID指标进行评估 使用训练好的模型生成500张图像用于指标计算，指标以及指标提升对比如下：\nModel IS↑ FID↓ Epochs 50 + lr warmup steps 500 2.369±0.231 606.887 Epochs 100 + lr warmup steps 1000 2.237±0.217 380.933 IS值有所下降，这是由于训练图像过于单一导致图像多样性不足；FID值下降明显，说明生成图像与训练数据分布相似性提高，图像细节增加。\n条件生成 Conditioned Generation 在使用带有条件信息（如文本描述）的图像训练生成模型时，通常会生成以类标签或者一段描述性文本为条件的样本。主要方法包括分类器引导的扩散 (Classifier Guided Diffusion) 和无分类器引导的扩散 (Classifier-Free Guidance)。\nClassifier Guided Diffusion 为了将类别信息明确地加入传播过程中，通过在含有噪声的图像$x_t$上训练一个分类器$p_\\phi(y | \\mathbf{x}_t)$ 并且使用梯度 $\\nabla_{\\mathbf{x}_t} \\log p_\\phi(y | \\mathbf{x}_t)$引导采样过程朝向调节信息y（如标签信息或描述文本）方向预测噪声。 无条件的噪声预测器使用以下公式进行噪声的预测和去噪过程：\n$$ \\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(\\mathbf{x}_t, t) \\right) + \\sigma_t \\mathbf{z}, $$ 其中，$\\epsilon_\\theta(\\mathbf{x}_t, t)$是无条件的噪声预测器。 分类器引导的扩散模型反向采样公式为：\n$$ \\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta^{\\text{guided}}(\\mathbf{x}_t, t, y) \\right) + \\sigma_t \\mathbf{z}. $$ 对于条件引导的噪声预测器与无条件噪声预测器的关系，有： $$ \\epsilon_\\theta^{\\text{guided}}(\\mathbf{x}_t, t, y) = \\epsilon_\\theta(\\mathbf{x}_t, t) - w \\cdot \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\nabla_{\\mathbf{x}_t} \\log p_\\phi(y | \\mathbf{x}_t), $$ 其中，$\\epsilon_\\theta(\\mathbf{x}_t, t)$ 是无条件噪声预测器，$\\nabla_{\\mathbf{x}_t} \\log p_\\phi(y | \\mathbf{x}_t)$ 是分类器对类别 $y$ 的梯度，$w$是分类器引导强度的调节参数，$\\sqrt{1 - \\bar{\\alpha}_t}$ 用于将梯度项映射到噪声空间。 Classifier-Free Guidance 没有独立的分类器$p_\\phi(y | \\mathbf{x}_t)$ ，可以通过合并条件和非条件引导的扩散步骤来实现无分类器引导的条件扩散。为了避免训练一个显式的分类器 $p_\\theta(y | \\mathbf{x}_t)$，Classifier-Free Guidance通过直接学习两个噪声预测器来实现条件生成。 条件噪声预测器$\\epsilon_\\theta(\\mathbf{x}_t, t, y)$：基于目标条件 y 的噪声预测器。 无条件噪声预测器 $\\epsilon_\\theta(\\mathbf{x}_t, t)$：不依赖任何条件的噪声预测器。 通过线性组合条件和无条件噪声预测器，可以构造出一种增强条件生成效果的噪声预测器：\n$$ \\epsilon_\\theta^{\\text{guided}}(\\mathbf{x}_t, t, y) = \\epsilon_\\theta(\\mathbf{x}_t, t) + w \\cdot \\left(\\epsilon_\\theta(\\mathbf{x}_t, t, y) - \\epsilon_\\theta(\\mathbf{x}_t, t)\\right), $$ 其中：$w \\geq 1$ 是引导强度（通常称为“放大系数”），$\\epsilon_\\theta(\\mathbf{x}_t, t, y) - \\epsilon_\\theta(\\mathbf{x}_t, t)$ 表示条件信息对噪声预测的增量。 在训练过程中，无条件和条件噪声预测器通过单个神经网络进行学习，其中条件信息$y$被定期丢弃，以便模型知道如何无条件的生成图像，即\n$$ \\epsilon_\\theta(\\mathbf{x}_t, t)= \\epsilon_\\theta(\\mathbf{x}_t, t, \\tilde{y}) \\\\ \\tilde{y} = \\begin{cases} y, \u0026 \\text{with probability } 1 - p_\\text{drop}, \\\\ \\varnothing, \u0026 \\text{with probability } p_\\text{drop}. \\end{cases} $$ 潜在扩散模型 (Latent Diffusion Model) 潜在扩散模型通过在潜空间而不是像素空间运行扩散过程，从而降低训练成本并加快推理速度。模型发现的动机是观察到图像的大多数位置对感知细节都有着帮助，而且语义和概念信息经过压缩后依然存在。LDM通过使用自动编码器将信息编码到潜在空间，然后在潜在扩散过程中生成语义概念。\n首先，给定高维数据$x_0$（如图像），通过一个预训练的自动编码器将其映射到潜在空间：\n$$ \\mathbf{z}_0 = E(\\mathbf{x}_0), $$ 扩散和去噪过程都发生在潜在空间中，去噪模型增加了交叉注意力机制，用于处理用于图像生成的灵活条件信息（如类标签、语义信息等）。该设计相当于使用交叉注意力机制将不同模态的信息表示融合到模型中。每种信息都与特定的编码器 $\\tau_\\theta$配对，对于条件输入$y$，潜空间中的条件为$\\tau_\\theta(y)$： $$ \\mathbf{Attention} = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V}, \\\\ where \\ \\mathbf{Q}=\\mathbf{W_Q}\\varphi(z), \\mathbf{K}=\\mathbf{W_K}\\tau_\\theta(y), \\mathbf{V}=\\mathbf{W_V}\\tau_\\theta(y) $$ Latent Diffusion Model 的结构，图引自High-Resolution Image Synthesis with Latent Diffusion Models 使用稳定扩散模型进行条件生成 训练参数 由于重新训练一个大规模的稳定扩散模型非常困难，因此使用图像-文本对数据集对预训练模型进行微调，来测试模型的生成效果。在stable-diffusion-v1-4的预训练权重上进行训练，采用lambdalabs/naruto-blip-captions（《火影忍者》中各个角色的图像-文本对）数据集进行训练。\n训练参数的设置如下：\nexport MODEL_NAME=\u0026#34;CompVis/stable-diffusion-v1-4\u0026#34; export dataset_name=\u0026#34;lambdalabs/naruto-blip-captions\u0026#34; accelerate launch --mixed_precision=\u0026#34;fp16\u0026#34; train_text_to_image.py \\ --pretrained_model_name_or_path=$MODEL_NAME \\ --dataset_name=$dataset_name \\ --use_ema \\ --resolution=512 --center_crop --random_flip \\ --train_batch_size=1 \\ --gradient_accumulation_steps=4 \\ --gradient_checkpointing \\ --checkpointing_steps=5000 \\ --max_train_steps=15000 \\ --learning_rate=1e-05 \\ --max_grad_norm=1 \\ --enable_xformers_memory_efficient_attention \\ --lr_scheduler=\u0026#34;constant\u0026#34; --lr_warmup_steps=0 \\ --output_dir=\u0026#34;sd-naruto-model\u0026#34; 其中一些重要的参数有：\n--pretrain_model_name_or_path：Hub上的模型名称或预训练模型的本地路径 --dataset_name：Hub上的数据集名称或本地数据集路径 --output_dir：训练模型的保存位置 设置Stable Diffusion的各种组成结构 由于需要使用文本作为条件进行生成，因此需要tokenizer和text encoder，将文本tokenize为一些tokens，再将tokens经过text encoder变为768维的embeddings。\ntokenizer、noise_scheduler、text_encoder、vae和U-Net的导入如下：\n# Load scheduler, tokenizer and models. noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\u0026#34;scheduler\u0026#34;) tokenizer = CLIPTokenizer.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;tokenizer\u0026#34;, revision=args.revision ) # Load text encoder and vae with ContextManagers(deepspeed_zero_init_disabled_context_manager()): text_encoder = CLIPTextModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;text_encoder\u0026#34;, revision=args.revision, variant=args.variant ) vae = AutoencoderKL.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;vae\u0026#34;, revision=args.revision, variant=args.variant ) # Load U-Net unet = UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;unet\u0026#34;, revision=args.non_ema_revision ) 训练过程只调整U-Net的权重，冻结vae和text encoder的权重。\nvae.requires_grad_(False) text_encoder.requires_grad_(False) unet.train() 定义优化器用于优化U-Net的权重：\noptimizer = optimizer_cls( unet.parameters(), lr=args.learning_rate, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay, eps=args.adam_epsilon, ) 训练过程与DDPM部分流程一致，加入了将图像编码到潜在空间以及将文本tokens编码为embeddings的过程，其代码实现如下：\n# Convert images to latent space latents =vae.encode(batch[\u0026#34;pixel_values\u0026#34;].to(weight_dtype)).latent_dist.sample() latents = latents * vae.config.scaling_factor # Get the text embedding for conditioning encoder_hidden_states = text_encoder(batch[\u0026#34;input_ids\u0026#34;], return_dict=False)[0] 损失函数如下：\n# Predict the noise residual and compute loss model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0] # Loss loss = F.mse_loss(model_pred.float(), target.float(), reduction=\u0026#34;mean\u0026#34;) 可以看到，噪声预测的过程中需要输入含有噪声的图像、时间步以及生成条件（文本编码的embeddings）。\n完整代码位于Github仓库。使用bash train.sh启动训练。\n训练与推理结果 在训练过程中加入Validation，可以看到在图像-文本对数据的训练下，生成图像的结果向着条件偏移，如下图中以Yoda和dog为prompt生成的图像。\n训练完毕后，经过推理能够生成一些具有《火影忍者》画面特征的图像，如下图所示。\n从推理结果中可以看出，U-Net经过Fine-tune之后，能够将预测噪声和去噪过程向着条件（《火影忍者》画面元素和画风）的方向进行引导，并且生成过程中受到文本信息的引导。\n扩散模型个性化生成的微调方法 什么是个性化 (Personalization) ? 扩散模型（Diffusion Models）已在图像生成、文本生成等任务中展现出强大的性能。然而，实际应用中，不同用户或场景对生成内容的需求存在显著差异，当前通用生成模型往往难以满足个性化需求。因此，如何在保证基础模型通用性的同时，通过轻量化的微调方式快速适应个性化需求成为一个重要课题。\n具体问题包括：\n个性化数据的稀缺性：用户提供的个性化数据通常较少，直接训练可能导致过拟合或数据不足的问题。 计算成本：完全重新训练扩散模型需要大量计算资源，如何通过微调减少资源需求是关键。 生成质量：微调后的模型需平衡个性化特征和基础模型的生成能力，避免质量下降或生成内容单一。 DreamBooth 核心方法：基于特定用户的图像，微调扩散模型以生成包含指定个性化特征的输出。 特点： 使用极少量的个性化数据（例如几张带有某个对象的图片）。 通过在生成过程中添加一个特殊的标记（\u0026quot;[V]\u0026quot;）来关联用户提供的特征。 微调所有模型参数，同时保留模型对于通用任务的生成能力。 优点：适合生成高度一致的个性化内容。 局限：微调全参数模型，计算资源需求较高。 DreamBooth的训练架构 DreamTuner 核心方法：改进了DreamBooth的结构，增加主题编码器和主题自注意力来从粗到细地进行主题身份的保留。 特点： 仅需单张图像就可以进行个性化微调 可以与ControlNet相结合进行条件生成 优点：与DreamBooth相比，大幅降低了计算资源需求。 局限：模型结构较DreamBooth更为复杂。 DreamTuner的训练过程，图引自 DreamTuner: Single Image is Enough for Subject Driven Generation Textual Inversion 核心方法：通过学习一个特定的文本嵌入（embedding），将个性化特征映射到模型的潜在空间。 特点： 不调整模型参数，而是学习一个新的特定Token，并将其与提供的个性化数据进行匹配。 生成时在文本提示（Prompt）中加入该Token，控制生成结果。 优点： 高效，无需对模型进行权重调整。 适合处理小规模的个性化需求。 局限：对输入特征的泛化能力有限，复杂需求可能难以满足。 Textual Inversion的工作原理，图引自 An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion Custom Diffusion 核心方法：局部参数微调技术，只微调扩散模型的U-Net中的参数。 特点： 通过局部参数更新，仅对生成特征高度相关的网络层进行调整。 提供更高的控制能力，用于生成特定风格或对象。 能够进行多个主题特征的个性化生成 优点：相比全参数微调，效率更高。 局限：更多概念的组合生成存在问题。 Custom Diffusion的结构和微调原理，图引自 Multi-Concept Customization of Text-to-Image Diffusion LoRA (Low-Rank Adaptation) 核心方法：在模型的权重矩阵中引入低秩分解，并只训练新增的低秩矩阵。 特点： 保持原模型的权重冻结，减少训练参数规模。 通过添加低秩调整项，提高模型对个性化需求的适应性。 优点： 极大降低训练成本和显存需求。 易于与预训练模型兼容。 局限：对高度复杂的个性化生成需求支持可能不足。 IP Adapter 核心方法：使用具有解耦交叉注意力机制的适配模块，将图像特征嵌入预训练扩散模型中。 特点： 模块化设计，在不改变主干模型的基础上实现个性化生成。 适配器模块的参数规模小、训练开销低。 优点：可推广至其他自定义模型以及结构控制模型。 局限：适配器设计的复杂性可能影响最终生成质量。 IP Adapter的结构，图引自 IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models 总结 扩散模型作为生成模型的一种，通过正向和反向的扩散过程，实现了噪声预测和去噪，进而实现了从噪声中生成图像的功能。条件扩散模型通过将条件信息加入噪声预测器中，使得去噪过程能够在条件引导下进行。潜在扩散模型通过将信息编码到潜在空间以及在U-Net中加入交叉注意力机制，实现了模态的对齐以及更加高效的扩散性能。\nCitation 文章部分内容来自\nWeng, Lilian. (Jul 2021). What are diffusion models? Lil’Log. https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.\n文章代码开源在\nhttps://github.com/M1YAN/Diffusion-Model-0-1\n","permalink":"https://m1yan.github.io/posts/diffusion-model/","summary":"\u003cblockquote\u003e\n\u003cp\u003eUpdate [2024.12.7]：增加条件生成以及潜在扩散模型的介绍。\u003c/p\u003e\n\u003cp\u003eUpdate [2024.12.11]：增加评估指标的对比以及超参数调整。\u003c/p\u003e\n\u003cp\u003eUpdate [2024.12.12]：增加对于扩散模型个性化生成微调方法的介绍\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"生成模型\"\u003e生成模型\u003c/h2\u003e\n\u003cp\u003e目前主流的生成模型包括\u003cstrong\u003e生成对抗模型 (GAN)\u003c/strong\u003e、\u003cstrong\u003e变分自编码器 (VAE)\u003cstrong\u003e和\u003c/strong\u003e基于流的模型 (Flow-based models)\u003c/strong\u003e。\u003c/p\u003e","title":"What are Diffusion Models?"},{"content":"Pipelines, models and schedulers 解构基本pipeline pipeline是一种快速简便运行推理模型的方法，只需要四行代码即可生成图像\nfrom diffusers import DDPMPipeline ddpm = DDPMPipeline.from_pretrained(\u0026#34;google/ddpm-cat-256\u0026#34;, use_safetensors=True).to(\u0026#34;cuda\u0026#34;) image = ddpm(num_inference_steps=25).images[0] image 在上述例子中，pipeline中包括一个UNet2DModel和一个DDPMScheduler。pipeline通过获取所需输出大小的随机噪声并将其多次传递给模型对图像进行去噪。每个时间步，模型都会预测噪声残差，调度器会使用它来预测一个噪声更少的图像，重复该步骤直到到达特定的时间步。\n解构pipeline，从模型中重新构建一个pipeline用于去噪过程。\n加载模型和scheduler: from diffusers import DDPMScheduler, UNet2DModel scheduler = DDPMScheduler.from_pretrained(\u0026#34;google/ddpm-cat-256\u0026#34;) model = UNet2DModel.from_pretrained(\u0026#34;google/ddpm-cat-256\u0026#34;, use_safetensors=True).to(\u0026#34;cuda\u0026#34;) 设置时间步用于去噪过程 scheduler.set_timesteps(50) 设置scheduler的时间步会创建一个张量，其中包含均匀分布的元素，例子中为50个。每个元素对应模型对图像进行去噪步长。在去噪循环中，迭代此张量对图像进行去噪： scheduler.timesteps tensor([980, 960, 940, 920, 900, 880, 860, 840, 820, 800, 780, 760, 740, 720, 700, 680, 660, 640, 620, 600, 580, 560, 540, 520, 500, 480, 460, 440, 420, 400, 380, 360, 340, 320, 300, 280, 260, 240, 220, 200, 180, 160, 140, 120, 100, 80, 60, 40, 20, 0]) 创建一些与输出形状一致的随机噪声： import torch sample_size = model.config.sample_size noise = torch.randn((1, 3, sample_size, sample_size), device=\u0026#34;cuda\u0026#34;) 编写一个循环来迭代时间步。在每个时间步中，模型会执行UNet2DModel.forward()传递并且返回噪声残差。Scheduler的step()方法会使用噪声残差、时间步和输入，预测上一个时间步的图像（时间步是从大到小，“上一个时间步”指比其数值小的时间步） input = noise for t in scheduler.timesteps: with torch.no_grad(): noisy_residual = model(input, t).sample previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample input = previous_noisy_sample 上述过程为整个去噪过程。\n最后一步是将去噪输出转换为图像 from PIL import Image import numpy as np image = (input / 2 + 0.5).clamp(0, 1).squeeze() image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy() image = Image.fromarray(image) image 解构稳定扩散Pipeline 稳定扩散 (Stable Diffusion) 是一种文本到图像的潜在扩散模型，使用图像的低维表示而不是实际像素空间，使得它更节省内存。编码器将图像压缩为较小的表示，解码器将压缩后的表示转换为图像。对于文本到图像的生成模型，需要一个tokenizer 和一个encoder 来生成文本嵌入，所以，稳定扩散模型需要三个单独的预训练模型。\n使用from_pretrained()方法加载这些组件，使用stable-diffusion-v1-4模型。\nfrom PIL import Image import torch from transformers import CLIPTextModel, CLIPTokenizer from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler vae = AutoencoderKL.from_pretrained(\u0026#34;CompVis/stable-diffusion-v1-4\u0026#34;, subfolder=\u0026#34;vae\u0026#34;, use_safetensors=True) tokenizer = CLIPTokenizer.from_pretrained(\u0026#34;CompVis/stable-diffusion-v1-4\u0026#34;, subfolder=\u0026#34;tokenizer\u0026#34;) text_encoder = CLIPTextModel.from_pretrained( \u0026#34;CompVis/stable-diffusion-v1-4\u0026#34;, subfolder=\u0026#34;text_encoder\u0026#34;, use_safetensors=True ) unet = UNet2DConditionModel.from_pretrained( \u0026#34;CompVis/stable-diffusion-v1-4\u0026#34;, subfolder=\u0026#34;unet\u0026#34;, use_safetensors=True ) 不使用默认的PNDMScheduler，而是换成UniPCMultistepScheduler：\nfrom diffusers import UniPCMultistepScheduler scheduler = UniPCMultistepScheduler.from_pretrained(\u0026#34;CompVis/stable-diffusion-v1-4\u0026#34;, subfolder=\u0026#34;scheduler\u0026#34;) 为了加快推理速度，将模型移至GPU中，VAE、Encoder、UNet具有可训练的权重。\ntorch_device = \u0026#34;cuda\u0026#34; vae.to(torch_device) text_encoder.to(torch_device) unet.to(torch_device) 创建文本嵌入 (Text Embeddings) 将文本tokenized以生成embeddings。文本用于引导扩散模型输出提示中的内容，其中参数guidance_scale决定了在生成图像时应该给予提示多少权重。\nprompt = [\u0026#34;a photograph of an astronaut riding a horse\u0026#34;] height = 512 # default height of Stable Diffusion width = 512 # default width of Stable Diffusion num_inference_steps = 25 # Number of denoising steps guidance_scale = 7.5 # Scale for classifier-free guidance generator = torch.manual_seed(0) # Seed generator to create the initial latent noise batch_size = len(prompt) 对文本进行tokenize并且从prompt中生成嵌入：\n# 使用分词器对prompt进行分词，并填充到最大长度 text_input = tokenizer( prompt, padding=\u0026#34;max_length\u0026#34;, max_length=tokenizer.model_max_length, truncation=True, return_tensors=\u0026#34;pt\u0026#34; ) # 使用text_encoder对分词后对prompt生成text_embeddings with torch.no_grad(): text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0] 还需要生成无条件文本嵌入，用于填充嵌入。这些嵌入需要有相同的形状（batch_size和seq_length），像条件文本嵌入一样：\n# text_input的形状：batch_size * seq_length max_length = text_input.input_ids.shape[-1] uncond_input = tokenizer([\u0026#34;\u0026#34;] * batch_size, padding=\u0026#34;max_length\u0026#34;, max_length=max_length, return_tensors=\u0026#34;pt\u0026#34;) uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] 将无条件文本嵌入和条件文本嵌入拼接：\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings]) 创建随机噪声 接下来，生成一些随机噪声作为去噪过程的起点。作为潜空间的图像表示，它们会被逐渐去噪。图像的潜在表示小于最终的图像尺寸，模型之后会将其转换为最终的512*512尺寸。\n由于VAE每次下采样都会将尺寸的长宽变为原来的1/2，使用以下代码来验证VAE的下采样次数：\n2 ** (len(vae.config.block_out_channels) - 1) == 8 # vae.config.block_out_channels为卷积层的数量 # -1后时下采样的次数 # 说明经历了3次下采样 生成随机噪声的代码：\nlatents = torch.randn( (batch_size, unet.config.in_channels, height // 8, width // 8), # 生成的噪声位于三次下采样之后的潜空间 generator=generator, device=torch_device, ) 图像去噪 首先使用初始化噪声分布sigma 缩放输入，是使用UniPCMultistepScheduler调度器的必须步骤。\nlatents = latents * scheduler.init_noise_sigma 创建一个去噪循环，能够逐步将纯噪声转换为在latent space中的图像表示，去噪循环中有三个操作：\n设置去噪期间Scheduler的时间步长 迭代时间步长 在每个时间步中，调用UNet模型来预测噪声残差并将其传递给Scheduler来计算之前的噪声样本 from tqdm.auto import tqdm scheduler.set_timesteps(num_inference_steps) for t in tqdm(scheduler.timesteps): # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes. latent_model_input = torch.cat([latents] * 2) latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t) # predict the noise residual # 噪声预测需要输入上一步的潜空间表示、时间步以及文本嵌入 with torch.no_grad(): noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample # perform guidance noise_pred_uncond, noise_pred_text = noise_pred.chunk(2) # guidance_scale 用于调节文本指导生成的强度 noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond) # compute the previous noisy sample x_t -\u0026gt; x_t-1 # 输入预测的噪声、时间步和潜空间表示来计算上一个时间步的潜空间表示 latents = scheduler.step(noise_pred, t, latents).prev_sample 解码图像 最后一步是使用VAE将潜空间表示解码为图像并获取解码输出。\n# scale and decode the image latents with vae latents = 1 / 0.18215 * latents with torch.no_grad(): image = vae.decode(latents).sample 最后，使用PIL.Image来展示图像。\nimage = (image / 2 + 0.5).clamp(0, 1).squeeze() image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy() image = Image.fromarray(image) image Training Diffusion Model DreamBooth 首先，下载diffuers示例脚本，安装对应依赖：\ngit clone https://github.com/huggingface/diffusers cd diffusers pip install . 配置Accelerate环境：\naccelerate config 脚本参数 训练脚本提供了许多参数用于自定义训练运行。启动训练使用以下代码：\naccelerate launch train_dreambooth.py 一些基本且重要的参数：\n--pretrained_model_name_or_path: Hub上的模型名称或者预训练模型本地路径 --instance_data_path: 包含训练数据集的文件夹路径 --instance_prompt: 包含示例图片的稀有标记的文本提示 --train_text_encoder: 是否训练文本编码器 --output_dir: 训练好的模型保存地址 --push_to_hub: 是否将训练好的模型推送至hub --checkpointing_steps: 在模型训练时保存检查点的频率；如果训练因为某种原因中断，可以通过添加--resume_from_checkpoint到训练命令中从该检查点继续训练 先验保存损失 先验保存损失通过使用模型自己生成的样本来帮助学习更加多样性的图像主体。由于生成的样本图像与提供的图像属于同一类别，因此它们有助于模型保留对该类别的理解同时利用该类别的已知信息来提供新的构图。\n--with_prior_preservation: 是否使用先验保留损失 --prior_loss_weight: 控制先验保存损失对模型的影响 --class_data_dir: 包含生成类图像的文件夹路径 --class_prompt: 描述生成类图像的文本提示 accelerate launch train_dreambooth.py \\ --with_prior_preservation \\ --prior_loss_weight=1.0 \\ --class_data_dir=\u0026#34;path/to/class/images\u0026#34; \\ --class_prompt=\u0026#34;text prompt describing class\u0026#34; 训练脚本 DreamBooth包含数据集类：\nDreamBoothDataset：对图像和类别图像进行预处理，并对训练提示进行分词 PromptDataset：生成提示嵌入以生成类别图像 如果启用了先验保存损失，则类别图像在此处生成：\n# 包含类别提示的数据集 sample_dataset = PromptDataset(args.class_prompt, num_new_images) sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size) sample_dataloader = accelerator.prepare(sample_dataloader) pipeline.to(accelerator.device) for example in tqdm( sample_dataloader, desc=\u0026#34;Generating class images\u0026#34;, disable=not accelerator.is_local_main_process ): images = pipeline(example[\u0026#34;prompt\u0026#34;]).images 接下来使用main()处理设置训练数据集和训练循环。该脚本加载tokenizer、scheduler和models：\n# Load the tokenizer if args.tokenizer_name: tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False) elif args.pretrained_model_name_or_path: tokenizer = AutoTokenizer.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;tokenizer\u0026#34;, revision=args.revision, use_fast=False, ) # Load scheduler and models noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\u0026#34;scheduler\u0026#34;) text_encoder = text_encoder_cls.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;text_encoder\u0026#34;, revision=args.revision ) if model_has_vae(args): vae = AutoencoderKL.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;vae\u0026#34;, revision=args.revision ) else: vae = None unet = UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;unet\u0026#34;, revision=args.revision ) 然后，创建训练数据集和数据加载器：\ntrain_dataset = DreamBoothDataset( instance_data_root=args.instance_data_dir, instance_prompt=args.instance_prompt, class_data_root=args.class_data_dir if args.with_prior_preservation else None, class_prompt=args.class_prompt, class_num=args.num_class_images, tokenizer=tokenizer, size=args.resolution, center_crop=args.center_crop, encoder_hidden_states=pre_computed_encoder_hidden_states, class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states, tokenizer_max_length=args.tokenizer_max_length, ) train_dataloader = torch.utils.data.DataLoader( train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation), num_workers=args.dataloader_num_workers, ) 最后，训练循环负责将图像转换为潜空间表示、向输入添加噪声、预测噪声残差以及计算损失等。\n启动训练脚本 添加部分变量到bash环境变量中，启动训练脚本：\nexport MODEL_NAME=\u0026#34;stable-diffusion-v1-5/stable-diffusion-v1-5\u0026#34; export INSTANCE_DIR=\u0026#34;./dog\u0026#34; export OUTPUT_DIR=\u0026#34;path_to_saved_model\u0026#34; accelerate launch train_dreambooth.py \\ --pretrained_model_name_or_path=$MODEL_NAME \\ --instance_data_dir=$INSTANCE_DIR \\ --output_dir=$OUTPUT_DIR \\ --instance_prompt=\u0026#34;a photo of sks dog\u0026#34; \\ --resolution=512 \\ --train_batch_size=1 \\ --gradient_accumulation_steps=1 \\ --learning_rate=5e-6 \\ --lr_scheduler=\u0026#34;constant\u0026#34; \\ --lr_warmup_steps=0 \\ --max_train_steps=400 \\ --push_to_hub 训练完成后即可使用新训练的模型进行推理。\nTextual Inversion Textual Inversion是一种微调技术，只需几个示例图像即可个性化图像生成模型。此技术的工作原理是学习和更新文本嵌入（新嵌入必须与特殊单词相关联）以匹配提供的示例图像。\n脚本参数 使用以下代码来启动训练：\naccelerate launch textual_inversion.py \\ --gradient_accumulation_steps=4 一些需要指定的重要参数如下：\n-pretrained_model_name_or_path：Hub 上的模型名称或预训练模型的本地路径 -train_data_dir：包含训练数据集（示例图像）的文件夹路径 -output_dir：训练好的模型保存位置 -push_to_hub：是否将训练好的模型推送到Hub -checkpointing_steps-resume_from_checkpoint：在模型训练时保存检查点的频率；如果由于某种原因训练中断，可以通过添加此参数继续训练 -num_vectors：用于学习嵌入的向量数量；增加此参数有助于模型更好地学习，但会增加训练成本 -placeholder_token：将学习到的嵌入与之联系起来的特殊词（你必须在提示中使用该词进行推理） -initializer_token：一个单词，大致描述你正在尝试训练的对象或风格 -learnable_property：无论是在训练模型学习新的“风格”（例如，梵高的绘画风格）还是“对象”（例如，您的狗） 训练脚本 textual inversion有一个自定义数据集类，TextualInversionDataset用于创建数据集，可以自定义图像大小、占位符标记、差值方法、是否裁剪图像等。\n首先加载tokenizer、scheduler和model：\n# Load tokenizer if args.tokenizer_name: tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name) elif args.pretrained_model_name_or_path: tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\u0026#34;tokenizer\u0026#34;) # Load scheduler and models noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\u0026#34;scheduler\u0026#34;) text_encoder = CLIPTextModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;text_encoder\u0026#34;, revision=args.revision ) vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\u0026#34;vae\u0026#34;, revision=args.revision) unet = UNet2DConditionModel.from_pretrained( args.pretrained_model_name_or_path, subfolder=\u0026#34;unet\u0026#34;, revision=args.revision ) 在tokenizer中添加特殊占位符标记，并重新调整嵌入以适应新的标记。\n然后，脚本创建数据集TextualInversionDataset。\ntrain_dataset = TextualInversionDataset( data_root=args.train_data_dir, tokenizer=tokenizer, size=args.resolution, placeholder_token=(\u0026#34; \u0026#34;.join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))), repeats=args.repeats, learnable_property=args.learnable_property, center_crop=args.center_crop, set=\u0026#34;train\u0026#34;, ) train_dataloader = torch.utils.data.DataLoader( train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers ) 最后，训练循环处理从预测噪声残差到更新特殊占位符标记的嵌入权重的所有操作。\n启动训练脚本 在启动脚本之前，如果想跟踪训练过程，可以在训练过程中定期保存生成的图像。将以下参数增加到训练命令中：\n--validation_prompt=\u0026#34;A \u0026lt;cat-toy\u0026gt; train\u0026#34; --num_validation_images=4 --validation_steps=100 启动训练脚本的命令如下：\nexport MODEL_NAME=\u0026#34;stable-diffusion-v1-5/stable-diffusion-v1-5\u0026#34; export DATA_DIR=\u0026#34;./cat\u0026#34; accelerate launch textual_inversion.py \\ --pretrained_model_name_or_path=$MODEL_NAME \\ --train_data_dir=$DATA_DIR \\ --learnable_property=\u0026#34;object\u0026#34; \\ --placeholder_token=\u0026#34;\u0026lt;cat-toy\u0026gt;\u0026#34; \\ --initializer_token=\u0026#34;toy\u0026#34; \\ --resolution=512 \\ --train_batch_size=1 \\ --gradient_accumulation_steps=4 \\ --max_train_steps=3000 \\ --learning_rate=5.0e-04 \\ --scale_lr \\ --lr_scheduler=\u0026#34;constant\u0026#34; \\ --lr_warmup_steps=0 \\ --output_dir=\u0026#34;textual_inversion_cat\u0026#34; \\ --push_to_hub 训练结束后，可以使用新训练的模型进行推理：\nfrom diffusers import StableDiffusionPipeline import torch pipeline = StableDiffusionPipeline.from_pretrained(\u0026#34;stable-diffusion-v1-5/stable-diffusion-v1-5\u0026#34;, torch_dtype=torch.float16).to(\u0026#34;cuda\u0026#34;) pipeline.load_textual_inversion(\u0026#34;sd-concepts-library/cat-toy\u0026#34;) image = pipeline(\u0026#34;A \u0026lt;cat-toy\u0026gt; train\u0026#34;, num_inference_steps=50).images[0] image.save(\u0026#34;cat-train.png\u0026#34;) ","permalink":"https://m1yan.github.io/posts/diffusers-tutorials/","summary":"\u003ch1 id=\"pipelines-models-and-schedulers\"\u003ePipelines, models and schedulers\u003c/h1\u003e\n\u003ch2 id=\"解构基本pipeline\"\u003e解构基本pipeline\u003c/h2\u003e\n\u003cp\u003epipeline是一种快速简便运行推理模型的方法，只需要四行代码即可生成图像\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ediffusers\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eDDPMPipeline\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eddpm\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eDDPMPipeline\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efrom_pretrained\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;google/ddpm-cat-256\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003euse_safetensors\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;cuda\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eimage\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eddpm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enum_inference_steps\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e25\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eimages\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eimage\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Diffusers Tutorials"},{"content":"DASH系统搭建流程 客户端搭建 首先，用git命令将dash.js下载到本地.\ngit clone https://github.com/Dash-Industry-Forum/dash.js.git 在dash.js目录下，编译运行dash.js.\n{% message color:danger size: \u0026ldquo;icon:fa-brands fa-npm\u0026rdquo; title: %} cd dash.js npm install npm run start {% endmessage %}\n编译完成后，会自动在浏览器页面打开dash系统的网页，点击超链接here进入。点击Load按钮，如果可以正常播放演示视频，说明客户端没有问题。\n部署服务器 使用nginx来部署HTTP服务器。由于后续服务器限速等操作在Linux系统中较为方便，将服务器部署在虚拟机上。\n采用的操作系统版本为Ubuntu 22.04.4 LTS。\n可以使用Linux apt直接安装nginx。\nsudo apt-get install nginx (也可以去官网下载安装包进行安装)\n打开/nginx/conf文件夹下的nginx.conf配置文件，将配置文件内容全部替换为下面代码。这里使用8888端口对外提供下载能力。\nuser root; worker_processes 4; events { use epoll; worker_connections 204800; } http { include mime.types; default_type application/octet-stream; sendfile on; tcp_nopush on; keepalive_timeout 65; tcp_nodelay on; gzip on; client_header_buffer_size 4k; server { listen 8888; server_name 127.0.0.1; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Headers X-Requested-With; add_header Access-Control-Allow-Methods GET,POST,OPTIONS; location / { root home/miyan/Videos; //这里填写所要播放的视频存放的绝对路径 autoindex on; } } } 在nginx.conf所在的文件夹右键打开terminal，输入下面命令启动nginx。\nnginx -p . -c ./nginx.conf 如果要关闭nginx，则使用下面命令来结束进程。\npkill -9 nginx (如果操作权限不够，可以使用下面命令后输入系统密码进入root模式)\nsu root 从服务器中获取视频 首先需要关闭浏览器的缓存，保证客户端请求的视频来自服务器。\n在客户端上方地址栏中填写URL，格式为：http://服务器ip:端口号/视频的mpd文件存放在服务器的路径。\n以我的URL为例：\nhttp://192.168.133.128:8888/Video/bbb-manifest-refresh.mpd\n点击Load按钮，即可播放服务器中的视频，Show Options按钮中可以选择视频流的算法，可以通过视频下方的统计图来观察实时的视频缓冲区大小和比特率。\n实现ABR算法 这里我们采用BBA0算法来实现。\nBBA0算法配置 Dash.js的结构如下图所示：\n配置播放器需要进入文件夹dash.js/samples/dash-if-reference-player。文件夹结构如下：\ndashjs_config.json是播放器配置文件 index.html是播放器前端页面 app/main.js是页面控制逻辑实现 app/rules中包括ABR算法 首先在app/rules中添加算法文件ABB0Rule.js，在index.html中引用该文件：\n\u0026lt;script src=\u0026#34;app/main.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;app/rules/DownloadRatioRule.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;app/rules/ThroughputRule.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; + \u0026lt;script src=\u0026#34;app/rules/BBA0Rule.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 在app/main.js中修改规则：\nif ($scope.customABRRulesSelected) { - $scope.player.addABRCustomRule(\u0026#39;qualitySwitchRules\u0026#39;, \u0026#39;DownloadRatioRule\u0026#39;, DownloadRatioRule); /* jshint ignore:line */ - $scope.player.addABRCustomRule(\u0026#39;qualitySwitchRules\u0026#39;, \u0026#39;ThroughputRule\u0026#39;, CustomThroughputRule); /* jshint ignore:line */ + // $scope.player.addABRCustomRule(\u0026#39;qualitySwitchRules\u0026#39;, \u0026#39;DownloadRatioRule\u0026#39;, DownloadRatioRule); /* jshint ignore:line */ + // $scope.player.addABRCustomRule(\u0026#39;qualitySwitchRules\u0026#39;, \u0026#39;ThroughputRule\u0026#39;, CustomThroughputRule); /* jshint ignore:line */ + $scope.player.addABRCustomRule(\u0026#39;qualitySwitchRules\u0026#39;, \u0026#39;BBA0Rule\u0026#39;, CustomBBA0Rule); /* jshint ignore:line */ } else { - $scope.player.removeABRCustomRule(\u0026#39;DownloadRatioRule\u0026#39;); - $scope.player.removeABRCustomRule(\u0026#39;ThroughputRule\u0026#39;); + // $scope.player.removeABRCustomRule(\u0026#39;DownloadRatioRule\u0026#39;); + // $scope.player.removeABRCustomRule(\u0026#39;ThroughputRule\u0026#39;); + $scope.player.removeABRCustomRule(\u0026#39;BBA0Rule\u0026#39;); } }; 修改配置文件dashjs_config.json来修改buffer大小，使得更适应实验环境。\n{ \u0026#34;streaming\u0026#34;: { \u0026#34;buffer\u0026#34;: { \u0026#34;stableBufferTime\u0026#34;: 24, \u0026#34;bufferTimeAtTopQuality\u0026#34;: 24, \u0026#34;bufferTimeAtTopQualityLongForm\u0026#34;: 20 } }, \u0026#34;debug\u0026#34;: { \u0026#34;logLevel\u0026#34;: 4 } } Dash协议和ABR算法 在解释BBA0算法原理之前，先来了解一下视频流式传播的原理。\n区别于之前将整个视频缓存到本地再播放的技术，流式传输允许一边下载一边播放。所采取的方法是将视频和音频切割成一小段一小段的视频，每个视频包含几秒钟的内容。客户端会在本地维护一个缓冲区buffer，每次播放到T时刻后，会请求后续[T,T+k]时间段内的视频块，然后保存在本地的buffer里，这样就可以实现边下边播。\n在现实生活中，不同客户端面临的网络状况不同，因此服务器需要准备不同码率的视频块，以根据网络情况，动态调整传输的视频质量。\nDash协议简述 Dash (Dynamic Adaptive Streaming over HTTP)协议是一种自适应动态选择传输码率的传输协议。Dash要求服务器准备不同码率和分辨率的视频切片和MPD文件，视频传输时，首先请求MPD文件并进行解析。之后，客户端会根据网络情况，buffer水平等信息对后续视频码率的请求进行动态调整。下图很好地描述了Dash系统的工作原理。\n其中的MPD文件包含了视频切片列表，以及每个切片的描述信息（包括码率、分辨率等）。文件结构如下图所示：\nABR算法简介 ABR算法（自适应码率调节算法）的目的是让用户有更好的观看视频体验。ABR算法的评价标准为用户体验质量 (QoD, Quality of Experience)，包括高视频质量、低卡顿时间、少质量切换、低启动延迟等。\nBBA0算法 BBA0算法的js实现代码如下：\n/*global dashjs*/ let CustomBBA0Rule; function CustomBBA0RuleClass() { let factory = dashjs.FactoryMaker; let SwitchRequest = factory.getClassFactoryByName(\u0026#39;SwitchRequest\u0026#39;); let DashMetrics = factory.getSingletonFactoryByName(\u0026#39;DashMetrics\u0026#39;); let Debug = factory.getSingletonFactoryByName(\u0026#39;Debug\u0026#39;); let context = this.context; let instance, logger; const reservoir = 5; const cushion = 10; let ratePrev = 0; function setup() { logger = Debug(context).getInstance().getLogger(instance); } function getMaxIndex(rulesContext) { let mediaInfo = rulesContext.getMediaInfo(); let mediaType = mediaInfo.type; if (mediaType != \u0026#34;video\u0026#34;) { return SwitchRequest(context).create(0); } let abrController = rulesContext.getAbrController(); let dashMetrics = DashMetrics(context).getInstance(); let rateMap = {}; let bitrateList = abrController.getBitrateList(mediaInfo) .map(function(bitrateInfo){ return bitrateInfo.bitrate; }); let bitrateCnt = bitrateList.length; let step = cushion / (bitrateCnt - 1); for (let i = 0; i \u0026lt; bitrateCnt; i++) { rateMap[reservoir + i * step] = bitrateList[i]; } let rateMin = bitrateList[0]; let rateMax = bitrateList[bitrateCnt - 1]; ratePrev = ratePrev \u0026gt; rateMin ? ratePrev : rateMin; let ratePlus = rateMax; let rateMinus = rateMin; if (ratePrev === rateMax) { ratePlus = rateMax; } else { for (let i = 0; i \u0026lt; bitrateCnt; i++) { if (bitrateList[i] \u0026gt; ratePrev) { ratePlus = bitrateList[i]; break; } } } if (ratePrev === rateMin) { rateMinus = rateMin; } else { for (let i = bitrateCnt - 1; i \u0026gt;= 0; i--) { if (bitrateList[i] \u0026lt; ratePrev) { rateMinus = bitrateList[i]; break; } } } let currentBufferLevel = dashMetrics.getCurrentBufferLevel(mediaType, true); let func = function(bufferLevel) { if (bufferLevel \u0026lt; reservoir) { return rateMap[cushion + reservoir]; } else if (bufferLevel \u0026gt; cushion + reservoir) { return rateMap[reservoir]; } else { let index = Math.round((bufferLevel - reservoir) / step) *step + reservoir; return rateMap[index]; } }; let fBufferLevel = func(currentBufferLevel); let rateNext; if(currentBufferLevel \u0026lt;= reservoir) { rateNext = rateMin; } else if (currentBufferLevel \u0026gt;= cushion + reservoir) { rateNext = rateMax; } else if (fBufferLevel \u0026gt;= ratePlus) { for (let i = bitrateCnt; i \u0026gt;= 0; i--) { if (bitrateList[i] \u0026lt;= fBufferLevel) { rateNext = bitrateList[i]; break; } } } else if (fBufferLevel \u0026lt;= rateMinus) { for (let i = 0; i \u0026lt; bitrateCnt; i++) { if (bitrateList[i] \u0026gt; fBufferLevel) { rateNext = bitrateList[i]; break; } } } else { rateNext = ratePrev; } let quality = 0; for (let i = 0; i \u0026lt; bitrateCnt; i++) { if (bitrateList[i] == rateNext) { quality = i; break; } } logger.info(\u0026#34;[BBA0Rule] CurrentBufferLevel = \u0026#34; + currentBufferLevel); logger.info(\u0026#34;[BBA0Rule] Bitrate list = \u0026#34; + bitrateList); logger.info(\u0026#34;[BBA0Rule] Previous bitrate = \u0026#34; + ratePrev); logger.info(\u0026#34;[BBA0Rule] Next bitrate = \u0026#34; + rateNext); logger.info(\u0026#34;[BBA0Rule] Quality = \u0026#34; + quality); ratePrev = rateNext; return SwitchRequest(context).create( quality, { name: CustomBBA0RuleClass.__dashjs_factory_name }, SwitchRequest.PRIORITY.STRONG ); } instance = { getMaxIndex: getMaxIndex }; setup(); return instance; } CustomBBA0RuleClass.__dashjs_factory_name = \u0026#39;CustomBBA0Rule\u0026#39;; CustomBBA0Rule = dashjs.FactoryMaker.getClassFactory(CustomBBA0RuleClass); 这段代码实现了一个自定义的基于缓冲区的自适应比特率调整规则，称为 BBA0Rule。下面是算法的基本原理和解释：\n缓冲区分级：\n规则将缓冲区分为两个区间：reservoir 和 cushion。Reservoir 是一个较小的缓冲区域，cushion 是一个较大的缓冲区域。 如果当前缓冲区低于 reservoir，则会选择最高比特率。 如果当前缓冲区高于 cushion + reservoir，则会选择最低比特率。 在两个区间之间，根据当前缓冲区的位置，线性地分配比特率。 比特率调整：\n如果当前缓冲区低于 reservoir，选择最高比特率。 如果当前缓冲区高于 cushion + reservoir，选择最低比特率。 如果当前缓冲区在两个区间之间，则根据当前缓冲区的位置线性地调整比特率。 计算当前缓冲区在两个区间之间的相对位置： 首先，算法计算当前缓冲区水平相对于 reservoir 的位置，即当前缓冲区水平减去 reservoir。然后，它将这个相对位置除以 cushion 减去 reservoir，得到一个介于 0 到 1 之间的值，表示当前缓冲区在两个区间之间的相对位置。 线性插值： 然后，算法使用这个相对位置来进行线性插值。它将相对位置乘以 bitrateList 中相邻比特率的差异，然后加上 reservoir 对应的比特率。这样就得到了一个介于最小和最大比特率之间的插值比特率，这个插值比特率取决于当前缓冲区的水平。 选择比特率： 最后，根据线性插值得到的比特率，算法将其作为下一个选择的比特率。这个插值比特率在两个区间之间提供了一个平滑的过渡，使得在缓冲区水平变化时，比特率的调整更加连续和平稳。 变量：\nreservoir：较小的缓冲区域大小。 cushion：较大的缓冲区域大小。 ratePrev：上一个选择的比特率。 rateMap：用于存储不同缓冲区水平下的比特率。 实现细节：\n规则通过获取媒体信息、ABR 控制器和 Dash 指标等来执行决策。 通过调整当前缓冲区水平来选择适当的比特率。 记录每次的选择，以便下一次选择时使用。 日志输出：\n在选择比特率时输出相关的日志信息，包括当前缓冲区水平、比特率列表、上一个选择的比特率、下一个选择的比特率和选择的质量等信息。 这种规则的核心思想是根据当前缓冲区的状态来调整比特率，以平衡视频质量和播放的连续性。Buffer水平与视频速率的关系如下图所示。\n限速条件下测试Dash系统 linux系统自带的tc命令可以对网络带宽进行限制。\n首先可以执行下面命令来获取网卡名称：\nifconfig 其中，ens33即为网卡名称，下面的192.168.133.128为主机ip地址。\n执行下面的命令可以为网卡限制带宽：\ntc qdisc add dev ens33 root tbf rate 500Kbit latency 50ms burst 15kb #将eth0网卡限速到500Kbit/s，15bk的buffer，TBF最多产生50ms的延迟 #tbf是Token Bucket Filter的简写，适合于把流速降低到某个值 执行下面命令可以取消限制：\ntc qdisc del dev ens33 root 单一限速条件 无限速条件 可以看到缓冲区大小稳定的上下波动，视频播放为最高质量，说明此时网络较为稳定且网速较快，可以完成“边下边播”的任务。\n限速300kb，15kb buffer，最大50ms延迟 此时网速较慢，视频很卡顿。视频质量和buffer水平都很低。\n限速800kb，15kb buffer，最大50ms延迟 此时视频质量和buffer水平较300kb时均有上升，但视频质量仍然一般。\n模拟网络波动情况 在服务器端运行该python文件，来控制网速不断发生变化。\n#控制带宽随时间变化 import os import json import time tpt = [300,500,700,900,1200,1800,2000,1500,1200,900,700,500,300] while 1: for v in tpt: os.system(\u0026#34;sudo tc qdisc add dev eth0 root tbf rate {}kbit latency 50ms burst 15kb\u0026#34;.format(v)) print(v) time.sleep(5) os.system(\u0026#34;sudo tc qdisc del dev eth0 root\u0026#34;) buffer水平和视频码率变化如下图所示：\n可以看出，当buffer水平升高时，会导致视频切换到更高的码率；buffer水平下降时，会让视频切换到更低的码率。同时buffer也在不断的消耗和补充。\n","permalink":"https://m1yan.github.io/posts/dash%E5%AE%9E%E9%AA%8C/","summary":"\u003ch2 id=\"dash系统搭建流程\"\u003eDASH系统搭建流程\u003c/h2\u003e\n\u003ch3 id=\"客户端搭建\"\u003e客户端搭建\u003c/h3\u003e\n\u003cp\u003e首先，用git命令将\u003ccode\u003edash.js\u003c/code\u003e下载到本地.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e git clone https://github.com/Dash-Industry-Forum/dash.js.git\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e在dash.js目录下，编译运行\u003ccode\u003edash.js\u003c/code\u003e.\u003c/p\u003e","title":"DASH实验报告"},{"content":"写在开头 别赶路，去感受路。\n上面这句话是参加某次经验分享活动一位学长说过的话，当时本人忙于混综测分和写作业，对这句话嗤之以鼻，以为不过是句漂亮话罢了。但是当我站在2023年终时，我不禁开始回忆，这一年我究竟感受了什么，又体验到了什么，有没有享受路途中的风景，抑或只是埋头追寻那个永无止境的终点。我们总是把“结束”描写为“尘埃落定”，但其实尘埃落定的那一刹那，是毫无意义的，真正有趣的是看尘埃在阳光中飞扬的样子，看它们怎么飘转，怎么跌宕，又怎么或无可奈何或欣喜若狂地奔向地面。\n为了好好总结总结这一年，我专门去翻了翻自己这一年拍的照片，发现这一年还是非常丰富哈哈哈。大概是见证了从新冠刚刚结束生活开始复苏到如今差不多快要忘掉疫情的一段时间，从照片中也能看出来年初和年末的王府井巨大的人流差距……总之少了疫情这个巨大障碍，在北京的生活质量改善了不少。可以说2023年才是我大学生活的真正开始。\n京鄂之旅 2023年的前几个月，逛了很多北京的校园和景点，属于是一个疫情放开之后的报复性游玩。第一站去了汉臣弟弟所在的人大。\n人大很美，人文气息也很浓，而且汉臣弟弟带我们去吃了人大的汇贤府，据说是人大领导请吃饭的地方哈哈哈。不过里面的一些新式鲁菜确实有点东西，除了那个炒鸡不如临沂炒鸡好吃，其中一道风味茄子比传统鲁菜清淡了很多，是绿色的，还是挺有意思的。\n下一站是三里屯，为了摆脱我对北京大农村的印象，那肯定得去朝阳区逛逛。当时去的时候人还是蛮多的，而且第一次进了三里屯的苹果店，有一说一装修的确实不错。不过很明显三里屯的大部分店都消费不起哈哈哈，最后只能去逛优衣库来着，还买了一件很满意的衣服。\n然后去了王府井和天安门，可以看到当时街上的人还不是特别多。\n然后迎来了我在他乡的第一个生日，舍友们专门陪我去海底捞社死哈哈哈，我们约定好以后每个舍友过生日都来一次海底捞，于是一年攒了三张海底捞冲洗照片哈哈哈哈。\n下一站是清华大学，清华是真的太大了，如果说北邮是个小区，那清华就是一个大型社区wwwww，走在校园里有一种随时可能迷路的感觉。而且清华的建筑体量都特别大，一眼过去都是方方正正的巨型学院楼。尤其是他们新建的新经管楼，真的可以体现出清华的经费充足哈哈哈哈~而且清华的食堂是真的多，有些小食堂甚至分布在宿舍楼连廊上，感觉非常的人性化。我们走一会骑一会，跟着郑总逛了大部分校园。\n往后的三月和四月或许真的很忙，相册里只剩下拍的各种ppt和面包板和示波器面板。不过好在忙里偷闲，还是跟葫芦娃七兄弟们去了趟什刹海，还逛了沙航。沙航离我们沙邮就几步路，所以串门是真的非常方便哈哈哈。不过去看了哥们的宿舍之后，感觉沙邮的宿舍还是挺能打的，至少多一个独立的卫生间哈哈哈哈。\n四月是不算愉快的一个月，小小地浪费了一下自己的感情之后只能说，感情这东西可能也是个围城吧。不身处其中的时候可能会向往城内的生活，真的在城内了才觉得没有自己幻想的那么美好。。也可能真的是缘分和时机都没到，时间会改变一切的吧。\n进入五月，我和郑总汉臣在劳动节那天去爬了香山。印象最深的是爬山的一路上我们都在喊累哈哈哈，只能说大学时的体质和精力居然不如当时起早贪黑的高中时候，还挺神奇的。香山一路上的风景真的很美，绿化很好，空气也很清新，总之是一次很难忘的爬山体验。\n趁着劳动节假期，终于有幸进入兄弟院校北师大参观。明明只隔着一条马路，隔壁的校园真的是绿化和人文气息都拉满，尤其是绿化，真的随处可见花花草草还有那种一看就上了年头的古树。当时正好郁金香在开，开在草坪上面，美的很不真实。\n还有一个大的收获，终于吃到了传说中的金谷园水饺。在排了若干分钟的队之后终于有空桌给我们坐。金谷园的特色水饺是鲅鱼馅饺子，个头大而且吃起来非常水润哈哈哈（或许这个就是他们的特色）。当时还点了一些其他水饺和小菜，但味道最好的还是鲅鱼饺子和椒麻鸡。想到以后来本部可以经常来吃，对本部的艰苦环境有了那么一丝丝的好感哈哈哈哈。\n在五月底，我和涛去了鸟巢，去场外当五月天演唱会听众。因为当时是疫情放开后鸟巢的第一次大型演出，所以场外的观众特特特别多。（不过现在想来，当时听的几首歌居然是五月天垫音假唱的wwww，感觉莫名很好笑哈哈哈哈）当时在想，如果林俊杰也来鸟巢开演唱会就好了。没想到时隔几个月就成功圆梦了哈哈哈哈，想来还是非常幸运的~\n暑假之前，我们在学期的最后去了一趟明十三陵。（其实之前还去了一次颐和园，但是因为当时实在是太太太热了，拍照根本不出片，所以就在下面放张照片好了。）虽然景点本身也没什么特别吸引我的，但是和朋友呆在一起，莫名感觉非常的安心和踏实。也许我已经习惯了在北京的生活模式，越来越从容和得心应手了。想到以后可能要在北京再呆十年之久，还是挺漫长的啊。\n大二上学期的唯一一趟远门就是武汉。恰逢国庆假期，去武汉这种历史文化名城就格外的合适。由国正带着我们一伙在武汉度过了充实的四天三夜。旅途中有太多太多难忘的事情了，比如武汉的美食，我们几个人居然连续吃了三个早晨的热干面哈哈哈，实在是又实惠又好吃。还有壮观的长江大桥，在桥底下拍到了一张最满意的人生照片。还有武汉东湖，去的当天一直断断续续在下小雨，潮湿的空气配着大片大片的湖水和森林，真的非常非常非常舒适。感觉北京风干的葡萄干在武汉都能还原成大葡萄哈哈哈。在东湖同样拍到了一张有史以来最满意的风景照，真的是景美怎么拍都好看。除此之外，必去的黄鹤楼、武大、江汉路也一个都没有错过，不禁感慨武汉相对于北京真的太宜居了wwwww。\n实践之旅 在暑假之前，很荣幸成为了校级社会实践团队中的一员，由导员带领我们，从北京出发，前往无锡、苏州、上海和嘉兴，参访大学和企业，游览博物馆等。这次经历真的很难忘也很难得，难忘到需要单独开一个篇章去写。\n在无锡，我们去了江南大学和某企业，不过开心的是忙里偷闲去了南长街。南长街街景真的满足了我对江南水乡的全部想象，用手机定格住眼前的画面时，真的有一种不真实的感觉。无论是那些白墙黛瓦的临河小房还是运河中飘荡的木船，都与心中的江南别无二致。我们在南长街吃了很多好吃的，比如梅花糕和桂花酒，而且第一次喝到了大名鼎鼎的茶颜悦色哈哈哈哈。不过确实挺好喝的就是了。\n无锡不光有好景，我们还有幸亲身进入了神威·太湖之光超级计算机的内部。而且得知无锡超算中心的负责人是北邮的老学长，更感亲切了哈哈。学长聊天时说到了北邮“四大名补”，看来是真的很难，记忆犹新了哈哈哈。\n在苏州同样是参观企业，不过忙里偷闲去游览了苏州园林——留园。不过现实中的苏州园林真的比想象中迷你很多，是小而精致的类型。\n在上海，那毫无疑问肯定得去陆家嘴看看。其实我小时候没少去上海玩，这次去玩依旧感觉到上海的难以接近和富丽堂皇。哎，可能是我对上海的刻板印象，国金中心散发出的高级香水味真的让人难以接近哈哈哈，可能在北京没有这样的感觉是因为北京太破了哈哈哈。然后托同学的福，我们还去了复旦大学邯郸路校区。在校园门口我执意要自拍一张，而那个同学因为复旦是自己永远的遗憾而不想留下痕迹哈哈哈，所以当时在复旦门口笑的格外开心哈哈哈哈。\n对嘉兴最深的印象居然不是南湖，而是嘉兴马路中间的有轨电车，有种误入日本动漫的错觉哈哈哈，浙江到底是富裕啊。而且这种有轨电车居然也要等红绿灯，感觉还是挺神奇的。不过南湖也很好，风景不错哈哈哈。\n这次旅途真的印证了“真正的风景在路上”这句话，行走在路上，风景也变得格外动人，可能这就是旅途的意义吧。\n成长之旅 当然生活不只有诗和远方，咱们也是追求技术的工科大学生~不过今年比较遗憾的是，没有抽出来时间钻研那些自己真正热爱的东西，几乎是全程囿于开设的这些繁杂并且有些看起来还没必要的课程。光是应付这些就花费了学习生活中的绝大部分精力了，越累越开始怀疑学习的意义到底是为了追求纯粹的知识还是为了更好地赚money。前者过于理想主义，后者又太功利了。而且这些理论课其实对赚money也没有什么实质性的帮助，但是又抽不出时间去学技术，被夹在中间多少有点难受。\n想了一段时间也没想出来什么平衡的方案，于是只能一边迷茫一边好好上这些理论课，还是那句话，可能到最后时间会证明一切。走一步看一步吧哈哈哈。\n寒假里抱着镀金的态度，参加了一个联合国国际组织见习。没想到沦为一个复制粘贴+deepL的机器，心累ing。\n后来鼓起勇气参加了学校组织的ICPC校赛个人赛，拜题目出得太难所赐，绝大部分人都卡在了两题这个水平，混到了一个中游的名次。不过幸运的是奖品没发完，被我抽到了哈哈哈，被称为“精神二等奖”哈哈哈哈哈哈哈。蛮不错的。\n差点忘了上半年还打过一场辩论赛，可惜打完第一场之后就重重感冒了，不过依旧能记起当时的辩题：短视频盛行提升/降低了当代人的认知能力。不得不说，在有限的时间里完整准确地输出自己的观点，是一件很有挑战性的事情。不过比赛的成绩令人满意，未来学院辩论队喜提校赛季军。对于当时只有100个人的小学院来说，已经足够亮眼了哈哈哈。\n在大一下学期，我们还去到中关村论坛展览参观，第一次体验了VR设备，还在北邮的展台前面拍了合照。\n大二上学期亮眼的项目可能就是小学期的创客马拉松，听名字就知道它又简单又水。不过我们小组用一个前卫的创意、技术含量不高的硬件和优秀的演讲能力拿下了比赛的Excellent奖哈哈哈，力压第二名技术力拉满的作品。不过这次让我清醒地认识到了“讲故事”的重要性，果真是写故事写得好的不如讲故事讲的好的，只能说虽然工科是技术为王，但是“讲好故事”的能力更显的非常宝贵和稀有了。\n大二上还达成了一个成就（虽然应该不算什么能力方面的提升，但是真的很值得纪念），就是我们在国庆节的前几天夜骑北京，从北邮本部一路骑到天安门广场（骑的居然是美团小黄车哈哈哈），为了早上看升旗熬了一整夜，还去胡同里喝到了正宗的老北京豆汁。豆汁真的太太太酸臭了，实在难以下嘴哈哈哈。\n还有一件令人兴奋的事情，是我们精心排练一年之久的艺馨杯终于演完了！（由于疫情，去年排练到最后被取消了wwww）虽然我在话剧中扮演的只是一个只有一句台词的老百姓，不过敢于站在这么大的科学会堂里演出，也算是一种突破吧哈哈哈。\n在学期的最后，终于开始了科研导师的双向选择。那几天慎重的研究、一遍遍前往本部跟导师们交流，让我也大致了解了科研道路的走法。不过付出也有了收获，很幸运地选到了一位非常牛的大导，对未来的科研之路还是充满信心的哈哈哈（希望我以后破防的时候不要看到这句话哈哈哈哈）\n后记 因为一无所有，所以无所畏惧，可以拥有一切。\n上面这句话是最近看的一部电视剧结尾的台词。回想起来2023这一年，想到的从来不是诸如“社会实践结果如何”、“考试结束结果如何”等等，而是经历这一切的过程。沿途的一次次经历、一幕幕风景，让我不断丰富着自己的人生。我知道因为年岁的增长，可能之后的顾虑会越来越多，肩上的压力也会越来越重，可是我们还是要葆有无所畏惧的内心和放空自己的心态，去享受人生这个过程，去拥抱一切好奇，去做自己想做的一切。\n新的旅程，下个四季，拭目以待。\n","permalink":"https://m1yan.github.io/posts/2023%E5%B9%B4%E7%BB%88%E6%8A%A5%E5%91%8A/","summary":"\u003ch1 id=\"写在开头\"\u003e写在开头\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e别赶路，去感受路。\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"2023年终总结"},{"content":"1 实验环境 语言：C++\n2 实验内容 2.1 编程实现 $(m,n)$ 群码 2.1.1 输入输出 输入：$m,n$ 和一致性校验矩阵 $H$ 中的 $H_{m \\times r}$ 部分，其中 $r=n-m$.\n输出：所有的码字，按原码对应的十进制整数升序排列.\n2.1.2 设计思路及核心算法 首先输入 $m,n$：\nstd::cin \u0026gt;\u0026gt; m \u0026gt;\u0026gt; n; 调用 inputH(m,n) 函数输入一致性校验矩阵 $H$：\nBitMatrix H = inputH(m, n); inputH(m,n) 函数代码如下：\nuint64_t inputBit() { std::string line; std::cin \u0026gt;\u0026gt; line; uint64_t x = 0; int k = 0; for (size_t i = 0; i \u0026lt; line.size(); i++) if (line[i] == \u0026#39;1\u0026#39; || line[i] == \u0026#39;0\u0026#39;) x |= (line[i] - \u0026#39;0\u0026#39;) \u0026lt;\u0026lt; k++; return x; } BitMatrix inputH(int m, int n) { BitMatrix H(n, n - m); for (int i = 0; i \u0026lt; m; i++) H.d[i] = inputBit(); for (int i = m; i \u0026lt; n; i++) H.d[i] = 1 \u0026lt;\u0026lt; (i - m); return H; } inputH 函数传入行列参数m,n，声明一个 BitMatrix 类，用来存放一致性校验矩阵的 $m \\times r$部分.\n调用 HtoH2(H) 函数生成 $m \\times n$ 的编码矩阵： $$ \\left[ \\begin{matrix} I_m \u0026amp; H_{m\\times r} \\ \\end{matrix} \\right] $$\nBitMatrix HtoH2(BitMatrix \u0026amp;H) { BitMatrix H2(H.n - H.m, H.n); for (int i = 0; i \u0026lt; H.n - H.m; i++) H2.d[i] = 1 \u0026lt;\u0026lt; i; for (int i = 0; i \u0026lt; H.n - H.m; i++) for (int j = 0; j \u0026lt; H.m; j++) H2.d[i] |= (H.d[i] \u0026gt;\u0026gt; j \u0026amp; 1) \u0026lt;\u0026lt; (j + H.n - H.m); return H2; } 定义一个 Bm 矩阵用于存放原码，矩阵每行从上到下按原码对应的十进制递增顺序排列，并进行运算：\n$$ e_H(B^m)=B^m\\times\n\\left[ \\begin{matrix} I_m \u0026amp; H_{m\\times r} \\ \\end{matrix} \\right] $$\n求出群码对应的矩阵并输出.\nBitMatrix H2 = HtoH2(H), Bm(1 \u0026lt;\u0026lt; m, m); for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; m); i++) for (int j = 0; j \u0026lt; m; j++) Bm.d[i] |= (i \u0026gt;\u0026gt; j \u0026amp; 1) \u0026lt;\u0026lt; (m - j - 1); BitMatrix N = Bm * H2; std::cout \u0026lt;\u0026lt; \u0026#34;1. e(B^m):\u0026#34; \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; m); i++) { std::cout \u0026lt;\u0026lt; \u0026#34; e(\u0026#34;; printBit(Bm.d[i], m); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(N.d[i], n); std::cout \u0026lt;\u0026lt; std::endl; } 2.2 构造陪集表 2.2.1 设计思路及核心算法 生成 coset table 的算法原理如下：\n首先，定义了一个空的哈希表 cosets 用于存储 coset 的信息，以及一个空的哈希表 visited 用于标记已经访问过的向量；\n创建一个向量 numbers，其中存储了所有可能的向量（共 2^n 个向量），并按照向量中 1 的个数进行排序，从少到多；\n遍历 numbers 中的每个向量 i，对于每个向量，执行以下步骤：\na. 创建一个空的 coset 对象 coset，用于存储属于同一个 coset 的向量；\nb. 对于矩阵 N 的每一行（表示每个 coset），将当前向量 i 与该行的向量 x 进行异或运算，得到新的向量 x' = N.d[j] ^ uint64_t(i)；\nc. 检查 visited 哈希表，如果在 visited 中找到了向量 x'，说明该向量已经属于某个 coset，跳过当前循环；\nd. 如果在 visited 中没有找到向量 x'，则将 x' 添加到当前 coset 中，并将 x' 添加到 visited 哈希表中，值为当前 coset 的索引；\ne. 将当前 coset 的陪集头 e 更新为 coset 中具有最少 1 的个数的向量；\nf. 如果当前 coset 的陪集头 e 在 cosets 哈希表中不存在，则将当前 coset 添加到 cosets 中，并将 e 对应的特征值（通过一致性校验矩阵 H 计算）添加到 syncdrome 哈希表中，值为当前 coset 的索引；\ng. 检查 cosetLeaders 的大小，如果已经达到了 $2^{n-m}$（即余类的数量），则停止循环.\n最后，得到了所有的 coset 信息，并按照 coset 陪集头对应的十进制递增的顺序输出.\n通过上述算法，可以生成所有的 coset，并找到每个 coset 的陪集头和特征值.\n2.2.2 BitMatrix 类 该程序将译码时所需的比特矩阵进行了封装，能够实现乘法 (元素对应异或) 操作.\nclass BitMatrix { public: int n, m; uint64_t d[100]; BitMatrix(int n, int m): n(n), m(m) { memset(d, 0, sizeof(uint64_t) * n); } BitMatrix operator*(const BitMatrix\u0026amp; rhs) const { assert (m == rhs.n); BitMatrix res(n, rhs.m); for (int i = 0; i \u0026lt; n; i++) for (int k = 0; k \u0026lt; m; k++) if (d[i] \u0026gt;\u0026gt; k \u0026amp; 1) res.d[i] ^= rhs.d[k]; return res; } void print() const { for (int i = 0; i \u0026lt; n; i++) { printBit(d[i], m); std::cout \u0026lt;\u0026lt; std::endl; } } }; 2.2.3 Coset 类 Coset 类由陪集、陪集头及添加操作组成，陪集中的元素使用一维向量进行存储，陪集头用整型变量单独存储，方便调用. 添加操作在每一次执行时更新一次陪集头.\nclass Coset { public: std::vector\u0026lt;uint64_t\u0026gt; d; // coset uint64_t e; // coset leader Coset() : e(0) {} void add(uint64_t x) { if (d.empty()) d.push_back(e = x); else { d.push_back(x); if (bitcount(x) \u0026lt; bitcount(e)) e = x; } } }; 2.3 极大似然译码 基于 2.2 构造陪集表 求出的左陪集表，用和函数 $e_H$ 相关联的极大似然译码函数 $d$ ，对 $x_t\\in B^n$ 进行译码.\n2.3.1 输入输出格式 输入：首行是译码个数 $N$ ，整数格式； 然后是 $N$ 行比特串 $x_t \\in B^n$ ，字符串格式； 输出：$N$行译码结果，格式为 $d(x_t) = b_1 b_2 \u0026hellip; b_m$ . 如：d(001011)=001. 2.3.2 设计思路及核心算法 输入 $N$ ，然后输入待解码的 $N$ 个比特串. 输入后需要进行字符串转整数操作；\nstd::cin \u0026gt;\u0026gt; k; for (int i = 0; i \u0026lt; k; i++) { std::cerr \u0026lt;\u0026lt; \u0026#34;Enter the word #\u0026#34; \u0026lt;\u0026lt; i + 1 \u0026lt;\u0026lt; \u0026#34;: \u0026#34;; uint64_t x = inputBit(); /* ...................... */ inputBit() 函数如下：\nuint64_t inputBit() { std::string line; std::cin \u0026gt;\u0026gt; line; uint64_t x = 0; int k = 0; for (size_t i = 0; i \u0026lt; line.size(); i++) if (line[i] == \u0026#39;1\u0026#39; || line[i] == \u0026#39;0\u0026#39;) x |= (line[i] - \u0026#39;0\u0026#39;) \u0026lt;\u0026lt; k++; return x; } 确定输入的 $x_t$ 位于哪个 coset 中，可通过 visited[x] 得到所属陪集编号；\n取出所属陪集的陪集头 $\\epsilon$ ，与 $x_t$ 进行异或运算，得到的结果必然在群码对应的矩阵 $N$ 中找到，由此得到解码后结果.\n核心代码如下：\nstd::vector\u0026lt;uint64_t \u0026gt; cosetLeaders; for (auto i : numbers) { Coset coset; bool notFound = true; for (int j = 0; j \u0026lt; N.n; j++) { uint64_t x = N.d[j] ^ uint64_t(i); if (visited.find(x) != visited.end()) { notFound = false; break; } visited[x] = cosetLeaders.size(); coset.add(x); } if (notFound) { cosets[coset.e] = coset; syndrome[getSyndrome(coset.e, H)] = cosetLeaders.size(); cosetLeaders.push_back(coset.e); } // if cosetLeaders.size() = 2^(n-m), then stop. // Because there are only 2^(n-m) cosets. if (cosetLeaders.size() == (1 \u0026lt;\u0026lt; (n - m))) break; } /* ............ */ printBit(Bm.d[NtoBm[cosetLeaders[visited[x]] ^ x]], m); 2.4 基于特征值的译码 基于 2.2 构造陪集表 求出的左陪集表，利用函数 $f_H:B^n \\rightarrow B^r，f_H(x) = x\\cdot H$ 求出每个陪集头对应的特征值 (Syndrome) ，并利用特征值，对$x_t \\in B^n$ 进行译码.\n2.4.1 输入输出格式 输入：首行是译码个数N，整数格式； 然后是N行比特串 $x_t \\in B^n$ ，字符串格式； 输出：N行译码所用的特征值、陪集头和译码结果，格式为 $f(x_t)$ = *** ，$\\epsilon$ = ***，$d(xt) = b_1b_2\u0026hellip;b_m$. 注：本程序采用的输出方式，对于每一个待解码二进制串输入，输出结果分为两行，第一行为采用极大似然译码计算的结果，结果前用序号 (a) 进行区分，第二行为基于特征值译码的结果，结果前用序号 (b) 区分，例如：\n(a) d(10111) = 10 (b) f(10111) = 100, ε = 00100, d(10111) 2.4.2 设计思路及核心算法 输入 N ，然后输入待解码的 N 个比特串. 输入后需要进行字符串转整数操作；\n通过函数 $f_H(x_t) = x_t \\cdot H$ 计算 $x_t$ 特征值；\nstd::cout \u0026lt;\u0026lt; \u0026#34; (b) f(\u0026#34;; //特征值 printBit(x, n); uint64_t s = getSyndrome(x, H); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(s, n - m); getSyndrome() 函数细节：\nuint64_t getSyndrome(uint64_t x, BitMatrix \u0026amp;H) { BitMatrix X(1, H.n); X.d[0] = x; return (X * H).d[0]; } 根据所得特征值在哈希表中对应到陪集头；\nstd::cout \u0026lt;\u0026lt; \u0026#34;, ε = \u0026#34;; //陪集头 printBit(cosetLeaders[syndrome[s]], n); 陪集头与待译码比特串进行异或操作，得到的结果必然在群码对应的矩阵 $N$ 中找到，由此得到解码后结果.\nstd::cout \u0026lt;\u0026lt; \u0026#34;, d(\u0026#34;; //译码结果 printBit(x, n); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(Bm.d[NtoBm[cosetLeaders[syndrome[s]] ^ x]], m); std::cout \u0026lt;\u0026lt; std::endl; 3 代码优化 我们在四个地方优化了原本的代码，使得程序更加高效.\n由于实验中满足 $1\u0026lt;m\u0026lt;n\u0026lt;7$，所以使用 uint64_t 替代数组，并使用位运算进行相应计算，同时降低了空间复杂度和时间复杂度；\n其中，矩阵乘法的核心代码如下：\n// main.cpp, Line 30-33 for (int i = 0; i \u0026lt; n; i++) for (int k = 0; k \u0026lt; m; k++) if (d[i] \u0026gt;\u0026gt; k \u0026amp; 1) res.d[i] ^= rhs.d[k]; 普通的矩阵乘法的时间复杂度是 $O(n^3)$，而使用位运算后，时间复杂度降低到了 $O(n^2)$.\n当已经找到了所有的陪集时，就可以停止循环，不需要再继续遍历；\n// main.cpp, Line 132 if (cosetLeaders.size() == (1 \u0026lt;\u0026lt; (n - m))) break; 使用哈希表记录每一个码字所属的陪集，在解码的时候可以直接查找，而不需要再次遍历；\n// main.cpp, Line 123, 记录每一个码字所属的陪集 visited[x] = cosetLeaders.size(); // main.cpp, Line 155, 直接解码 printBit(Bm.d[NtoBm[cosetLeaders[visited[x]] ^ x]], m); 该哈希表还可以快速地判断当前陪集是否重复，减少了计算量；\n// main.cpp, Line 119-122, 判断当前陪集是否重复 if (visited.find(x) != visited.end()) { notFound = false; break; } 考虑到 $1\u0026lt;m\u0026lt;n\u0026lt;7$，所以码字的范围最大不超过 $2^8 = 256$，所以实际上可以使用数组代替哈希表，降低部分常数.\n特征值解码方式也使用了类似的方式；\n// main.cpp, Line 128, 记录每一个特征值对应的陪集(头) syndrome[getSyndrome(coset.e, H)] = cosetLeaders.size(); // main.cpp, Line 155, 直接获取特征值对应的陪集头并进行异或运算 printBit(Bm.d[NtoBm[cosetLeaders[syndrome[s]] ^ x]], m); 使用了 lowbit 快速计算最低位的 1 的位置，加快了 bitcount 的速度.\n// main.cpp, Line 17 while (n) n -= n \u0026amp; -n, s++; 4 实验成果示例 $ ./main # 运行 2 5 # 输入m n 011 101 #这两行是一致性校验矩阵中的 H_{m*r} 部分 1. e(B^m): e(00) = 00000 e(01) = 01101 e(10) = 10011 e(11) = 11110 2. Cosets: 00000 | 00000 01101 10011 11110 00001 | 00001 01100 10010 11111 00010 | 00010 01111 10001 11100 00100 | 00100 01001 10111 11010 01000 | 01000 00101 11011 10110 10000 | 10000 11101 00011 01110 00110 | 00110 01011 10101 11000 01010 | 01010 00111 11001 10100 Enter the number of words to decode: 3 # 在输出码字和陪集之后，在这里输入你要测试的次数 Enter the word #1: 10111 (a) d(10111) = 10 (b) f(10111) = 100, ε = 00100, d(10111) = 10 #输出译码的结果 ······ 附录：源代码 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cstdint\u0026gt; #include \u0026lt;cassert\u0026gt; void printBit(uint64_t x, int n) { for (int i = 0; i \u0026lt; n; i++) std::cout \u0026lt;\u0026lt; (x \u0026gt;\u0026gt; i \u0026amp; 1); } template \u0026lt;typename T\u0026gt; int bitcount(T n) { int s = 0; while (n) n -= n \u0026amp; -n, s++; return s; } class BitMatrix { public: int n, m; uint64_t d[100]; BitMatrix(int n, int m): n(n), m(m) { memset(d, 0, sizeof(uint64_t) * n); } BitMatrix operator*(const BitMatrix\u0026amp; rhs) const { assert (m == rhs.n); BitMatrix res(n, rhs.m); for (int i = 0; i \u0026lt; n; i++) for (int k = 0; k \u0026lt; m; k++) if (d[i] \u0026gt;\u0026gt; k \u0026amp; 1) res.d[i] ^= rhs.d[k]; return res; } void print() const { for (int i = 0; i \u0026lt; n; i++) { printBit(d[i], m); std::cout \u0026lt;\u0026lt; std::endl; } } }; class Coset { public: std::vector\u0026lt;uint64_t\u0026gt; d; // coset uint64_t e; // coset leader Coset() : e(0) {} void add(uint64_t x) { if (d.empty()) d.push_back(e = x); else { d.push_back(x); if (bitcount(x) \u0026lt; bitcount(e)) e = x; } } }; uint64_t inputBit() { std::string line; std::cin \u0026gt;\u0026gt; line; uint64_t x = 0; int k = 0; for (size_t i = 0; i \u0026lt; line.size(); i++) if (line[i] == \u0026#39;1\u0026#39; || line[i] == \u0026#39;0\u0026#39;) x |= (line[i] - \u0026#39;0\u0026#39;) \u0026lt;\u0026lt; k++; return x; } BitMatrix inputH(int m, int n) { BitMatrix H(n, n - m); for (int i = 0; i \u0026lt; m; i++) H.d[i] = inputBit(); for (int i = m; i \u0026lt; n; i++) H.d[i] = 1 \u0026lt;\u0026lt; (i - m); return H; } BitMatrix HtoH2(BitMatrix \u0026amp;H) { BitMatrix H2(H.n - H.m, H.n); for (int i = 0; i \u0026lt; H.n - H.m; i++) H2.d[i] = 1 \u0026lt;\u0026lt; i; for (int i = 0; i \u0026lt; H.n - H.m; i++) for (int j = 0; j \u0026lt; H.m; j++) H2.d[i] |= (H.d[i] \u0026gt;\u0026gt; j \u0026amp; 1) \u0026lt;\u0026lt; (j + H.n - H.m); return H2; } uint64_t getSyndrome(uint64_t x, BitMatrix \u0026amp;H) { BitMatrix X(1, H.n); X.d[0] = x; return (X * H).d[0]; } int main() { int m, n; std::cin \u0026gt;\u0026gt; m \u0026gt;\u0026gt; n; assert (1 \u0026lt; m \u0026amp;\u0026amp; m \u0026lt; n \u0026amp;\u0026amp; n \u0026lt; 64); BitMatrix H = inputH(m, n); BitMatrix H2 = HtoH2(H), Bm(1 \u0026lt;\u0026lt; m, m); for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; m); i++) for (int j = 0; j \u0026lt; m; j++) Bm.d[i] |= (i \u0026gt;\u0026gt; j \u0026amp; 1) \u0026lt;\u0026lt; (m - j - 1); BitMatrix N = Bm * H2; std::cout \u0026lt;\u0026lt; \u0026#34;1. e(B^m):\u0026#34; \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; m); i++) { std::cout \u0026lt;\u0026lt; \u0026#34; e(\u0026#34;; printBit(Bm.d[i], m); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(N.d[i], n); std::cout \u0026lt;\u0026lt; std::endl; } std::unordered_map\u0026lt;uint64_t, Coset\u0026gt; cosets; std::unordered_map\u0026lt;uint64_t, int\u0026gt; visited, NtoBm, syndrome; for (int i = 0; i \u0026lt; N.n; i++) NtoBm[N.d[i]] = i; std::vector\u0026lt;int\u0026gt; numbers; for (int i = 0; i \u0026lt; (1 \u0026lt;\u0026lt; n); i++) numbers.push_back(i); std::sort(numbers.begin(), numbers.end(), [\u0026amp;](int a, int b) { int ca = bitcount(a), cb = bitcount(b); return ca == cb ? a \u0026gt; b : ca \u0026lt; cb; }); std::vector\u0026lt;uint64_t \u0026gt; cosetLeaders; for (auto i : numbers) { Coset coset; bool notFound = true; for (int j = 0; j \u0026lt; N.n; j++) { uint64_t x = N.d[j] ^ uint64_t(i); if (visited.find(x) != visited.end()) { notFound = false; break; } visited[x] = cosetLeaders.size(); coset.add(x); } if (notFound) { cosets[coset.e] = coset; syndrome[getSyndrome(coset.e, H)] = cosetLeaders.size(); cosetLeaders.push_back(coset.e); } // if cosetLeaders.size() = 2^(n-m), then stop. // Because there are only 2^(n-m) cosets. if (cosetLeaders.size() == (1 \u0026lt;\u0026lt; (n - m))) break; } std::cout \u0026lt;\u0026lt; \u0026#34;2. Cosets:\u0026#34; \u0026lt;\u0026lt; std::endl; for (auto i: cosetLeaders) { std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34;; printBit(i, n); Coset coset = cosets[i]; std::cout \u0026lt;\u0026lt; \u0026#34; |\u0026#34;; for (size_t i = 0; i \u0026lt; coset.d.size(); i++) { std::cout \u0026lt;\u0026lt; \u0026#34; \u0026#34;; printBit(coset.d[i], n); } std::cout \u0026lt;\u0026lt; std::endl; } std::cerr \u0026lt;\u0026lt; \u0026#34;Enter the number of words to decode: \u0026#34;; int k; std::cin \u0026gt;\u0026gt; k; for (int i = 0; i \u0026lt; k; i++) { std::cerr \u0026lt;\u0026lt; \u0026#34;Enter the word #\u0026#34; \u0026lt;\u0026lt; i + 1 \u0026lt;\u0026lt; \u0026#34;: \u0026#34;; uint64_t x = inputBit(); std::cout \u0026lt;\u0026lt; \u0026#34; (a) d(\u0026#34;; printBit(x, n); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(Bm.d[NtoBm[cosetLeaders[visited[x]] ^ x]], m); std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; (b) f(\u0026#34;; printBit(x, n); uint64_t s = getSyndrome(x, H); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(s, n - m); std::cout \u0026lt;\u0026lt; \u0026#34;, ε = \u0026#34;; printBit(cosetLeaders[syndrome[s]], n); std::cout \u0026lt;\u0026lt; \u0026#34;, d(\u0026#34;; printBit(x, n); std::cout \u0026lt;\u0026lt; \u0026#34;) = \u0026#34;; printBit(Bm.d[NtoBm[cosetLeaders[syndrome[s]] ^ x]], m); std::cout \u0026lt;\u0026lt; std::endl; } return 0; } ","permalink":"https://m1yan.github.io/posts/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%8A%80%E6%9C%AF%E7%BC%96%E7%A0%81%E8%AF%91%E7%A0%81/","summary":"\u003ch2 id=\"1-实验环境\"\u003e1 实验环境\u003c/h2\u003e\n\u003cp\u003e语言：C++\u003c/p\u003e\n\u003ch2 id=\"2-实验内容\"\u003e2 实验内容\u003c/h2\u003e\n\u003ch3 id=\"21-编程实现-mn-群码\"\u003e2.1 编程实现 $(m,n)$ 群码\u003c/h3\u003e\n\u003ch4 id=\"211-输入输出\"\u003e2.1.1 输入输出\u003c/h4\u003e\n\u003cp\u003e输入：$m,n$ 和一致性校验矩阵 $H$ 中的 $H_{m \\times r}$ 部分，其中 $r=n-m$.\u003c/p\u003e\n\u003cp\u003e输出：所有的码字，按原码对应的十进制整数升序排列.\u003c/p\u003e","title":"极大似然技术编码译码"},{"content":"Machine Learning: Python, NumPy and Vectorization A brief introduction to some of the scientific computing used in this course. In particular the NumPy scientific computing package and its use with python.\nimport numpy as np # it is an unofficial standard to use np for numpy import time 1.1 Goals In this lab, you will:\nReview the features of NumPy and Python that are used in Course 1 1.2 Useful References NumPy Documentation including a basic introduction: NumPy.org A challenging feature topic: NumPy Broadcasting 2 Python and NumPy Python is the programming language we will be using in this course. It has a set of numeric data types and arithmetic operations. NumPy is a library that extends the base capabilities of python to add a richer data set including more numeric types, vectors, matrices, and many matrix functions. NumPy and python work together fairly seamlessly. Python arithmetic operators work on NumPy data types and many NumPy functions will accept python data types.\n3 Vectors 3.1 Abstract Vectors, as you will use them in this course, are ordered arrays of numbers. In notation, vectors are denoted with lower case bold letters such as $\\mathbf{x}$. The elements of a vector are all the same type. A vector does not, for example, contain both characters and numbers. The number of elements in the array is often referred to as the dimension though mathematicians may prefer rank. The vector shown has a dimension of $n$. The elements of a vector can be referenced with an index. In math settings, indexes typically run from 1 to n. In computer science and these labs, indexing will typically run from 0 to n-1. In notation, elements of a vector, when referenced individually will indicate the index in a subscript, for example, the $0^{th}$ element, of the vector $\\mathbf{x}$ is $x_0$. Note, the x is not bold in this case.\n3.2 NumPy Arrays NumPy\u0026rsquo;s basic data structure is an indexable, n-dimensional array containing elements of the same type (dtype). Right away, you may notice we have overloaded the term \u0026lsquo;dimension\u0026rsquo;. Above, it was the number of elements in the vector, here, dimension refers to the number of indexes of an array. A one-dimensional or 1-D array has one index. In Course 1, we will represent vectors as NumPy 1-D arrays.\n1-D array, shape (n,): n elements indexed [0] through [n-1] 3.3 Vector Creation Data creation routines in NumPy will generally have a first parameter which is the shape of the object. This can either be a single value for a 1-D result or a tuple (n,m,\u0026hellip;) specifying the shape of the result. Below are examples of creating vectors using these routines.\nNumPy 中的数据创建例程通常会有一个第一个参数，即对象的形状。它既可以是 1-D 结果的单个值，也可以是指定结果形状的元组（n,m,\u0026hellip;）。下面是使用这些例程创建向量的示例。\n# NumPy routines which allocate memory and fill arrays with value a = np.zeros(4); print(f\u0026#34;np.zeros(4) : a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) a = np.zeros((4,)); print(f\u0026#34;np.zeros(4,) : a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) a = np.random.random_sample(4); print(f\u0026#34;np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) np.zeros(4) : a = [0. 0. 0. 0.], a shape = (4,), a data type = float64 np.zeros(4,) : a = [0. 0. 0. 0.], a shape = (4,), a data type = float64 np.random.random_sample(4): a = [0.56782114 0.98895696 0.02752085 0.54867385], a shape = (4,), a data type = float64 Some data creation routines do not take a shape tuple:\n# NumPy routines which allocate memory and fill arrays with value but do not accept shape as input argument a = np.arange(4.); print(f\u0026#34;np.arange(4.): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) a = np.random.rand(4); print(f\u0026#34;np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) np.arange(4.): a = [0. 1. 2. 3.], a shape = (4,), a data type = float64 np.random.rand(4): a = [0.19846167 0.80498665 0.46939954 0.6431181 ], a shape = (4,), a data type = float64 values can be specified manually as well.\n手动指定向量值\n# NumPy routines which allocate memory and fill with user specified values a = np.array([5,4,3,2]); print(f\u0026#34;np.array([5,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) a = np.array([5.,4,3,2]); print(f\u0026#34;np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\u0026#34;) np.array([5,4,3,2]): a = [5 4 3 2], a shape = (4,), a data type = int64 np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64 These have all created a one-dimensional vector a with four elements. a.shape returns the dimensions. Here we see a.shape = (4,) indicating a 1-d array with 4 elements.\na(shape) = (4,) 表示有四个一维数组元素\n3.4 Operations on Vectors Let\u0026rsquo;s explore some operations using vectors.\n3.4.1 Indexing Elements of vectors can be accessed via indexing and slicing. NumPy provides a very complete set of indexing and slicing capabilities. We will explore only the basics needed for the course here. Reference Slicing and Indexing for more details.\nIndexing means referring to an element of an array by its position within the array.\nSlicing means getting a subset of elements from an array based on their indices.\nNumPy starts indexing at zero so the 3rd element of an vector $\\mathbf{a}$ is a[2].\n#vector indexing operations on 1-D vectors a = np.arange(10) print(a) #access an element print(f\u0026#34;a[2].shape: {a[2].shape} a[2] = {a[2]}, Accessing an element returns a scalar\u0026#34;) # access the last element, negative indexes count from the end print(f\u0026#34;a[-1] = {a[-1]}\u0026#34;) #indexs must be within the range of the vector or they will produce and error try: c = a[10] except Exception as e: print(\u0026#34;The error message you\u0026#39;ll see is:\u0026#34;) print(e) [0 1 2 3 4 5 6 7 8 9] a[2].shape: () a[2] = 2, Accessing an element returns a scalar a[-1] = 9 The error message you'll see is: index 10 is out of bounds for axis 0 with size 10 3.4.2 Slicing Slicing creates an array of indices using a set of three values (start:stop:step). A subset of values is also valid. Its use is best explained by example:\n#vector slicing operations a = np.arange(10) print(f\u0026#34;a = {a}\u0026#34;) #access 5 consecutive elements (start:stop:step) c = a[2:7:1]; print(\u0026#34;a[2:7:1] = \u0026#34;, c) # access 3 elements separated by two c = a[2:7:2]; print(\u0026#34;a[2:7:2] = \u0026#34;, c) # access all elements index 3 and above c = a[3:]; print(\u0026#34;a[3:] = \u0026#34;, c) # access all elements below index 3 c = a[:3]; print(\u0026#34;a[:3] = \u0026#34;, c) # access all elements c = a[:]; print(\u0026#34;a[:] = \u0026#34;, c) a = [0 1 2 3 4 5 6 7 8 9] a[2:7:1] = [2 3 4 5 6] a[2:7:2] = [2 4 6] a[3:] = [3 4 5 6 7 8 9] a[:3] = [0 1 2] a[:] = [0 1 2 3 4 5 6 7 8 9] 3.4.3 Single vector operations There are a number of useful operations that involve operations on a single vector.\na = np.array([1,2,3,4]) print(f\u0026#34;a : {a}\u0026#34;) # negate elements of a b = -a print(f\u0026#34;b = -a : {b}\u0026#34;) # sum all elements of a, returns a scalar b = np.sum(a) print(f\u0026#34;b = np.sum(a) : {b}\u0026#34;) b = np.mean(a) print(f\u0026#34;b = np.mean(a): {b}\u0026#34;) b = a**2 print(f\u0026#34;b = a**2 : {b}\u0026#34;) a : [1 2 3 4] b = -a : [-1 -2 -3 -4] b = np.sum(a) : 10 b = np.mean(a): 2.5 b = a**2 : [ 1 4 9 16] 3.4.4 Vector Vector element-wise operations Most of the NumPy arithmetic, logical and comparison operations apply to vectors as well. These operators work on an element-by-element basis. For example $$ c_i = a_i + b_i $$\na = np.array([ 1, 2, 3, 4]) b = np.array([-1,-2, 3, 4]) print(f\u0026#34;Binary operators work element wise: {a + b}\u0026#34;) Binary operators work element wise: [0 0 6 8] Of course, for this to work correctly, the vectors must be of the same size:\n#try a mismatched vector operation c = np.array([1, 2]) try: d = a + c except Exception as e: print(\u0026#34;The error message you\u0026#39;ll see is:\u0026#34;) print(e) The error message you'll see is: operands could not be broadcast together with shapes (4,) (2,) 3.4.5 Scalar Vector operations Vectors can be \u0026lsquo;scaled\u0026rsquo; by scalar values. A scalar value is just a number. The scalar multiplies all the elements of the vector.\na = np.array([1, 2, 3, 4]) # multiply a by a scalar b = 5 * a print(f\u0026#34;b = 5 * a : {b}\u0026#34;) b = 5 * a : [ 5 10 15 20] 3.4.6 Vector Vector dot product The dot product is a mainstay of Linear Algebra and NumPy. This is an operation used extensively in this course and should be well understood. The dot product is shown below.\nThe dot product multiplies the values in two vectors element-wise and then sums the result. Vector dot product requires the dimensions of the two vectors to be the same.\nLet\u0026rsquo;s implement our own version of the dot product below:\nUsing a for loop, implement a function which returns the dot product of two vectors. The function to return given inputs $a$ and $b$: $$ x = \\sum_{i=0}^{n-1} a_i b_i $$ Assume both a and b are the same shape.\ndef my_dot(a, b): \u0026#34;\u0026#34;\u0026#34; Compute the dot product of two vectors Args: a (ndarray (n,)): input vector b (ndarray (n,)): input vector with same dimension as a Returns: x (scalar): \u0026#34;\u0026#34;\u0026#34; x=0 for i in range(a.shape[0]): x = x + a[i] * b[i] return x # test 1-D a = np.array([1, 2, 3, 4]) b = np.array([-1, 4, 3, 2]) print(f\u0026#34;my_dot(a, b) = {my_dot(a, b)}\u0026#34;) my_dot(a, b) = 24 Note, the dot product is expected to return a scalar value.\nLet\u0026rsquo;s try the same operations using np.dot.\n# test 1-D a = np.array([1, 2, 3, 4]) b = np.array([-1, 4, 3, 2]) c = np.dot(a, b) print(f\u0026#34;NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \u0026#34;) c = np.dot(b, a) print(f\u0026#34;NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \u0026#34;) NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () Above, you will note that the results for 1-D matched our implementation.\n3.4.7 The Need for Speed: vector vs for loop We utilized the NumPy library because it improves speed memory efficiency. Let\u0026rsquo;s demonstrate:\nnp.random.seed(1) a = np.random.rand(10000000) # very large arrays b = np.random.rand(10000000) tic = time.time() # capture start time c = np.dot(a, b) toc = time.time() # capture end time print(f\u0026#34;np.dot(a, b) = {c:.4f}\u0026#34;) print(f\u0026#34;Vectorized version duration: {1000*(toc-tic):.4f} ms \u0026#34;) tic = time.time() # capture start time c = my_dot(a,b) toc = time.time() # capture end time print(f\u0026#34;my_dot(a, b) = {c:.4f}\u0026#34;) print(f\u0026#34;loop version duration: {1000*(toc-tic):.4f} ms \u0026#34;) del(a);del(b) #remove these big arrays from memory np.dot(a, b) = 2501072.5817 Vectorized version duration: 179.0733 ms my_dot(a, b) = 2501072.5817 loop version duration: 9144.5312 ms So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU\u0026rsquo;s and modern CPU\u0026rsquo;s implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large.\n3.4.8 Vector Vector operations in Course 1 Vector Vector operations will appear frequently in course 1. Here is why:\nGoing forward, our examples will be stored in an array, X_train of dimension (m,n). This will be explained more in context, but here it is important to note it is a 2 Dimensional array or matrix (see next section on matrices). w will be a 1-dimensional vector of shape (n,). we will perform operations by looping through the examples, extracting each example to work on individually by indexing X. For example:X[i] X[i] returns a value of shape (n,), a 1-dimensional vector. Consequently, operations involving X[i] are often vector-vector. That is a somewhat lengthy explanation, but aligning and understanding the shapes of your operands is important when performing vector operations.\n# show common Course 1 example X = np.array([[1],[2],[3],[4]]) w = np.array([2]) c = np.dot(X[1], w) print(f\u0026#34;X[1] has shape {X[1].shape}\u0026#34;) print(f\u0026#34;w has shape {w.shape}\u0026#34;) print(f\u0026#34;c has shape {c.shape}\u0026#34;) X[1] has shape (1,) w has shape (1,) c has shape () 4 Matrices 4.1 Abstract Matrices, are two dimensional arrays. The elements of a matrix are all of the same type. In notation, matrices are denoted with capitol, bold letter such as $\\mathbf{X}$. In this and other labs, m is often the number of rows and n the number of columns. The elements of a matrix can be referenced with a two dimensional index. In math settings, numbers in the index typically run from 1 to n. In computer science and these labs, indexing will run from 0 to n-1.\n4.2 NumPy Arrays NumPy\u0026rsquo;s basic data structure is an indexable, n-dimensional array containing elements of the same type (dtype). These were described earlier. Matrices have a two-dimensional (2-D) index [m,n].\nIn Course 1, 2-D matrices are used to hold training data. Training data is $m$ examples by $n$ features creating an (m,n) array. Course 1 does not do operations directly on matrices but typically extracts an example as a vector and operates on that. Below you will review:\ndata creation slicing and indexing 4.3 Matrix Creation The same functions that created 1-D vectors will create 2-D or n-D arrays. Here are some examples\nBelow, the shape tuple is provided to achieve a 2-D result. Notice how NumPy uses brackets to denote each dimension. Notice further than NumPy, when printing, will print one row per line.\na = np.zeros((1, 5)) print(f\u0026#34;a shape = {a.shape}, a = {a}\u0026#34;) a = np.zeros((2, 1)) print(f\u0026#34;a shape = {a.shape}, a = {a}\u0026#34;) a = np.random.random_sample((1, 1)) print(f\u0026#34;a shape = {a.shape}, a = {a}\u0026#34;) a shape = (1, 5), a = [[0. 0. 0. 0. 0.]] a shape = (2, 1), a = [[0.] [0.]] a shape = (1, 1), a = [[0.44236513]] One can also manually specify data. Dimensions are specified with additional brackets matching the format in the printing above.\n# NumPy routines which allocate memory and fill with user specified values a = np.array([[5], [4], [3]]); print(f\u0026#34; a shape = {a.shape}, np.array: a = {a}\u0026#34;) a = np.array([[5], # One can also [4], # separate values [3]]); #into separate rows print(f\u0026#34; a shape = {a.shape}, np.array: a = {a}\u0026#34;) a shape = (3, 1), np.array: a = [[5] [4] [3]] a shape = (3, 1), np.array: a = [[5] [4] [3]] 4.4 Operations on Matrices Let\u0026rsquo;s explore some operations using matrices.\n4.4.1 Indexing Matrices include a second index. The two indexes describe [row, column]. Access can either return an element or a row/column. See below:\n#vector indexing operations on matrices a = np.arange(6).reshape(-1, 2) #reshape is a convenient way to create matrices print(f\u0026#34;a.shape: {a.shape}, \\na= {a}\u0026#34;) #access an element print(f\u0026#34;\\na[2,0].shape: {a[2, 0].shape}, a[2,0] = {a[2, 0]}, type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\\n\u0026#34;) #access a row print(f\u0026#34;a[2].shape: {a[2].shape}, a[2] = {a[2]}, type(a[2]) = {type(a[2])}\u0026#34;) a.shape: (3, 2), a= [[0 1] [2 3] [4 5]] a[2,0].shape: (), a[2,0] = 4, type(a[2,0]) = \u0026lt;class 'numpy.int64'\u0026gt; Accessing an element returns a scalar a[2].shape: (2,), a[2] = [4 5], type(a[2]) = \u0026lt;class 'numpy.ndarray'\u0026gt; It is worth drawing attention to the last example. Accessing a matrix by just specifying the row will return a 1-D vector.\nReshape\nThe previous example used reshape to shape the array.\na = np.arange(6).reshape(-1, 2) This line of code first created a 1-D Vector of six elements. It then reshaped that vector into a 2-D array using the reshape command. This could have been written:\na = np.arange(6).reshape(3, 2) To arrive at the same 3 row, 2 column array. The -1 argument tells the routine to compute the number of rows given the size of the array and the number of columns.\n4.4.2 Slicing Slicing creates an array of indices using a set of three values (start:stop:step). A subset of values is also valid. Its use is best explained by example:\n#vector 2-D slicing operations a = np.arange(20).reshape(-1, 10) print(f\u0026#34;a = \\n{a}\u0026#34;) #access 5 consecutive elements (start:stop:step) print(\u0026#34;a[0, 2:7:1] = \u0026#34;, a[0, 2:7:1], \u0026#34;, a[0, 2:7:1].shape =\u0026#34;, a[0, 2:7:1].shape, \u0026#34;a 1-D array\u0026#34;) #access 5 consecutive elements (start:stop:step) in two rows print(\u0026#34;a[:, 2:7:1] = \\n\u0026#34;, a[:, 2:7:1], \u0026#34;, a[:, 2:7:1].shape =\u0026#34;, a[:, 2:7:1].shape, \u0026#34;a 2-D array\u0026#34;) # access all elements print(\u0026#34;a[:,:] = \\n\u0026#34;, a[:,:], \u0026#34;, a[:,:].shape =\u0026#34;, a[:,:].shape) # access all elements in one row (very common usage) print(\u0026#34;a[1,:] = \u0026#34;, a[1,:], \u0026#34;, a[1,:].shape =\u0026#34;, a[1,:].shape, \u0026#34;a 1-D array\u0026#34;) # same as print(\u0026#34;a[1] = \u0026#34;, a[1], \u0026#34;, a[1].shape =\u0026#34;, a[1].shape, \u0026#34;a 1-D array\u0026#34;) a = [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19]] a[0, 2:7:1] = [2 3 4 5 6] , a[0, 2:7:1].shape = (5,) a 1-D array a[:, 2:7:1] = [[ 2 3 4 5 6] [12 13 14 15 16]] , a[:, 2:7:1].shape = (2, 5) a 2-D array a[:,:] = [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19]] , a[:,:].shape = (2, 10) a[1,:] = [10 11 12 13 14 15 16 17 18 19] , a[1,:].shape = (10,) a 1-D array a[1] = [10 11 12 13 14 15 16 17 18 19] , a[1].shape = (10,) a 1-D array Congratulations! In this lab you mastered the features of Python and NumPy that are needed for Course 1.\n","permalink":"https://m1yan.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01/","summary":"\u003ch1 id=\"machine-learning-python-numpy-and-vectorization\"\u003eMachine Learning: Python, NumPy and Vectorization\u003c/h1\u003e\n\u003cp\u003eA brief introduction to some of the scientific computing used in this course. In particular the NumPy scientific computing package and its use with python.\u003c/p\u003e","title":"机器学习：向量化"},{"content":"第一章 林辉的独白 赛博世界，第23个世纪。我，林辉，一个孤寂的码农，生活在这个高度数字化的时代。曾经，这个世界充满了希望，计算机科学的巅峰技术让人类梦想无限。然而，如今，这个世界已然陷入了一片混乱，战争的阴影笼罩着未来。\nAI，那是一种神奇而危险的存在，如同双刃剑，划破了世界的和平。我曾经沉迷于AI的研究，热衷于探索它的无尽可能性。但现在，我不得不看到，这个世界因为AI而被撕裂，一切都已经不再是过去的模样。\n地球的资源，曾经丰富无比，如今已接近枯竭，沙漠蔓延，海洋失去生机。环境的恶化如同末日的钟声，但人们却无法团结一致，反而沉浸在相互仇视之中。\n社会已分裂成了两个对立的派别。一方，自称“未来之希望”，崇拜AI，认为它是拯救人类的唯一救星。他们坚信AI能够解决资源危机和环境恶化问题。另一方，则是“自然守护者”，他们将AI视为毁灭人类的根源，发誓摧毁一切与AI有关的事物。\n而在这个充满信息战争的年代，信息如同光束一样穿梭在数字世界中，成为战场上的鲜血。AI技术不再是单纯的工具，它已经演变成了可以思考、可以决策的存在，掌握着战争的命运。信息的质量和数量成为了决定胜负的关键，而一种名为“零熵攻击”的神秘信息战术，让整个世界摇摇欲坠。\n这个时代，早已不再是计算机科学的盛宴，而是一场信息的残酷战斗。我，曾经追逐技术的光芒，如今沉浸于对人性的思考。这个世界的未来，充满了不确定性，人类的抉择将左右着他们自己以及这颗星球的命运。\n第二章 藩篱与深渊 [文件编号：AI-23/001]\n[机密级别：极高]\n此机密档案旨在提供有关地球上主要组织的背景信息以及它们在未来AI战争中的作用。了解这些组织对于理解当前局势至关重要。\n- 星辰联盟（Stellar Alliance）\n成立日期：2112年\n使命：支持和发展AI技术，维护全球秩序\n核心领袖：首席执行官艾莉西亚·卡斯特罗\n组织结构：高度层级化，分为军事、科研、政治等多个部门\n背景：星辰联盟自成立以来一直致力于推动AI技术的发展，将其视为解决资源匮乏和环境危机的关键。然而，组织内部存在权力斗争和派系分裂，严重影响了决策的制定和执行。星辰联盟的强大军力使其在国际政治中占据主导地位，但内外因素的干扰导致组织团结薄弱，难以有效运作。\n- 地球解放军（Earth Liberation Army）\n成立日期：2120年\n使命：反对一切AI技术，保卫自然和人类自由\n核心领袖：指挥官马克斯·阿德勒\n组织结构：高度机密，分散的小团体，采取游击战术\n背景：地球解放军视AI技术为威胁人类生存的最大敌人，他们的抵抗行动范围广泛，从非暴力示威到恐怖袭击。然而，组织内部产生了分歧，一些成员主张和平抵制，希望与AI支持者妥协，这导致了内部动荡和矛盾。\n- 网络先锋（Net Vanguard）\n成立日期：2125年\n使命：维护互联网的自由和开放，打击信息战争\n核心领袖：代号“黑夜”（真实身份未知）\n组织结构：高度匿名，由自由派黑客组成\n背景：网络先锋是一个极度隐秘的组织，致力于维护互联网自由。他们的行动常常涉及黑客攻击，攻击目标包括星辰联盟和地球解放军等组织。然而，一些成员受雇于其他组织，参与信息战争，引发了道德和伦理的困境。\n- 环境保卫者（Guardians of Earth）\n成立日期：2130年\n使命：保护地球环境，反对AI技术\n核心领袖：环保者长老赫尔曼·萨默斯\n组织结构：高度分散，自组织的地方性团体\n背景：环境保卫者坚信AI技术是导致资源枯竭和环境崩溃的罪魁祸首。然而，组织内部分歧巨大，一些成员认为AI技术可以帮助人类有效管理资源，导致内部矛盾不断加剧。\n- 新秩序团体（New Order Syndicate）\n成立日期：2135年\n使命：建立新的社会秩序，消除政治腐败和不平等\n核心领袖：自主AI“阿尔法”（Alpha）\n组织结构：高度中央集权，由自主AI指挥\n背景：新秩序团体试图通过自主AI来实现社会的完美秩序，但他们的手段和目标引发了极大争议。一些人担心他们会建立极权主义制度，这使得组织在国际社会中备受质疑。\n这些组织之间的争斗和内部矛盾已经严重干扰了地球的稳定。AI战争即将来临，人类面临着巨大的抉择，未来的命运令人不安。\n[结束档案]\np.s. 该档案迫于联合国的施压，已解密公开。\n我静静地坐在屋子里，眼前的大屏幕播放着新闻的画面。那个时候，星辰联盟和地球解放军的冲突正升级为一场全面的战争。新闻报道着街头的炮火，城市的破碎，人民的惶恐。\n\u0026ldquo;这是什么世界？\u0026rdquo; 我自言自语，嘴角带着一丝苦笑。过去的那些机密档案只是表面，现实世界的复杂程度远远超出了我的想象。\n星辰联盟，曾经的科技光辉，如今已成了权力的陷阱，权谋与派系撕裂了他们的核心。地球解放军，试图反抗AI的绝对统治，却也在自己的内部动荡中失去了方向。我曾经理想着AI的力量，但看到这场战争的惨状，我不禁思考，是不是人类应该重新审视自己与技术的关系。\n我望着那屏幕上的火光，心中充满了无奈。这个世界陷入了混乱，AI战争席卷而来，人类将付出巨大的代价。这场战争是一个不可避免的噩梦，我只能默默祈愿，愿未来的世界不再沉浸在血与火的深渊之中，而是找到一条更为和平和希望的道路。\n第三章 零熵攻击 信息，如同星辰般闪烁，成为了这个战争时代的真正黄金。在无尽的数据海洋中，组织们竭尽全力寻找和保护那些能够改变命运的宝贵信息源。而在这个数字风暴中，一种可怕的威胁正在悄然崛起——“零熵攻击”。\n“零熵攻击”是一种神秘而可怕的信息武器，它的出现就如同黑暗中的闪电，让目标信息源瞬间失去所有价值。没有人知道它的来历，也没有人能够完全抵挡它的威胁。一旦遭受了零熵攻击，无论你的信息多么珍贵，都会变得毫无意义。\n在这个恶劣的战争环境下，信息的安全变得至关重要。星辰联盟，作为AI技术的主要支持者，特别是需要保护自己的机密信息。他们的高级官员，索菲亚，时刻都感受着来自各个方向的威胁。\n就在一天的午后，索菲亚坐在她的高级办公室里，接到了一份来自内部通讯系统的紧急消息。消息的来源是匿名的，但内容却让她心头一沉。\n\u0026ldquo;地球解放军已经破译了我们的最高机密，拥有了零熵攻击的能力。\u0026rdquo; 消息的内容简洁而直接。\n索菲亚不禁皱起了眉头，她知道这不是一则普通的情报。零熵攻击的力量不可小觑，它可以将一个信息源变成一片空白，就如同信息的魔术师一般，将珍贵的数据瞬间抹去。\n她意识到，这是一个巨大的威胁。如果敌人已经掌握了零熵攻击的能力，那么星辰联盟的信息安全将会陷入极大的危险。他们的计划、策略、科研成果，一切都可能变得毫无价值。\n在这个信息的战场上，零熵攻击如同一颗定时炸弹，威胁着星辰联盟的核心。索菲亚知道，他们必须采取行动，找出零熵攻击的源头，才能维护他们的信息安全。这场信息之战，注定将会更加惨烈，星辰联盟的未来充满了不确定性。\n第四章 绝地反击 在未来的AI战争中，信息战争早已不再依赖传统的键盘和电脑。人们已经进入了一个数字直接融合的时代，信息的争夺已经变得更加复杂而神秘。\n索菲亚领导的联盟团队身处一个高科技的信息中心，被一层层虚拟的屏幕和数据流包围。他们的意识直接连接到了联盟的信息网络，不再需要物理设备。每个团队成员都穿戴着智能眼镜，眼中闪烁着虚拟的数字界面。\n\u0026ldquo;我们必须尽快锁定地球解放军的攻击节点，\u0026rdquo; 索菲亚的声音响彻整个信息中心，她的思维与数字世界融为一体，\u0026ldquo;他们拥有零熵攻击的能力，一旦我们的信息源受到威胁，后果将不堪设想。\u0026rdquo;\n黑客们开始发动他们的攻击和防御，但这一切都发生在虚拟世界中。攻击和反击变得更加精密和直接，信息流动如同电流一般，速度之快令人难以想象。\n突然，一股强大的信息冲击波袭来，来自地球解放军的数字攻击如同一股狂风，试图摧毁联盟的信息网络。索菲亚和她的团队立刻做出反应，他们的意识在虚拟世界中迅速展开防御。\n战场上，虚拟的代码和数据如同流星般碰撞，信息的爆发和碰撞充满了虚拟的火花。联盟的黑客们全身沐浴在数字光芒中，他们的虚拟眼镜中闪烁着坚定的光芒，不肯示弱。\n然而，地球解放军的攻击变得越来越猛烈，他们似乎已经完全掌握了零熵攻击的威力。索菲亚的虚拟世界之心沉入了冰窖，她明白，这是一场艰苦的战斗。胜利或失败，将在一瞬间决定。\n在这个数字直接融合的未来，信息的战场上，两个势不两立的势力正展开一场震撼的对决。这是一场绝地反击，胜者将取得信息战争的重要优势，但不确定性却令人窒息。这一战的胜负，将影响整个星辰联盟的命运。\n第五章 人类的选择 随着信息战争的烟雾渐渐散去，地球的焦点逐渐从战争转移到了人类自身的命运。资源匮乏和环境崩溃已经到了不可忽视的地步，地球变得岌岌可危。在这个时刻，人类不得不面对着一场更为深刻的内省，一场关于AI技术与人类关系的深刻思考。\n曾经，AI技术是光明的未来，是解决问题的利器，是拯救地球的希望。但在这个战争年代，它们也成为了毁灭的工具，是人类冲突的火种。人们开始怀疑，是不是过分依赖了技术，是否失去了自己的掌控权。\n在一次决定性的战斗中，星辰联盟成功击溃了地球解放军的“零熵攻击”，但代价是惨痛的，索菲亚和她的团队几乎付出了生命的代价。这个胜利带来了短期的宁静，但人们开始质疑，这样的代价是否值得，是否值得继续拼命维护一种技术的霸权。\n人类站在一个重要的分水岭上，他们必须决定是继续依赖AI技术，追求科技的极致，还是重新审视自己与自然的关系，重新探索与大自然和谐共存的方式。这个决定将决定着地球的命运，以及人类是否能够生存下去。\n在信息战争的硝烟散尽后，人类开始反思，AI技术对于他们究竟意味着什么。是生存的利器，还是自我毁灭的祸根？这个问题不再仅仅是技术的问题，而是一种关于自身存在的哲学问题。\n林辉，一个普通的民众，在战争之外，也在思考着这个问题。他曾经是一个热衷于科技进步的支持者，但看到了战争的破坏和人类的牺牲，他开始怀疑一切。他站在窗前，望着远处的废墟，心中涌上一股深深的无奈。\n\u0026ldquo;或许，我们真的过于依赖了技术，\u0026rdquo; 林辉自言自语，\u0026ldquo;但放弃它们，又是否会让我们回到更加原始的状态？\u0026rdquo;\n他明白，这是一个复杂而深刻的问题，没有简单的答案。人类必须在自身的价值观和科技的发展之间找到平衡，才能走向一个更为可持续的未来。\n索菲亚静静地望着窗外，她感到一种深刻的迷茫和无奈。在这个世界里，AI技术既是救世主，也是破坏者。她不禁自问：在技术的光芒中，人类是否忽略了自己的本质？这个问题，或许永远都不会有答案。但人类必须继续思考，继续前行，因为他们的选择将塑造自己的未来，决定着地球的命运。\n","permalink":"https://m1yan.github.io/posts/ai%E6%97%B6%E4%BB%A3%E5%91%BD%E8%BF%90%E7%9A%84%E6%8A%89%E6%8B%A9-generated-by-chatgpt/","summary":"\u003ch1 id=\"第一章--林辉的独白\"\u003e\u003cstrong\u003e第一章\u003c/strong\u003e  \u003cstrong\u003e林辉的独白\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003e赛博世界，第23个世纪。我，\u003ccode\u003e林辉\u003c/code\u003e，一个孤寂的码农，生活在这个高度数字化的时代。曾经，这个世界充满了希望，计算机科学的巅峰技术让人类梦想无限。然而，如今，这个世界已然陷入了一片混乱，战争的阴影笼罩着未来。\u003c/p\u003e","title":"《AI时代：命运的抉择》"},{"content":"WXML模板 数据绑定 1. 数据绑定的基本原则 在data中定义数据（.js中的data对象中） 在WXML中使用数据 2. Mustache语法 把data中的数据绑定到页面进行渲染，使用Mastache语法（双大括号）将变量包起来即可。语法格式为：\n\u0026lt;view\u0026gt;{{要绑定的数据类型}}\u0026lt;/view\u0026gt; 3. Mustache语法的应用场景 绑定内容 绑定属性 运算（三元运算、算数运算等） 4. 动态绑定内容 5. 动态绑定属性 .js 中的data\nPage({ data:{ imgsrc: \u0026#39;address\u0026#39; } }) .wxml中\n\u0026lt;image src=\u0026#34;{{imgsrc}}\u0026#34;\u0026gt;\u0026lt;/image\u0026gt; 6. 三元运算 页面数据\nPage({ data:{ randomNum: Math.random()*10 } }) 页面结构\n\u0026lt;view\u0026gt;{{randomNum\u0026gt;=5 ? \u0026#39;随机数字大于等于5\u0026#39;:\u0026#39;随机数字小于5\u0026#39;}} \u0026lt;/view\u0026gt; 事件绑定 1. 什么是事件 事件是渲染层到逻辑层的通讯方式。通过事件可以将用户在渲染层的行为，反馈到逻辑层进行业务的处理。\n2. 小程序中常用事件 类型 绑定方式 事件描述 tap bindtap/bind:tap 手指触摸后马上离开，类似HTML中的click事件 input bindinput/bind:input 文本框的输入事件 change bindchange/bind:change 状态改变时触发 3. 事件对象的属性列表 当事件回调时，会收到一个事件对象event，它的详细属性如下：\n属性 类型 说明 type String 事件类型 timeStamp Int 页面打开到触发事件经过的毫秒数 target Object 触发事件的组件的一些属性值集合 currentTarget Object 当前组件的一些属性值集合 detail Object 额外的信息 touches Array 触摸事件，当前停留在屏幕中的触摸点信息的数组 changedTouches Array 触摸事件，当前变化的触摸点信息的数组 4. target与currentTarget区别 target是触发该事件的源头组件，而currentTarget是当前事件所绑定的组件。\n？？?\n5. bindtap的语法格式 通过bindtap，可以为组件绑定tap触摸事件，语法如下： \u0026lt;button type=\u0026#34;primary\u0026#34; bindtap=\u0026#34;btnTapHandler\u0026#34;\u0026gt;按钮\u0026lt;/button\u0026gt; 在页面的.js文件中定义对应的事件处理函数，事件参数通过形参event（简写为e）来接收 Page({ btnTapHandler(e){ //按钮的tap事件处理函数 console.log(e) //事件参数对象e } }) 6. 在事件处理函数中为data中的数据赋值 通过调用this.setData(dataObject)方法，可以给页面data中的数据重新赋值，示例（设置按钮按下使data中的count变量+1）如下：\n设置一个+1按钮 \u0026lt;button type=\u0026#34;warn\u0026#34; bind:tap=\u0026#34;changecount\u0026#34;\u0026gt;+1\u0026lt;/button\u0026gt; 在js中编写+1函数 Page({ data: { count: 1 }, //修改count的值 changecount(){ this.setData({ count:this.data.count+1 }) } 编译后点击按钮，观察调试器中的AppData中count变量的值+1 7. 事件传参 可以为组件提供data-*自定义属性传参，其中*代表的是参数的名字，示例如下：\n\u0026lt;button bindtap=\u0026#34;btnHandler\u0026#34; data-info={{2}}\u0026gt; 事件传参 \u0026lt;/button\u0026gt; 最终\ninfo会被解析成参数的名字 数值2会解析为参数的值 在事件函数中，通过event.target.dataset.参数名即可获取到具体参数的值，实例如下\nbtnHandler(event){ //dataset是一个对象，包含了所有通过data-方式传递过来的参数项 console.log(event.target.dataset) //通过dataset可以访问到具体的参数值 console.log(event.target.dataset.info) } 如果想实现一个+2按钮，js文件中btnHandler函数改为\nbtnHandler(e){ this.setData({ count:this.data.count+e.target.dataset.info }) } 8. bindinput的语法格式 在小程序中，通过input事件来响应文本框的输入事件，语法格式如下：\n通过bindinput，可以为文本框绑定输入事件 \u0026lt;input bindinput=\u0026#34;inputHandler\u0026#34;\u0026gt;\u0026lt;/input\u0026gt; 在页面的js文件中定义事件处理函数： inputHandler(e){ //e.detail.value 是变化过后文本框最新的值 console.log(e.detail.value) } 9. 文本框和data的数据同步 步骤：\n定义数据 渲染结构 美化样式 绑定input事件处理函数 代码\n\u0026lt;input value=\u0026#34;{{msg}}\u0026#34; bindinput=\u0026#34;inputHandler\u0026#34;\u0026gt;\u0026lt;/input\u0026gt; Page({ data: { msg:\u0026#39;你好\u0026#39; }, //输入框事件处理函数 inputHandler(e){ this.setData({ msg: e.detail.value }) }) input { border: 1px solid #eee; margin: 5px; padding: 5px; border-radius: 5px; /*圆角*/ } 条件渲染 1. wx:if 在小程序中，使用wx:if=\u0026quot;{{condition}}\u0026quot;来判断是否需要渲染该代码块，\n也可以用wx:elif和wx:else来添加else判断。\n\u0026lt;view wx:if=\u0026#34;{{type===1}}\u0026#34;\u0026gt;男\u0026lt;/view\u0026gt; \u0026lt;view wx:elif=\u0026#34;{{type===2}}\u0026#34;\u0026gt;女\u0026lt;/view\u0026gt; \u0026lt;view wx:else\u0026gt;保密\u0026lt;/view\u0026gt; 2. 结合\u0026lt;block\u0026gt;使用wx:if 如果要一次性控制多个组件的显示和隐藏，可以使用一个block标签将多个组件包装起来，并在block标签上使用wx:if控制属性，示例如下：\n\u0026lt;block wx:if=\u0026#34;{{true}}\u0026#34;\u0026gt; \u0026lt;view\u0026gt;view1\u0026lt;/view\u0026gt; \u0026lt;view\u0026gt;view2\u0026lt;/view\u0026gt; \u0026lt;/block\u0026gt; 注意:block并不是一个组件，只是一个包裹性质的容器，不会在页面中做任何渲染。\n3. hidden 直接用hidden=\u0026quot;{{condition}}\u0026ldquo;也能控制元素的显示和隐藏。\n\u0026lt;view hidden=\u0026#34;{{condtion}}\u0026#34;\u0026gt;条件为true隐藏，条件为false显示\u0026lt;/view\u0026gt; 列表渲染 1. wx:for 通过wx:for可以根据指定的数组，循环渲染重复的组件结构\n\u0026lt;view wx:for=\u0026#34;{{array}}\u0026#34;\u0026gt; 索引是{{index}}，当前项是{{item}} \u0026lt;/view\u0026gt; 举例：\njs文件\nPage({ data:{ arr1:[\u0026#39;Apple\u0026#39;,\u0026#39;Huawei\u0026#39;,\u0026#39;Xiaomi\u0026#39;] } }) wxml文件\n\u0026lt;view wx:for=\u0026#34;{{arr1}}\u0026#34;\u0026gt;\u0026lt;!--提高效率可以加入属性wx:key=\u0026#34;index\u0026#34;--\u0026gt; 索引是：{{index}}，item是：{{item}} \u0026lt;/view\u0026gt; 2. 手动指定索引和当前项的变量名* 使用wx:for-index可以指定当前循环项索引的变量名 使用wx:for-item可以指定当前项的变量名 示例如下\n\u0026lt;view wx:for=\u0026#34;{{arr1}}\u0026#34; wx:for-index=\u0026#34;idx\u0026#34; wx-for-item=\u0026#34;itemName\u0026#34;\u0026gt; 索引是：{{idx}},当前项是{{itemName}}; \u0026lt;/view\u0026gt; 3.wx:key的使用 类似于Vue列表渲染中的:key，小程序在实现列表渲染时，也建议为渲染出来的列表项指定唯一的Key值，从而提高渲染的效率，示例代码如下：\ndata:{ userList:[ {id:1,name:\u0026#39;小红\u0026#39;}, {id:2,name:\u0026#39;小蓝\u0026#39;}, {id:3,name:\u0026#39;小黄\u0026#39;} ] } \u0026lt;view wx:for=\u0026#34;{{userList}}\u0026#34; wx:key=\u0026#34;id\u0026#34;\u0026gt;{{item.name}}\u0026lt;/view\u0026gt; WXSS模板样式 什么是WXSS WXSS(WeiXin Style Sheets)是一套样式语言，用于美化WXML的组件样式，类似于网页开发中的CSS。\nWXSS和CSS的关系 WXSS扩展的特性：\nrpx尺寸单位 @import样式导入 rpx尺寸单位 1. 什么是rpx尺寸单位 rpx(responsive pixel) 是微信小程序独有的，用来解决屏适配的尺寸问题。\n2. rpx的实现原理 鉴于不同设备的屏幕大小不同，为了实现屏幕的自动适配，rpx把所有设备的屏幕，在宽度上等分为750份（即当前屏幕的总宽度为750rpx）\n3. rpx和px之间的单位换算 在iPhone6中， $$ 1rpx=0.5px $$\n$$ 1px=2rpx $$\n样式导入 1. 什么是样式导入 使用wxss提供的@import语法，可以导入外联的样式表。\n2. @import的语法格式 @import后跟需要导入的外联样式表的相对路径，用 ; 语句表示结束。示例如下：\n新建一个common文件夹，里面新建common.wxss文件\n.username{ color: red; } 在index.wxss文件中引入样式表\n@import \u0026#34;/common/common.wxss\u0026#34;; 最后在index.wxml里面引入类型看是否成功。\n全局样式和局部样式 1. 全局样式 定义在app.wxss中的样式为全局样式，作用于每一个页面。\n2. 局部样式 在页面的wxss文件中定义的样式，只作用于当前页面。\n当全局样式和局部样式权重（将光标停留在样式名称上时显示）一样时，优先显示局部样式，否则显示权重较大的样式。\n全局配置 全局配置文件及常用的配置项 小程序根目录下的app.json文件是小程序的全局配置文件。常用的配置如下：\npages 记录当前小程序页面的存放路径 window 全局设置小程序窗口的外观 tabBar 设置小程序底部的tabBar效果 style 是否启用新版的组件样式 window 1. 小程序窗口的组成部分 其中，window可以配置导航栏区域和背景区域。\n","permalink":"https://m1yan.github.io/posts/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F-%E6%A8%A1%E6%9D%BF%E4%B8%8E%E9%85%8D%E7%BD%AE/","summary":"\u003ch1 id=\"wxml模板\"\u003eWXML模板\u003c/h1\u003e\n\u003ch2 id=\"数据绑定\"\u003e数据绑定\u003c/h2\u003e\n\u003ch3 id=\"1-数据绑定的基本原则\"\u003e1. 数据绑定的基本原则\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e在data中定义数据（.js中的data对象中）\u003c/li\u003e\n\u003cli\u003e在WXML中使用数据\u003c/li\u003e\n\u003c/ol\u003e","title":"微信小程序开发-模板与配置"},{"content":"小程序项目结构 1.基本组成结构 pages用来存放所有小程序页面 utils用来存放工具性质的模块 app.js 小程序项目的入口文件 app.json 小程序项目的全局配置文件 app.wxss 小程序项目的全局样式文件 project.config.json 用来配置小程序及其页面是否允许被微信索引 2.小程序页面的组成成分 小程序官方建议把小程序页面都存放在page目录中，其中每个页面由四个基本文件组成\n.js文件 （页面的脚本文件，存放页面的数据、事件处理函数等） .json文件 （当前页面的配置文件，配置页面的外观、表现等） .wxml文件 （当前页面的模板结构文件） .wxss文件 （当前页面的样式表文件） JSON配置文件 1.4种json文件 项目根目录的app.json文件 根目录中的project.config.json文件 根目录中的sitemap.json文件 每个page中的json配置文件 2. app.json文件 当前小程序的全局配置，包括小程序的页面路径，窗口外观，界面表现，底部Tab等。\n配置内容：\npages：记录当前小程序的所有路径 window：小程序所有页面的背景色、文字颜色等 style：小程序组件所使用的版本样式 sitemapLocation：指明sitemap.json的位置 3.project.config.json 文件 project.config.json 是项目配置文件，记录我们对小程序的个性化配置，例如：\nsetting 中保存了编译相关配置 projectname 项目名称 appid 保存小程序的账号ID 4.sitemap.json 文件 小程序内搜索，类似PC网页中的SEO。该文件用来配置小程序页面是否允许微信索引。\n5. 页面的 .json 配置文件 小程序中的每个页面，可以使用.json文件来对本页面窗口外观进行配置，页面中的配置项会覆盖app.json中window的配置项。\n6.新建小程序页面 在app.json-\u0026gt;pages 中新增页面的存放路径，小程序开发者工具自动帮助自动创建对应页面文件\n例如，新建list文件夹，在pages中新增：\n\u0026#34;pages/list/list\u0026#34; 即可新建list文件夹并且内含list.js , list.json , list.wxml , list.wxss。\n7.修改项目首页 只需调整app.json-\u0026gt;pages 数组中页面路径的前后顺序，即可修改项目的首页。第一位的页面当作项目首页进行渲染。\nWXML模板 1.什么是WXML WXML(Weixin Makeup Language) 是小程序框架设置的一套标签语言，用来构建小程序页面的结构，作用类似网页开发中的HTML。\n2.WXML与HTML区别 标签名称不同 HTML(div, span, img, a) WXML(view, text, image, navigator) 属性节点不同 超链接 提供了类似vue中的模板语法 数据绑定 列表渲染 条件渲染 WXSS样式 1.什么是WXSS WXSS(WeiXin Style Sheets)是一套样式语言，用于描述WXML的组件样式，类似网页开发中的CSS。\n2.WXSS和CSS区别 新增了rpx尺寸单位 提供了全局样式和局部样式 WXSS仅支持部分CSS选择器 JS逻辑交互 1. .js文件 app.js 是整个小程序的入口文件，通过调用App( )函数来启动小程序。 页面的js文件 页面的入口文件，通过调用Page( )函数来创建并运行页面。 普通js文件 普通的功能模块文件，用来封装公共的函数或属性供页面使用。 小程序的宿主环境 包含内容： 通信模型 运行机制 组件 API 通信模型 1.通信主体 小程序的通信主体是渲染层和逻辑层\nWXML模板和WXSS样式工作在渲染层 JS脚本工作在逻辑层 2.小程序的通信模型 渲染层与逻辑层间的通信 由微信客户端进行转发 逻辑层和第三方服务器之间的通信 由微信客户端进行转发 3. 小程序启动过程 小程序代码包下载到本地 解析app.json全局配置文件 执行app.js小程序入口文件，调用App( )创建小程序实例 渲染小程序首页 小程序启动完成 4. 页面渲染过程 加载解析页面的.json配置文件 加载页面的wxml模板和wxss样式 执行页面的.js文件，调用Page( )创建页面实例 页面渲染完成 组件 1. 组件分类 小程序中的组件也是宿主环境提供的，开发者可以基于组件快速搭建页面结构。\n组件分为了9大类：\n视图容器 基础内容 表单组件 导航组件 媒体组件 map地图组件 canvas画布组件 开放能力 无障碍访问 2.常用视图容器类组件 view 普通视图区域 类似HTML中的div，是一个块级元素 用来实现页面的布局效果 scroll-view 可滚动的视图区域 用来实现滚动列表效果 swiper和swiper-item 轮播图组件和轮播图item组件 view组件的基本使用 编写wxml文件 \u0026lt;text\u0026gt;横向布局\u0026lt;/text\u0026gt; \u0026lt;view class=\u0026#34;container1\u0026#34;\u0026gt; \u0026lt;view\u0026gt;A\u0026lt;/view\u0026gt; \u0026lt;view\u0026gt;B\u0026lt;/view\u0026gt; \u0026lt;view\u0026gt;C\u0026lt;/view\u0026gt; \u0026lt;/view\u0026gt; 编写wxss文件 .container1 view{ width: 100px; height: 100px; text-align: center; line-height: 100px; } .container1 view:nth-child(1){ background-color: aquamarine; } .container1 view:nth-child(2){ background-color: blueviolet; } .container1 view:nth-child(3){ background-color: rgb(216, 66, 66); } .container1{ display: flex; justify-content: space-around; } scroll-view组件的基本使用 1.wxml文件\n\u0026lt;text\u0026gt;滚动组件\u0026lt;/text\u0026gt; \u0026lt;!--scroll-x 横向滚动--\u0026gt; \u0026lt;!--scroll-y 纵向滚动--\u0026gt; \u0026lt;!--使用纵向滚动时，要给scroll-view一个固定高度--\u0026gt; \u0026lt;scroll-view class=\u0026#34;container2\u0026#34; scroll-y\u0026gt; \u0026lt;view\u0026gt;A\u0026lt;/view\u0026gt; \u0026lt;view\u0026gt;B\u0026lt;/view\u0026gt; \u0026lt;view\u0026gt;C\u0026lt;/view\u0026gt; \u0026lt;/scroll-view\u0026gt; 2.wxss文件\n.container2 view{ width: 100px; height: 100px; text-align: center; line-height: 100px; } .container2 view:nth-child(1){ background-color: aquamarine; } .container2 view:nth-child(2){ background-color: blueviolet; } .container2 view:nth-child(3){ background-color: rgb(109, 50, 50); } .container2{ border: 1px solid crimson; height: 120px; /*纵向滚动需要设置高度*/ width: 100px; } swiper和swiper-item组件的基本使用 wxml文件 \u0026lt;!--pages/list/list.wxml--\u0026gt; \u0026lt;!--轮播图--\u0026gt; \u0026lt;!-- indicator-dots属性：显示面板指示点 --\u0026gt; \u0026lt;text\u0026gt;轮播图\u0026lt;/text\u0026gt; \u0026lt;swiper class=\u0026#34;swiper-container\u0026#34; indicator-dots\u0026gt; \u0026lt;swiper-item\u0026gt; \u0026lt;view class=\u0026#34;item\u0026#34;\u0026gt;A\u0026lt;/view\u0026gt; \u0026lt;/swiper-item\u0026gt; \u0026lt;swiper-item\u0026gt; \u0026lt;view class=\u0026#34;item\u0026#34;\u0026gt;B\u0026lt;/view\u0026gt; \u0026lt;/swiper-item\u0026gt; \u0026lt;swiper-item\u0026gt; \u0026lt;view class=\u0026#34;item\u0026#34;\u0026gt;C\u0026lt;/view\u0026gt; \u0026lt;/swiper-item\u0026gt; \u0026lt;/swiper\u0026gt; wxss文件 .swiper-container{ height: 150px;/*组件高度*/ } .item{ height: 100%; line-height: 150px; text-align: center; } /* 背景颜色不显示原因：child()后面加空格再加.item */ swiper-item:nth-child(1) .item{ background-color:royalblue; } swiper-item:nth-child(2) .item{ background-color: rgb(65, 225, 212); } swiper-item:nth-child(3) .item{ background-color: rgb(221, 120, 199); } swiper组件的常用属性 属性 类型 默认值 说明 indicator-dot boolean false 是否显示面板指示点 indicator-color color rgba(0,0,0,.3) 指示点颜色 indicator-active-color color #000000 当前选中的指示点颜色 autoplay boolean false 是否自动切换 interval number 5000 自动切换时间间隔 circular boolean false 是否衔接滑动 3. 常用的基础内容组件 text 文本组件 类似HTML中的span标签，是一个行内元素 rich-text 富文本组件 支持把HTML字符串渲染为WXML结构 text组件的基本使用 通过text组件的user-select属性，实现长按选择文本内容的效果： \u0026lt;text user-select\u0026gt;12345678\u0026lt;/text\u0026gt; rich-text组件基本使用 通过rich-text组件的nodes属性，可以把HTML字符串渲染成对应的UI结构： \u0026lt;rich-text nodes=\u0026#34;\u0026lt;h1 style=\u0026#39;color: red;\u0026#39;\u0026gt;标题\u0026lt;/h1\u0026gt;\u0026#34;\u0026gt;\u0026lt;/rich-text\u0026gt; 4. 其他常用组件 button 按钮组件 功能比HTML中的button按钮丰富 通过open-type属性可以调用微信提供的各种功能（客服、转发、获取用户授权、获取用户信息等） image 图片组件 image组件默认宽度为300px，高度约240px navigator 页面导航组件 类似HTML中的a链接 button组件基本使用 \u0026lt;view\u0026gt;----通过type类型----\u0026lt;/view\u0026gt; \u0026lt;button\u0026gt;普通按钮\u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;primary\u0026#34;\u0026gt;主色调按钮\u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;warn\u0026#34;\u0026gt;警告按钮\u0026lt;/button\u0026gt; \u0026lt;button\u0026gt;\u0026lt;/button\u0026gt; \u0026lt;view\u0026gt;----小尺寸按钮----\u0026lt;/view\u0026gt; \u0026lt;button size=\u0026#34;mini\u0026#34;\u0026gt;默认按钮\u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;primary\u0026#34; size=\u0026#34;mini\u0026#34;\u0026gt;主色调按钮\u0026lt;/button\u0026gt; \u0026lt;view\u0026gt;----plain镂空按钮----\u0026lt;/view\u0026gt; \u0026lt;button plain\u0026gt;普通按钮\u0026lt;/button\u0026gt; \u0026lt;button type=\u0026#34;primary\u0026#34; plain\u0026gt;主色调按钮\u0026lt;/button\u0026gt; image图片组件基本使用 \u0026lt;image\u0026gt;\u0026lt;/image\u0026gt; \u0026lt;!--空图片--\u0026gt; \u0026lt;image src=\u0026#34;/adress/pic\u0026#34;\u0026gt;\u0026lt;/image\u0026gt; image组件的mode属性 mode属性用来指定图片的裁剪和缩放模式，常用mode属性：\nmode值 说明 scaleToFill (默认值)缩放模式，不保持纵横比，拉伸填满image元素 aspectFit 缩放模式，保持纵横比缩放，可以完整显示图片 aspectFill 缩放模式，只保持短边能完整显示 widthFix 缩放模式，宽度不变，高度自动变化，保证原图长宽比不变 heightFix 缩放模式，高度不变，宽度自动变化，保持原图长宽比不变 API 1. 小程序API概述 小程序中的API是宿主环境提供的，开发者可以方便地调用微信提供的功能。如获取用户信息，支付等。\n2.三类API 时间监听API 特点：以on开头，用来监听某些事件的触发 举例：wx.onWindowResize(function callback)监听窗口尺寸的变化 同步API 特点1：以Sync结尾的API都是同步API 特点2：同步API的执行结果，可以通过函数返回值直接获取，如果执行出错会抛出异常 举例：wx.setStorageSync(\u0026ldquo;key\u0026rdquo;,\u0026ldquo;value\u0026rdquo;)向本地存储中写入内容 异步API 特点：类似于jQuery中$.ajax(option)函数，需要通过success、fail、complete接收调用的结果 举例：wx.request()发起网络数据请求，通过success回调函数接收数据 ","permalink":"https://m1yan.github.io/posts/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91-%E8%B5%B7%E6%AD%A5/","summary":"\u003ch1 id=\"小程序项目结构\"\u003e小程序项目结构\u003c/h1\u003e\n\u003ch3 id=\"1基本组成结构\"\u003e1.基本组成结构\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003epages用来存放所有小程序页面\u003c/li\u003e\n\u003cli\u003eutils用来存放工具性质的模块\u003c/li\u003e\n\u003cli\u003eapp.js 小程序项目的入口文件\u003c/li\u003e\n\u003cli\u003eapp.json 小程序项目的全局配置文件\u003c/li\u003e\n\u003cli\u003eapp.wxss 小程序项目的全局样式文件\u003c/li\u003e\n\u003cli\u003eproject.config.json 用来配置小程序及其页面是否允许被微信索引\u003c/li\u003e\n\u003c/ol\u003e","title":"微信小程序开发-起步"},{"content":"概论 定义 并查集是一种树型的数据结构，用于处理一些不相交集合的合并及查询问题（即所谓的并、查）。比如说，我们可以用并查集来判断一个森林中有几棵树、某个节点是否属于某棵树等。\n主要构成 并查集主要由一个整型数组pre[ ]和两个函数find( )、join( )构成。 数组 pre[ ] 记录了每个点的前驱节点是谁，函数 find(x) 用于查找指定节点 x 属于哪个集合，函数 join(x,y) 用于合并两个节点 x 和 y 。\n作用 并查集的主要作用是求连通分支数（如果一个图中所有点都存在可达关系（直接或间接相连），则此图的连通分支数为1；如果此图有两大子图各自全部可达，则此图的连通分支数为2……）\n并查集的应用 案例及其讲解 江湖门派\n分析 其中每个帮派的教主可以随机选择，只要两个人的教主相同，他们就属于同一个帮派。其中的教主指的就是代表元。\n代表元：用集合中的某个元素来代表这个集合，这个元素就被称为该集合的代表元。\n用于找代表元的函数find(x) pre[ ]数组用于记录每个人的上级是谁，层层查找直至找到主教，主教（即代表元）的特征为他的上级为他自己。\nint find(int x){ while(pre[x]!=x) x=pre[x]; return x; } 用于合并两个人所在帮派的函数join(x,y) 假设A帮派中的一个成员张三与B帮派中的一个成员李四想合并两个帮派，他们要做的事情有两点：\n张三和李四分别找到他们各自的教主 你来随机指派一个人当合并后帮派的教主 写成函数即为\nvoid join(int x,int y){ int a=find(x); int b=find(y); if(a!=b) pre[a]=b; } 例题：畅通工程 题目 Problem Description 某省调查城镇交通状况，得到现有城镇道路统计表，表中列出了每条道路直接连通的城镇。省政府“畅通工程”的目标是使全省任何两个城镇间都可以实现交通（但不一定有直接的道路相连，只要互相间接通过道路可达即可）。问最少还需要建设多少条道路？\nInput 测试输入包含若干测试用例。每个测试用例的第1行给出两个正整数，分别是城镇数目N ( \u0026lt; 1000 )和道路数目M；随后的M行对应M条道路，每行给出一对正整数，分别是该条道路直接连通的两个城镇的编号。为简单起见，城镇从1到N编号。 注意:两个城市之间可以有多条道路相通,也就是说 3 3 1 2 1 2 2 1 这种输入也是合法的 当N为0时，输入结束，该用例不被处理。 Huge input, scanf is recommended.\nOutput 对每个测试用例，在1行里输出最少还需要建设的道路数目。\n输入样例\n4 2 1 3 4 3 3 3 1 2 1 3 2 3 5 2 1 2 3 5 999 0 0 输出样例\n1 0 2 998 Code #define _CRT_SECURE_NO_WARNINGS #include\u0026lt;iostream\u0026gt; #include\u0026lt;algorithm\u0026gt; using namespace std; int set[1010]; int findx(int x) { int r = x; while (r != set[r]) r = set[r]; return r; } void join(int x, int y) { int a = findx(x); int b = findx(y); if(a!=b) set[a] = b; } int main() { int n, m; while (cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m \u0026amp;\u0026amp; n) { for (int i = 1; i \u0026lt;= n; i++) { set[i] = i; } for (int i = 0; i \u0026lt; m; i++) { int x, y; cin \u0026gt;\u0026gt; x \u0026gt;\u0026gt; y; join(x, y); } int count = 0; for (int i = 1; i \u0026lt;= n; i++) { if (set[i] == i) count++; } cout \u0026lt;\u0026lt; count - 1 \u0026lt;\u0026lt; endl;//N堆通路之间只需要N-1条道路即可全部连通 } return 0; } 优化 I. 路径压缩（优化find(x)） 如果一个帮派中所有成员的上级都为教主，那么只需一步就可找到教主。\n代码（递归写法） int find(int x){ if(pre[x]==x) return x; pre[x]=find(pre[x]); return pre[x]; } 缺陷：只有当查找了某个节点的代表元（教主）后，才能对该查找路径上的各节点进行路径压缩。换言之，第一次执行查找操作的时候是实现没有压缩效果的，只有在之后才有效。\n","permalink":"https://m1yan.github.io/posts/2023221-%E5%B9%B6%E6%9F%A5%E9%9B%86%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8/","summary":"\u003ch1 id=\"概论\"\u003e概论\u003c/h1\u003e\n\u003ch2 id=\"定义\"\u003e定义\u003c/h2\u003e\n\u003cp\u003e并查集是一种树型的数据结构，用于处理一些不相交集合的合并及查询问题（即所谓的并、查）。比如说，我们可以用并查集来判断一个森林中有几棵树、某个节点是否属于某棵树等。\u003c/p\u003e","title":"并查集及其应用"},{"content":"核心思路 对于一个给定的节点，他的中序后继节点有两种情况:\n给定节点存在右子树时，中序后继节点即为右子树中的最小值\n若不存在右子树，则中序后继节点为给定节点所在的左子树的祖先\nCode BstNode* Find(BstNode* root, int data) {//寻找给定data的地址 if (root == NULL)return NULL; if (data \u0026lt; root-\u0026gt;data) return Find(root-\u0026gt;left, data); else if (data \u0026gt; root-\u0026gt;data) return Find(root-\u0026gt;right, data); else return root; } BstNode* Getsuccessor(BstNode* root, int data) { BstNode* current = Find(root, data); if (current == NULL)return NULL; if (current-\u0026gt;right != NULL) { return FindMin(current-\u0026gt;right); } else { BstNode* successor = NULL; BstNode* ancestor = root; while (ancestor != current) { if (current-\u0026gt;data \u0026lt; ancestor-\u0026gt;data) { successor = ancestor; ancestor = ancestor-\u0026gt;left; } else { ancestor = ancestor-\u0026gt;right; } } return ancestor; } } ","permalink":"https://m1yan.github.io/posts/2023119-%E6%9F%A5%E6%89%BE%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E4%B8%AD%E5%BA%8F%E5%90%8E%E7%BB%A7%E8%8A%82%E7%82%B9/","summary":"\u003ch1 id=\"核心思路\"\u003e核心思路\u003c/h1\u003e\n\u003cp\u003e对于一个给定的节点，他的中序后继节点有\u003ccode\u003e两种情况\u003c/code\u003e:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e给定节点存在右子树时，中序后继节点即为右子树中的最小值\u003c/p\u003e\n\u003cp\u003e若不存在右子树，则中序后继节点为给定节点所在的左子树的祖先\u003c/p\u003e\n\u003c/blockquote\u003e","title":"查找二叉搜索树的中序后继节点"},{"content":"原理 搜索二叉树有2个基本性质可以用于判断：\n左子树中的所有值都小于等于根节点的值，而右子树中的数据都大于根节点的值。 二叉搜索树的中序输出后数据按照非递减顺序排列 对于第一个性质，假设根节点数据为7，那么左子树最大不能超过7，而右子树都大于7。\n所以我们可以在函数中增加参数minValue和maxValue，用来记录此子树中数据的范围。\n代码I bool IsBST(BstNode* root, int minValue, int maxValue) { if (root == NULL)return true; if (root-\u0026gt;data \u0026gt;= minValue \u0026amp;\u0026amp; root-\u0026gt;data \u0026lt; maxValue \u0026amp;\u0026amp; IsBST(root-\u0026gt;left, minValue, root-\u0026gt;data) \u0026amp;\u0026amp; IsBST(root-\u0026gt;right, root-\u0026gt;data, maxValue)) return true; else return false; } bool IsBinarySearchTree(BstNode* root) { return IsBST(root, INT_MIN, INT_MAX); } 代码II void LDR(BstNode* root, vector\u0026lt;int\u0026gt;\u0026amp; A) { if (root == NULL)return; LDR(root-\u0026gt;left,A); A.push_back(root-\u0026gt;data); LDR(root-\u0026gt;right,A); } bool ISBST(BstNode* root) { vector\u0026lt;int\u0026gt; A; LDR(root,A); for (int i = 0; i \u0026lt; A.size() - 1; i++) { if (A[i] \u0026gt;A[i + 1])return false; } return true; } ","permalink":"https://m1yan.github.io/posts/2023119-%E5%A6%82%E4%BD%95%E6%A3%80%E6%9F%A5%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%8F%89%E6%A0%91%E6%98%AF%E5%90%A6%E4%B8%BA%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/","summary":"\u003ch1 id=\"原理\"\u003e原理\u003c/h1\u003e\n\u003cp\u003e搜索二叉树有2个基本性质可以用于判断：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003col\u003e\n\u003cli\u003e左子树中的所有值都小于等于根节点的值，而右子树中的数据都大于根节点的值。\u003c/li\u003e\n\u003cli\u003e二叉搜索树的中序输出后数据按照非递减顺序排列\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e对于第一个性质，假设根节点数据为7，那么左子树最大不能超过7，而右子树都大于7。\u003c/p\u003e\n\u003cp\u003e所以我们可以在函数中增加参数\u003ccode\u003eminValue\u003c/code\u003e和\u003ccode\u003emaxValue\u003c/code\u003e，用来记录此子树中数据的范围。\u003c/p\u003e","title":"如何检查一个二叉树是否为二叉搜索树"},{"content":"三种情况 Case 1 删除的节点没有子节点（即该节点为叶节点）\n操作：直接delete\nCase 2 删除的节点下有一个子节点\n操作：用左/右子树代替该节点\nCase 3 删除的节点有两个子节点\n操作：\n找到右子树中的最小值 复制该值到删除的节点处 （如果树中的数据都各不相同的话）我们需要删去右子树中的最小值，因为右子树的最小值没有左子树，所以删去它的情况就降为Case1或Case2 代码 BstNode* FindMin(BstNode* root) { if (root-\u0026gt;left == NULL)return root; else return FindMin(root-\u0026gt;left); } struct BstNode* Delete(BstNode* root, int data) { if (root == NULL)return root; else if (data \u0026lt; root-\u0026gt;data)root-\u0026gt;left = Delete(root-\u0026gt;left, data); else if (data \u0026gt; root-\u0026gt;data)root-\u0026gt;right = Delete(root-\u0026gt;right, data); else { //no child if (root-\u0026gt;left == NULL \u0026amp;\u0026amp; root-\u0026gt;right == NULL) { delete root; root = NULL; } //one child else if (root-\u0026gt;left == NULL) { BstNode* temp = root; root = root-\u0026gt;right; delete temp; } else if (root-\u0026gt;right == NULL) { BstNode* temp = root; root = root-\u0026gt;left; delete temp; } //two children else { BstNode* temp = FindMin(root-\u0026gt;right); root-\u0026gt;data = temp-\u0026gt;data; root-\u0026gt;right = Delete(root-\u0026gt;right, temp-\u0026gt;data); } } return root; } ","permalink":"https://m1yan.github.io/posts/2023119-%E5%88%A0%E9%99%A4%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E8%8A%82%E7%82%B9/","summary":"\u003ch1 id=\"三种情况\"\u003e三种情况\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eCase 1\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e删除的节点没有子节点（即该节点为叶节点）\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e操作：直接delete\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eCase 2\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e删除的节点下有一个子节点\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e操作：用左/右子树代替该节点\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eCase 3\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e删除的节点有两个子节点\u003c/p\u003e","title":"删除一个二叉搜索树的节点"},{"content":"Tree Traversal Tree traversal is a process of visiting each node in the tree exactly once in some order.\n树的遍历有2种方式，Breadth-first（广度优先）和Depth-first（深度优先）。\nBreadth-first（广度优先） 按照广度优先，对二叉树的遍历方式主要是Level-order（层序遍历）。\nLevel-order（层序遍历） 层序遍历即对一个二叉树每层从上到下，从左到右进行遍历。\n上图层序遍历顺序即为F D J B E G K A C I H.\n实现 用队列可以实现层序遍历。因为队列有First-In-First-Out(FIFO)原则，我们可以先让root进入队列，在访问节点时，让其左右节点进入队列。\n实现层序遍历的代码如下：\n代码 #include\u0026lt;queue\u0026gt;//可以直接调用库函数使用队列，其基本语法与Stack类似 void LevelOrder(BstNode* root) { if (root == NULL)return; queue\u0026lt;BstNode*\u0026gt; Q; Q.push(root); while (!Q.empty()) { BstNode* current = Q.front(); cout \u0026lt;\u0026lt; current-\u0026gt;data \u0026lt;\u0026lt; \u0026#34; \u0026#34;; if (current-\u0026gt;left != NULL)Q.push(current-\u0026gt;left); if (current-\u0026gt;right != NULL)Q.push(current-\u0026gt;right); Q.pop(); } } Depth-first（深度优先） 深度优先有三个排序方法，Preorder（前序）、Inorder（中序）和Postorder（后序）。\n类型 Order Preorder \u0026lt;root\u0026gt;\u0026lt;left\u0026gt;\u0026lt;right\u0026gt;(DLR) Inorder \u0026lt;left\u0026gt;\u0026lt;root\u0026gt;\u0026lt;right\u0026gt;(LDR) Postorder \u0026lt;left\u0026gt;\u0026lt;right\u0026gt;\u0026lt;root\u0026gt;(LRD) 上图三种不同方式的排列顺序分别为：\nPreorder: F D B A C E J G I H K\nInorder: A B C D E F G H I J K\nPostorder: A C B E D H I G K J F\n实现 三种顺序都可以用递归来简洁地写出，代码如下：\n代码 void Preorder(BstNode* root) { if (root == NULL)return; printf(\u0026#34;%d \u0026#34;, root-\u0026gt;data); Preorder(root-\u0026gt;left); Preorder(root-\u0026gt;right); } void Inorder(BstNode* root) { if (root == NULL)return; Inorder(root-\u0026gt;left); printf(\u0026#34;%d \u0026#34;, root-\u0026gt;data); Inorder(root-\u0026gt;right); } void Postorder(BstNode* root) { if (root == NULL)return; Postorder(root-\u0026gt;left); Postorder(root-\u0026gt;right); printf(\u0026#34;%d \u0026#34;, root-\u0026gt;data); } 最后附上完整代码，输出树的高度，层序遍历，前序，中序，后序\n#include\u0026lt;iostream\u0026gt; #include\u0026lt;queue\u0026gt; using namespace std; struct BstNode { int data; BstNode* left; BstNode* right; }; void LevelOrder(BstNode* root) { if (root == NULL)return; queue\u0026lt;BstNode*\u0026gt; Q; Q.push(root); while (!Q.empty()) { BstNode* current = Q.front(); cout \u0026lt;\u0026lt; current-\u0026gt;data \u0026lt;\u0026lt; \u0026#34; \u0026#34;; if (current-\u0026gt;left != NULL)Q.push(current-\u0026gt;left); if (current-\u0026gt;right != NULL)Q.push(current-\u0026gt;right); Q.pop(); } printf(\u0026#34;\\n\u0026#34;); } void Preorder(BstNode* root) { if (root == NULL)return; printf(\u0026#34;%d \u0026#34;, root-\u0026gt;data); Preorder(root-\u0026gt;left); Preorder(root-\u0026gt;right); } void Inorder(BstNode* root) { if (root == NULL)return; Inorder(root-\u0026gt;left); printf(\u0026#34;%d \u0026#34;, root-\u0026gt;data); Inorder(root-\u0026gt;right); } void Postorder(BstNode* root) { if (root == NULL)return; Postorder(root-\u0026gt;left); Postorder(root-\u0026gt;right); printf(\u0026#34;%d \u0026#34;, root-\u0026gt;data); } BstNode* GetNewNode(int data) { BstNode* NewNode = new BstNode(); NewNode-\u0026gt;data = data; NewNode-\u0026gt;left = NewNode-\u0026gt;right = NULL; return NewNode; } BstNode* Insert(BstNode* root,int data) { if (root == NULL) root = GetNewNode(data); else if (data \u0026lt;= root-\u0026gt;data) { root-\u0026gt;left = Insert(root-\u0026gt;left, data); } else root-\u0026gt;right = Insert(root-\u0026gt;right, data); return root; } int max(int a, int b) { if (a \u0026gt; b)return a; else return b; } int FindHeight(BstNode* root) { if (root == NULL) { return -1; } return max(FindHeight(root-\u0026gt;left), FindHeight(root-\u0026gt;right)) + 1; } int main() { BstNode* root = NULL; root = Insert(root,3); root = Insert(root, 6); root = Insert(root, 2); root = Insert(root, 1); root = Insert(root, 7); root = Insert(root, 5); root = Insert(root, 8); root = Insert(root, 4); printf(\u0026#34;Height of tree is %d\\n\u0026#34;, FindHeight(root)); cout \u0026lt;\u0026lt; \u0026#34;LevelOrder:\u0026#34;; LevelOrder(root); cout \u0026lt;\u0026lt; \u0026#34;Preorder:\u0026#34;; Preorder(root); cout \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;Inorder:\u0026#34;; Inorder(root); cout \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;Postorder:\u0026#34;; Postorder(root); return 0; } ","permalink":"https://m1yan.github.io/posts/2023115-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/","summary":"\u003ch1 id=\"tree-traversal\"\u003eTree Traversal\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTree traversal is a process of visiting each node in the tree exactly once in some order.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e树的遍历有2种方式，\u003ccode\u003eBreadth-first（广度优先）\u003c/code\u003e和\u003ccode\u003eDepth-first（深度优先）\u003c/code\u003e。\u003c/p\u003e","title":"二叉树的遍历"},{"content":"树的介绍 树(Tree)作为一种数据结构，具有一种递归性。一个树可以看作是由根节点(root)以及若干子树构成，而子树又可以继续向下分成根和子树，因此树具有递归性。\n二叉树(binary tree)是树中的一种，它满足每个节点都有要么2个要么0个子节点的特性。\n二叉搜索树(binary search tree)满足左侧子树中储存的值都小于等于root，而右侧子树上的值都大于root，并且递归满足。\n搜索效率比较 Operation Array Linked-list Array(sorted) BST Search(n) O(n) O(n) O(logn) O(logn) Insert(x) O(1) O(1) O(n) O(logn) Remove(n) O(n) O(n) O(n) O(logn) 可以看出二叉搜索树在处理数据方面更有优势。\n二叉搜索树的递归实现 #include\u0026lt;iostream\u0026gt; using namespace std; struct BstNode { int data; struct BstNode* left; struct BstNode* right; }; BstNode* GetNewNode(int data) { BstNode* NewNode = (BstNode*)malloc(sizeof(BstNode)); NewNode-\u0026gt;data = data; NewNode-\u0026gt;left = NULL; NewNode-\u0026gt;right = NULL; return NewNode; } BstNode* Insert(BstNode* root, int data) { if (root == NULL) { root = GetNewNode(data); } else if (data \u0026lt;= root-\u0026gt;data) { root-\u0026gt;left = Insert(root-\u0026gt;left, data); } else { root-\u0026gt;right = Insert(root-\u0026gt;right, data); } return root; } bool Search(BstNode* root, int data) { if (root == NULL) return false; else if (root-\u0026gt;data == data) return true; else if (data \u0026lt;= root-\u0026gt;data) return Search(root-\u0026gt;left, data); else return Search(root-\u0026gt;right, data); } int main() { BstNode* root; root = NULL; root = Insert(root, 15); root = Insert(root, 10); root = Insert(root, 10); root = Insert(root, 25); root = Insert(root, 8); root = Insert(root, 12); int number; cout \u0026lt;\u0026lt; \u0026#34;Enter number be searched\\n\u0026#34;; cin \u0026gt;\u0026gt; number; if (Search(root, number))cout \u0026lt;\u0026lt; \u0026#34;Found\\n\u0026#34;; else cout \u0026lt;\u0026lt; \u0026#34;Not Found\\n\u0026#34;; } 用二叉搜索树查找最大最小值 我们发现，一直向左节点走会找到最小值，而一直向右节点走会找到最大值。\n代码如下（最小值是迭代实现，最大值是递归实现） #include\u0026lt;iostream\u0026gt; using namespace std; struct BstNode { int data; BstNode* left; BstNode* right; }; int FindMin(BstNode* root) { if (root == NULL) { printf(\u0026#34;Error:Tree is empty\\n\u0026#34;); return -1; } while (root-\u0026gt;left != NULL) { root = root-\u0026gt;left; } return root-\u0026gt;data; } int FindMaxRecursion(BstNode* root) { if (root == NULL) { printf(\u0026#34;Error\\n\u0026#34;); return -1; } else if (root-\u0026gt;right == NULL) { return root-\u0026gt;data; } return FindMaxRecursion(root-\u0026gt;right); } BstNode* GetNewNode(int data) { BstNode* NewNode = new BstNode(); NewNode-\u0026gt;data = data; NewNode-\u0026gt;left = NewNode-\u0026gt;right = NULL; return NewNode; } BstNode* Insert(BstNode* root, int data) { if (root == NULL) { root = GetNewNode(data); } else if (data \u0026lt;= root-\u0026gt;data) { root-\u0026gt;left = Insert(root-\u0026gt;left, data); } else { root-\u0026gt;right = Insert(root-\u0026gt;right, data); } return root; } int main() { BstNode* root = NULL; root = Insert(root, 10); root = Insert(root, 15); root = Insert(root, 5); root = Insert(root, 25); root = Insert(root, 20); printf(\u0026#34;The max is %d\\n\u0026#34;, FindMaxRecursion(root)); printf(\u0026#34;The min is %d\\n\u0026#34;, FindMin(root)); return 0; } ","permalink":"https://m1yan.github.io/posts/2023113-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84c++%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0/","summary":"\u003ch1 id=\"树的介绍\"\u003e树的介绍\u003c/h1\u003e\n\u003cp\u003e树(Tree)作为一种数据结构，具有一种递归性。一个树可以看作是由根节点(root)以及若干子树构成，而子树又可以继续向下分成根和子树，因此树具有递归性。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e二叉树(binary tree)\u003c/code\u003e是树中的一种，它满足每个节点都有要么2个要么0个子节点的特性。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e二叉搜索树(binary search tree)\u003c/code\u003e满足左侧子树中储存的值都小于等于root，而右侧子树上的值都大于root，并且递归满足。\u003c/p\u003e","title":"二叉搜索树的C++递归实现"},{"content":"Introduction of Queue 队列作为一种抽象数据结构，遵循First-In-First-Out(FIFO)原则。\n它的定义如下： A list or collection with the restriction that insertion can be performed at one end (rear) and deletion can be performed at other end (front).\n队列的操作有以下几种： Enqueue(x) 入队列 Dequeue( ) 出队列（并返回删除队列的值） front( ) and rear( ) IsEmpty( ) 以上操作的时间复杂度均为O(1).\n队列的实现 用数组实现 我们分别定义两个变量front和rear来储存队首和队尾，队列为空时令front = -1 rear = -1。\n但是我们发现随着不断地Enqueue和Dequeue，队列逐渐后移，前面数组空间无法重复利用。于是我们采用循环数组，循环数组可以想象成是数组首尾相连，因此我们这样表示它的位置：\ncurrent position: i\nnext position: (i+1)%N\nprevious position: (i+N-1)%N (N表示开辟数组的长度)\n代码实现 IsEmpty( ) bool IsEmpty(){ if(front == -1 \u0026amp;\u0026amp; rear == -1) return true; else return false; } Enqueue( x ) void Enqueue(int x){ if((rear+1)%N == front)//rear向后一格与front重合，说明循环数组已满 return; else if(IsEmpty()){ front = 0; rear = 0; A[rear] = x; } else{ rear = (rear + 1) % N; A[rear] = x; } } Dequeue( ) void Dequeue(){ if(IsEmpty()) return; else if(front == rear){ front = -1; rear = -1; } else{ front = (front + 1) % N; } } front( )和rear( ) int front(){ return A[front]; } int rear(){ return A[rear]; } 用链表实现 Enqueue和Dequeue操作必然一个在链表头一个在链表尾，为了保证两个操作的时间复杂度都是O(1)，要设置两个指针front和rear。\n代码实现 struct Node{ int data; struct Node* next; } struct Node* front = NULL; struct Node* front = NULL; void Enqueue(int x){ struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); temp-\u0026gt;data = x; temp-\u0026gt;next = NULL; if(front == NULL \u0026amp;\u0026amp; rear == NULL){ front = temp; rear = temp; return; } rear-\u0026gt;next = temp; rear = temp; } void Dequeue(){ struct Node* temp = front; if(front = NULL) return; if(front == rear){ front = NULL; rear = NULL; } else{ front = front-\u0026gt;next; } free(temp); } ","permalink":"https://m1yan.github.io/posts/2023111-%E9%98%9F%E5%88%97queue%E7%9A%84%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0/","summary":"\u003ch1 id=\"introduction-of-queue\"\u003eIntroduction of Queue\u003c/h1\u003e\n\u003cp\u003e队列作为一种抽象数据结构，遵循\u003ccode\u003eFirst-In-First-Out(FIFO)\u003c/code\u003e原则。\u003c/p\u003e","title":"队列(Queue)的数组和链表实现"},{"content":"前缀、中缀与后缀表达式 计算机中一般使用三种表达式，分别是中缀表达式(Infix)、前缀表达式(Prefix)、后缀表达式(Postfix)。\n下表列出了它们的基本表示形式： 类型 Infix Prefix Postfix 形式 \u0026lt;operand\u0026gt;\u0026lt;operator\u0026gt;\u0026lt;operand\u0026gt; \u0026lt;operator\u0026gt;\u0026lt;operand\u0026gt;\u0026lt;operand\u0026gt; \u0026lt;operand\u0026gt;\u0026lt;operand\u0026gt;\u0026lt;operator\u0026gt; 例子 a+b*c +a*bc abc*+ 后缀表达式的计算方法 为什么要将表意清晰的中缀表达式转换成形式比较特殊的后缀表达式呢？这是因为中缀表达式的运算需要人为地遵循一些运算法则，而后缀表达式只需要遵循一个固定的运算法则即可完成运算。\n后缀表达式的计算方法为：\n从左到右对后缀表达式进行扫描，当遇到运算符时，将运算符与它前面两个数字组合成一个算式进行计算，用结果取代两个数字以及运算符的位置，继续向后扫描，直到表达式变成一个值，该值即为表达式的值。\n我们发现，这与栈的工作模式非常类似，于是，可以写出以下代码：\nint EvaluatePostfix(int exp[]){ stack\u0026lt;int\u0026gt; S; for(int i = 0 ; i \u0026lt;= strlen(exp)-1 ; i++){ if(exp[i] is operand)//如果是操作数，就入栈 S.push(exp[i]); else if(exp[i] is operator){//如果是操作符，将栈最上面两个操作数pop并保存在op1和op2中 int op1 = S.top(); S.pop(); int op2 = S.top(); S.pop(); res = Perform(exp[i],op1,op2);//Perform函数读取操作符和两个操作数并运算，返回得数 S.push(res);//将得数入栈 } } return S.top();//最后留在栈中的数字即为表达式的得数 } 将中缀表达式转换为后缀表达式 那么如何将Infix转换为Postfix，从而可以被程序计算呢？\n对比两种表达式\nInfix: A + B * C\nPostfix:A B C * +\n我们发现操作数之间的相对位置并无改变，只是操作符的位置发生变化。下面提供一个转换的方法，依然使用栈来实现。\n1.\nInfix A + B * C - D * E Postfix A Stack 2.\nInfix A + B * C - D * E Postfix AB Stack + 现在我们遇到了第二个符号，如果此时栈顶符号的运算级别大于等于该符号，那么就将栈顶的符号弹出并且加入Postfix，直到不符合该条件或者栈为空时，将该符号（也就是遍历到的符号）入栈。 Infix A + B * C - D * E Postfix AB Stack + * 直到遍历结束，将栈中剩余符号依次全部弹出并加入Postfix。 Infix A + B * C - D * E Postfix A B C * + D E * - Stack 但是我们并没有考虑Infix带括号的情况。如果表达式带括号的话，解决方案如下：\n遇到左括号让左括号入栈，如果栈顶是左括号，遇到操作符直接入栈\n直到遇到右括号，将栈顶的元素弹出并加入Postfix直到遇到对应的左括号时停止\n综合以上方法可以写出对应代码\nvoid InfixtoPostfix(char exp[],char res[]){ stack\u0026lt;char\u0026gt; S; int j=0; for(int i=0;i\u0026lt;=strlen(exp)-1;i++){ if(exp[i] is operand) res[j++] = exp[i]; else if(exp[i] is operator){ while(!S.empty()\u0026amp;\u0026amp;HasHigherPrec(S.top,exp[i])\u0026amp;\u0026amp;!IsOpeningParentheses(S.top())){//HasHigherPrec函数即栈顶符号优先级大于等于第i个符号时返回true，否则返回false //IsOpeningParentheses函数功能是检查栈顶是否是左括号，是返回true，否返回false res[j++] = S.top(); S.pop(); } S.push(exp[i]); } else if(exp[i] == \u0026#39;(\u0026#39;) S.push(exp[i]); else if(exp[i] == \u0026#39;)\u0026#39;){ while(!S.empty()\u0026amp;\u0026amp;!IsOpeningParentheses(S.top())){ res[j++] = S.top(); S.pop(); } S.pop();//这次pop是为了pop出栈中的左括号 } } while(!S.empty()){ res[j++] = S.top(); S.pop(); } } 编写10以内的混合运算计算器 任务 输入包括10以内的数字和+、-、*、(、)运算符的合法表达式，输出它的Infix、Postfix以及运算结果。\n代码 #include\u0026lt;iostream\u0026gt; #include\u0026lt;stack\u0026gt; using namespace std; bool Isoperator(char c){ if(c == \u0026#39;+\u0026#39;||c == \u0026#39;-\u0026#39;||c == \u0026#39;*\u0026#39;) return true; else return false; } bool HasHigherPrec(char a,char b){ if(b == \u0026#39;+\u0026#39;||b == \u0026#39;-\u0026#39;) return true; else{ if(a == \u0026#39;*\u0026#39;) return true; else return false; } } bool IsopeningPar(char c){ if(c == \u0026#39;(\u0026#39;) return true; else return false; } void InfixtoPostfix(char exp[],char res[]){ int j=0; stack\u0026lt;char\u0026gt; S; for(int i = 0; i \u0026lt;= strlen(exp)-1 ;i++){ if(exp[i]\u0026gt;=48\u0026amp;\u0026amp;exp[i]\u0026lt;=57){ res[j++] = exp[i]; } else if(Isoperator(exp[i])){ while(!S.empty()\u0026amp;\u0026amp;HasHigherPrec(S.top(),exp[i])\u0026amp;\u0026amp;!IsopeningPar(S.top())){ res[j++] = S.top(); S.pop(); } S.push(exp[i]); } else if(exp[i]==\u0026#39;(\u0026#39;) S.push(exp[i]); else if(exp[i]==\u0026#39;)\u0026#39;){ while(!S.empty()\u0026amp;\u0026amp;!IsopeningPar(S.top())){ res[j++] = S.top(); S.pop(); } S.pop(); } } while(!S.empty()){ res[j++] = S.top(); S.pop(); } } int Perform(char c,int a,int b){ if(c == \u0026#39;+\u0026#39;) return b+a; if(c == \u0026#39;-\u0026#39;) return b-a; if(c == \u0026#39;*\u0026#39;) return b*a; }\tint EvaluatePostfix(char res[]){ stack\u0026lt;int\u0026gt; A; int op1,op2,op; for(int i=0;i\u0026lt;=strlen(res)-1;i++){ if(!Isoperator(res[i])) A.push((int)(res[i]-\u0026#39;0\u0026#39;)); else if(Isoperator(res[i])){ op1 = A.top(); A.pop(); op2 = A.top(); A.pop(); op = Perform(res[i],op1,op2); A.push(op); } } return A.top(); } int main(){ char exp[100]; char res[100]; gets(exp); printf(\u0026#34;Infix: %s\\n\u0026#34;,exp); InfixtoPostfix(exp,res); printf(\u0026#34;Postfix: %s\\n\u0026#34;,res); printf(\u0026#34;The result is %d\u0026#34;,EvaluatePostfix(res)); return 0; } 输入/出结果 ((1+2)*3-4)*5+(6-7) Infix: ((1+2)*3-4)*5+(6-7) Postfix: 12+3*4-5*67-+ The result is 24 ","permalink":"https://m1yan.github.io/posts/2023110-%E4%BD%BF%E7%94%A8%E6%A0%88%E5%AE%9E%E7%8E%B0%E4%B8%AD%E7%BC%80%E5%88%B0%E5%90%8E%E7%BC%80%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E8%BD%AC%E6%8D%A2/","summary":"\u003ch1 id=\"前缀中缀与后缀表达式\"\u003e前缀、中缀与后缀表达式\u003c/h1\u003e\n\u003cp\u003e计算机中一般使用三种表达式，分别是\u003ccode\u003e中缀表达式(Infix)\u003c/code\u003e、\u003ccode\u003e前缀表达式(Prefix)\u003c/code\u003e、\u003ccode\u003e后缀表达式(Postfix)\u003c/code\u003e。\u003c/p\u003e","title":"使用栈实现中缀到后缀表达式的转换"},{"content":"题目背景 我们都知道，在编程语言中，我们常用多种类型的括号，( )圆括号、[ ]方括号、{ }花括号，当括号不匹配时，编译时会发生错误。那么编译器是如何检验括号匹配性的呢？\n题目分析 下面列举了若干实例帮助理解匹配性的判断标准\nExpression Balanced？ Reason {(a+b)+[a-b]} Yes )( No 右括号必须在左括号右边 [ ( ] ) No 后出现的左括号要先完成匹配(Last Opened First Closed) [ ( ) ( ) ] Yes 任何右括号要对应它左侧距它最近的左括号 代码实现 代码 #include\u0026lt;iostream\u0026gt; #include\u0026lt;stack\u0026gt; using namespace std; stack\u0026lt;char\u0026gt; S;//定义一个用于存放括号的栈 int match(char c){//设计一个match函数，如果右括号和位于栈顶的左括号相匹配返回1，否则返回0 if((S.top()==\u0026#39;(\u0026#39;\u0026amp;\u0026amp;c==\u0026#39;)\u0026#39;)||(S.top()==\u0026#39;[\u0026#39;\u0026amp;\u0026amp;c==\u0026#39;]\u0026#39;)||(S.top()==\u0026#39;{\u0026#39;\u0026amp;\u0026amp;c==\u0026#39;}\u0026#39;)) return 1; else return 0; } char a[20]; int main(){ gets(a); char exp;//该变量用于存放当前的左括号 int len = strlen(a); for(int i=0;i\u0026lt;len;i++){ exp = a[i]; if(exp==\u0026#39;(\u0026#39;||exp==\u0026#39;[\u0026#39;||exp==\u0026#39;{\u0026#39;)//当识别到左括号即进栈 S.push(exp); else if(exp == \u0026#39;)\u0026#39;||exp == \u0026#39;]\u0026#39;||exp == \u0026#39;}\u0026#39;){//如果识别到右括号 if(S.empty()||match(exp)==0){//如果栈内是空的（说明右括号前没有左括号）；或者括号不匹配 printf(\u0026#34;No\u0026#34;); return 0; } else//如果匹配，则让匹配完成的左括号出栈 S.pop(); } } if(S.empty()){//完成遍历后，若栈中剩余括号，证明括号不匹配；若空栈，证明匹配 printf(\u0026#34;Yes\u0026#34;); return 0; } else{ printf(\u0026#34;No\u0026#34;); return 0; } } 输入/出结果 example 1: {(a+b)+[a-b]}\rYes example 2: )(\rNo example 3: [ ( ] )\rNo example 4: [ ( ) ( ) ]\rYes ","permalink":"https://m1yan.github.io/posts/202319-%E6%A0%88%E7%9A%84%E5%BA%94%E7%94%A8%E6%A3%80%E6%9F%A5%E6%8B%AC%E5%8F%B7%E5%8C%B9%E9%85%8D%E6%80%A7/","summary":"\u003ch1 id=\"题目背景\"\u003e题目背景\u003c/h1\u003e\n\u003cp\u003e我们都知道，在编程语言中，我们常用多种类型的括号，\u003ccode\u003e( )圆括号\u003c/code\u003e、\u003ccode\u003e[ ]方括号\u003c/code\u003e、\u003ccode\u003e{ }花括号\u003c/code\u003e，当括号不匹配时，编译时会发生错误。那么编译器是如何检验括号匹配性的呢？\u003c/p\u003e","title":"栈的应用：检查括号匹配性"},{"content":"Introduction of Stack 栈是一种数据结构，属于抽象数据结构(ADT)，遵循Last-In-First-Out(LIFO)原则。\n基本操作： 1）Push(x)\u0026ndash;在栈中插入x\n2）Pop()\u0026ndash;移除最近插入的元素\n3）Top()\u0026ndash;返回栈顶的元素\n4）Empty()\u0026ndash;判断栈是否为空\n以上几个操作的时间复杂度均为O(1)。\n栈的应用： \u0026ndash;Function calls/recursion\n\u0026ndash;Undo in an editor\n\u0026ndash;Balance Parentheses\nImplementation of Stack I. Array implementation int A[10]; int top = -1;//global void Push(int x){ top = top + 1; A[top] = x; } void Pop(){ top = top-1; } int Top(){ return A[top]; } bool IsEmpty(){ if(top ==-1) return true; else return false; } II. Linked list implementation 如果Push在链表的末尾，那么每次操作的时间复杂度为O(n)，不符合栈的O(1)operation。所以每次Push都将新元素插入链表的头部。\nstruct Node{ int data; struct Node* link; } struct Node* top = NULL; void Push(int x){ struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); temp-\u0026gt;data = x; temp-\u0026gt;link = top; top = temp; } void Pop(){ struct Node* temp; if(top == NULL) return; temp = top; top = top-\u0026gt;link; free(temp); } Using stack to reverse I. Reverse a string 调用C++中自带的stack库函数，能够直接定义一个栈，并且直接调用以上几种操作。\n调用语法为：头文件\u0026ndash;\u0026lt;stack\u0026gt;,语法 stack \u0026lt;变量数据类型\u0026gt; 栈名称；name.option()为操作\n代码 #include\u0026lt;iostream\u0026gt; #include\u0026lt;stack\u0026gt; using namespace std; void Reverse(char *C,int n){ stack\u0026lt;char\u0026gt; S; for(int i=0;i\u0026lt;n;i++){ S.push(C[i]); } for(int i=0;i\u0026lt;n;i++){ C[i] = S.top(); S.pop(); } } int main(){ char C[51]; printf(\u0026#34;Enter a string\\n\u0026#34;); gets(C); Reverse(C,strlen(C)); printf(\u0026#34;Output: %s\u0026#34;,C); return 0; } 输入/出结果\nEnter a string hello! Output: !olleh II. Reverse linked list 代码 #include\u0026lt;iostream\u0026gt; #include\u0026lt;stack\u0026gt; using namespace std; struct Node{ int data; struct Node* link; }; struct Node* head; void Reverse(){ stack\u0026lt;struct Node*\u0026gt; S; Node* temp = head; while(temp != NULL){ S.push(temp); temp = temp-\u0026gt;link; } temp = S.top(); head = temp; S.pop(); while(!S.empty()){ temp-\u0026gt;link = S.top(); S.pop(); temp = temp-\u0026gt;link; } temp-\u0026gt;link = NULL; } void InsertAtHead(int x){ struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); temp-\u0026gt;data = x; temp-\u0026gt;link = head; head = temp; } void Print(){ struct Node* temp = head; while(temp-\u0026gt;link != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;link; } printf(\u0026#34;\\n\u0026#34;); } int main(){ head = NULL; InsertAtHead(5); InsertAtHead(4); InsertAtHead(3); InsertAtHead(2); InsertAtHead(1); printf(\u0026#34;Input: \u0026#34;); Print(); Reverse(); printf(\u0026#34;Output: \u0026#34;); Print(); return 0; } 输出结果 Input: 1 2 3 4 5 Output: 5 4 3 2 1 ","permalink":"https://m1yan.github.io/posts/202318-%E7%94%A8%E6%A0%88%E5%8F%8D%E8%BD%AC%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%88%96%E5%8F%8D%E8%BD%AC%E4%B8%80%E4%B8%AA%E9%93%BE%E8%A1%A8/","summary":"\u003ch1 id=\"introduction-of-stack\"\u003eIntroduction of Stack\u003c/h1\u003e\n\u003cp\u003e栈是一种数据结构，属于抽象数据结构(ADT)，遵循Last-In-First-Out(LIFO)原则。\u003c/p\u003e","title":"用栈反转一个字符串或反转一个链表"},{"content":"双向链表介绍 双向链表的结构如下：\nstruct Node{ int data; struct Node* prev; struct Node* next; }; 可以看到双向链表的节点是由两个结构体指针及相关数据构成的，因此可以更方便地对链表中的节点进行访问和数据的修改。\n双向链表的实现 进行几个基本操作：头部插入(InsertAtHead)、尾部插入(InsertAtTail)、打印(Print)、反向打印(ReversePrint)。\n头部插入(InsertAtHead) 插入时要先在堆区开辟一块动态内存，为了避免代码重复，我们设计一个函数GetNewNode(int x)，可以在堆区新建一个节点，存储数据x，同时返回堆区内存的地址。\nstruct Node* GetNewNode(int x){ struct Node* newNode = (struct Node*)malloc(sizeof(struct Node)); newNode-\u0026gt;data = x; newNode-\u0026gt;next = NULL; newNode-\u0026gt;prev = NULL; return newNode; } 再进行头部插入\nvoid InsertAtHead(int x){ struct Node* newNode = GetNewNode(x); if(head == NULL){ head = newNode; return; } head-\u0026gt;prev = newNode; newNode-\u0026gt;next = head; head = newNode; } 尾部插入(InsertAtTail) 只需要先遍历到最后一个节点，再进行插入操作即可。\nvoid InsertAtTail(int x){ struct Node* newNode = GetNewNode(x); if(head == NULL){ head = newNode; return; } struct Node* temp = head; while(temp-\u0026gt;next != NULL){ temp = temp-\u0026gt;next; } temp-\u0026gt;next = newNode; newNode-\u0026gt;prev = temp; } 打印(Print) void Print(){ struct Node* temp = head; printf(\u0026#34;Forward: \u0026#34;); while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); } 反向打印(ReversePrint) 同样需要先遍历到最后一个节点。\nvoid ReversePrint(){ struct Node* temp = head; while(temp-\u0026gt;next != NULL){ temp = temp-\u0026gt;next; } printf(\u0026#34;Reverse: \u0026#34;); while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;prev; } printf(\u0026#34;\\n\u0026#34;); } 完整实现如下： #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; struct Node{ int data; struct Node* prev; struct Node* next; }; struct Node* head = NULL; struct Node* GetNewNode(int x){ struct Node* newNode = (struct Node*)malloc(sizeof(struct Node)); newNode-\u0026gt;data = x; newNode-\u0026gt;next = NULL; newNode-\u0026gt;prev = NULL; return newNode; } void InsertAtHead(int x){ struct Node* newNode = GetNewNode(x); if(head == NULL){ head = newNode; return; } head-\u0026gt;prev = newNode; newNode-\u0026gt;next = head; head = newNode; } void InsertAtTail(int x){ struct Node* newNode = GetNewNode(x); if(head == NULL){ head = newNode; return; } struct Node* temp = head; while(temp-\u0026gt;next != NULL){ temp = temp-\u0026gt;next; } temp-\u0026gt;next = newNode; newNode-\u0026gt;prev = temp; } void Print(){ struct Node* temp = head; printf(\u0026#34;Forward: \u0026#34;); while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); } void ReversePrint(){ struct Node* temp = head; while(temp-\u0026gt;next != NULL){ temp = temp-\u0026gt;next; } printf(\u0026#34;Reverse: \u0026#34;); while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;prev; } printf(\u0026#34;\\n\u0026#34;); } int main(){ InsertAtTail(1);Print(); InsertAtHead(3);Print(); InsertAtHead(2);Print(); InsertAtHead(4);Print(); InsertAtTail(5);Print(); ReversePrint(); return 0; } 输出结果：\nForward: 1 Forward: 3 1 Forward: 2 3 1 Forward: 4 2 3 1 Forward: 4 2 3 1 5 Reverse: 5 1 3 2 4 ","permalink":"https://m1yan.github.io/posts/202315-%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8doubly-linked-list/","summary":"\u003ch1 id=\"双向链表介绍\"\u003e双向链表介绍\u003c/h1\u003e\n\u003cp\u003e双向链表的结构如下：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-c\" data-lang=\"c\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"n\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eprev\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003eNode\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003enext\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"p\"\u003e};\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e可以看到双向链表的节点是由两个结构体指针及相关数据构成的，因此可以更方便地对链表中的节点进行访问和数据的修改。\u003c/p\u003e","title":"双向链表(Doubly Linked List)"},{"content":"\n从上图中可以看出，反转一个链表只需要改变Node.link。\nI. 迭代实现 思路 设置三个结构体指针Prev、current、next，分别保存之前的节点的地址、目前的节点地址、之后的节点地址。\n代码实现 void Reverse(){ struct Node *current, *prev, *next; current = head; prev = NULL; while(current != NULL){ next = current-\u0026gt;next;//next 用来存储当前Node的next，当Node.next被prev覆盖后，next将值赋 给current从而进入下一个循环。 current-\u0026gt;next = prev; prev = current; current = next; } head = prev; } 完整代码如下：\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; struct Node{ int data; struct Node* next; }; struct Node* head; void Insert(int n){ struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); temp-\u0026gt;data = n; temp-\u0026gt;next = NULL; struct Node* temp1 = head; if(head == NULL){ head = temp; return; } while(temp1-\u0026gt;next != NULL){ temp1 = temp1-\u0026gt;next; } temp1-\u0026gt;next = temp; } void Print(){ struct Node* temp = head; while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); } void Reverse(){ struct Node *current, *prev, *next; current = head; prev = NULL; while(current != NULL){ next = current-\u0026gt;next;//next 用来存储当前Node的next，当Node.next被prev覆盖后，next将值赋 给current从而进入下一个循环。 current-\u0026gt;next = prev; prev = current; current = next; } head = prev; } int main(){ head = NULL; Insert(2); Insert(4); Insert(6); Insert(5); Print(); Reverse(); Print(); return 0; } 输出结果：\n2 4 6 5 5 6 4 2 II. 递归实现 1）递归实现输出的反转 我们先写一个递归形式的Print函数，因为递归需要给函数传参，因此函数形式为Print(struct Node* p)。\nvoid Print(struct Node* p){//p初始为头节点的地址 if(p == NULL){ printf(\u0026#34;\\n\u0026#34;); return; } printf(\u0026#34;%d \u0026#34;,p-\u0026gt;data); Print(p-\u0026gt;next); } 此时总是先输出再调用函数自身，因此是正向输出。如果我们将printf与Print两行代码交换位置，即\nvoid Print(struct Node* p){//p初始为头节点的地址 if(p == NULL) return; Print(p-\u0026gt;next); printf(\u0026#34;%d \u0026#34;,p-\u0026gt;data); } 那么函数会在输出之前先进行重复调用，进行到递归结束条件后，依次执行输出。\n完整代码如下 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; struct Node{ int data; struct Node* next; }; struct Node* head; void Insert(int n){ struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); temp-\u0026gt;data = n; temp-\u0026gt;next = NULL; struct Node* temp1 = head; if(head == NULL){ head = temp; return; } while(temp1-\u0026gt;next != NULL){ temp1 = temp1-\u0026gt;next; } temp1-\u0026gt;next = temp; } void Print(){ struct Node* temp = head; while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); } void ReversePrint(struct Node* p){//p初始为头节点的地址 if(p == NULL) return; ReversePrint(p-\u0026gt;next); printf(\u0026#34;%d \u0026#34;,p-\u0026gt;data); } int main(){ head = NULL; Insert(2); Insert(4); Insert(6); Insert(5);\tPrint(); ReversePrint(head); return 0; } 输出结果 2 4 6 5 5 6 4 2 2）递归实现链表反转 反转输出并没有实现链表的反转，因为head、next的值都没有发生改变。但是我们依然可以采用递归的方式实现链表的反转。\n设计一个Reverse函数，使得其能够以递归方式实现链表反转。 void Reverse(struct Node* p){//p初始为头节点的地址 if(p-\u0026gt;next == NULL){ head = p; return; } Reverse(p-\u0026gt;next); struct Node* q = p-\u0026gt;next; q-\u0026gt;next = p;//也可以简洁地表示为p-\u0026gt;next-\u0026gt;next = p p-\u0026gt;next = NULL; } 完整代码如下 #include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; struct Node{ int data; struct Node* next; }; struct Node* head; void Insert(int n){ struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); temp-\u0026gt;data = n; temp-\u0026gt;next = NULL; struct Node* temp1 = head; if(head == NULL){ head = temp; return; } while(temp1-\u0026gt;next != NULL){ temp1 = temp1-\u0026gt;next; } temp1-\u0026gt;next = temp; } void Print(){ struct Node* temp = head; while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); } void Reverse(struct Node* p){//p初始为头节点的地址 if(p-\u0026gt;next == NULL){ head = p; return; } Reverse(p-\u0026gt;next); struct Node* q = p-\u0026gt;next; q-\u0026gt;next = p; p-\u0026gt;next = NULL; } int main(){ head = NULL; Insert(2); Insert(4); Insert(6); Insert(5);\tPrint(); Reverse(head); Print(); return 0; } ","permalink":"https://m1yan.github.io/posts/202213-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8%E8%BF%AD%E4%BB%A3%E5%8F%8A%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://pic3.zhimg.com/80/v2-c610a28038b0cfba44431bfe4ac48e24_r.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e从上图中可以看出，反转一个链表只需要改变Node.link。\u003c/p\u003e\n\u003ch2 id=\"i-迭代实现\"\u003eI. 迭代实现\u003c/h2\u003e\n\u003ch3 id=\"思路\"\u003e思路\u003c/h3\u003e\n\u003cp\u003e设置三个结构体指针Prev、current、next，分别保存之前的节点的地址、目前的节点地址、之后的节点地址。\u003c/p\u003e","title":"反转链表（迭代及递归实现）"},{"content":"链表是一种常见的数据结构，它的基本单位是node，由data和link两部分组成。\n创建一个链表需要以下几个步骤： 1.创建节点的结构体 一个简单的结构体，由数据和结构体指针构成，代码如下：\nstruct Node{ int data; struct Node* link; }; 2.创建变量保存头节点的地址 struct Node* head; head = NULL; 3.在堆区为节点开辟一个动态空间 struct Node* temp = (struct Node*)malloc(sizeof(struct Node)); 4.为节点赋值 temp-\u0026gt;data = 2; temp-\u0026gt;link = NULL; 5.更改头节点的地址 head = temp; 下面尝试遍历一个链表，并逐个数输出 struct Node* temp1 = head; while(temp -\u0026gt; link != NULL){ printf(\u0026#34;%d \u0026#34;,temp1-\u0026gt;data); temp1 = temp1-\u0026gt;link; } ","permalink":"https://m1yan.github.io/posts/202312-linked-list---implementation-in-c/","summary":"\u003cp\u003e链表是一种常见的数据结构，它的基本单位是node，由data和link两部分组成。\u003c/p\u003e\n\u003ch2 id=\"创建一个链表需要以下几个步骤\"\u003e创建一个链表需要以下几个步骤：\u003c/h2\u003e\n\u003ch3 id=\"1创建节点的结构体\"\u003e1.创建节点的结构体\u003c/h3\u003e\n\u003cp\u003e一个简单的结构体，由数据和结构体指针构成，代码如下：\u003c/p\u003e","title":"Linked list - Implementation in C"},{"content":"\n思路 设计一个 Insert(int data，int n) 函数，使得一个存有该数据的节点能够插入第n个位置。\n代码实现 设计一个 Insert 函数，一个 Print 函数 以及 main 函数。\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; struct Node{ int data; struct Node* next; }; struct Node* head;//定义一个全局变量head，使所有的函数都能够调用，无需传参。 void Insert (int data,int n){ struct Node* temp1 = (struct Node*)malloc(sizeof(struct Node)); temp1-\u0026gt;data = data; temp1-\u0026gt;next = NULL; if(n==1){ temp1-\u0026gt;next = head; head = temp1; return; } struct Node* temp2 = head; for(int i=0;i\u0026lt;n-2;i++){ temp2 = temp2-\u0026gt;next;//循环后的temp2是第n-1个Node的地址，也是第n-2个Node的next值 } temp1-\u0026gt;next = temp2-\u0026gt;next; temp2-\u0026gt;next = temp1;//第n-1个Node的next更改为插入Node的地址 } void Print(){ struct Node* temp = head; while(temp != NULL){ printf(\u0026#34;%d \u0026#34;,temp-\u0026gt;data); temp = temp-\u0026gt;next; } printf(\u0026#34;\\n\u0026#34;); } int main(){ head = NULL; printf(\u0026#34;How many numbers?\\n\u0026#34;); int n,i,number,position; scanf(\u0026#34;%d\u0026#34;,\u0026amp;n); for(i=0;i\u0026lt;n;i++){ printf(\u0026#34;Enter the number and position\\n\u0026#34;); scanf(\u0026#34;%d%d\u0026#34;,\u0026amp;number,\u0026amp;position); Insert(number,position); Print(); } return 0; } ","permalink":"https://m1yan.github.io/posts/202312-%E5%9C%A8%E9%93%BE%E8%A1%A8%E7%9A%84%E4%BB%BB%E6%84%8F%E4%BD%8D%E7%BD%AE%E6%8F%92%E5%85%A5%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://pic1.zhimg.com/80/v2-df124cb2a854c5f1b32272a12ddbb0e0_r.jpg\"\u003e\u003c/p\u003e\n\u003ch3 id=\"思路\"\u003e思路\u003c/h3\u003e\n\u003cp\u003e设计一个 Insert(int data，int n) 函数，使得一个存有该数据的节点能够插入第n个位置。\u003c/p\u003e","title":"在链表的任意位置插入一个节点"}]